We're now going to move on to discuss the very important operating system concept of the process. The process is the active thing, the thing that allows people to run code on their computers to do anything they really need to do on the computer. Almost everything that happens at user request on a computer is going to be as a result of it happening in the context of a process. So we'll talk about what a process is, how we go about in an operating system executing and controlling execution of processes, and the very important concept of state. So in today's lecture, we'll first talk about that. What is a process? Then we'll talk about how the operating system handles processes, since it is primarily in charge of dealing with processes. And one important element of processes is their state, and we'll talk about how the The operating system manages the state of processes. What's a process? In the previous lecture, we talked about various kinds of abstractions that we have that are provided by the operating system. One class of those abstractions was an interpreter. A process is a particularly important type of interpreter used in an operating system. It is an executing instance of a program. If you want to run Microsoft Word, you run a process for running Microsoft Word. If you want to run a web browser, you run a process that contains the web browser. If you want to run a game, the game is run in the context of a process, and so on and so forth. Every program you run is run as part of a process. One other way of looking at a process is to say we try in operating systems to create the illusion for each process that it is its very own virtual computer, that it has complete control over everything that's going on in the computer, access to all of the computer's resources, and so on and so forth. This is, of course, not true. The operating system is what is in control, has access to everything. And in fact, even if you try from a process to do certain things, which are possible within the context of the computer, you may not be allowed to do so. The operating system may prevent that. But within some limitations, we try it look for each process as if it's running its very own computer, there's nothing else going on in the computer, it has complete control. Now another way of looking at a process is that a process is an object. We're not talking about object-oriented here. We're talking about more general computing concept of an object. So an object is something that is a entity that has state, which means it's got some data that describe its properties, and it's got operations. it is capable of doing. Now, not all things in the operating system that count as objects are processes. So, for example, a file is in a process, but it is an object within the operating system. One important class of objects for the operating system, however, is the process. They're central to modern operating system behavior. And if you want to run any kind of modern operating system, Windows or Mac OS or Linux or any of the others you might happen to be running, you're going to have to have processes. Now, I've used the term state a few times. It's part of what we're talking about as a major topic within this lecture. What is state? Well, if you have a persistent object, something that doesn't just go away instantly, and we're talking here not just about persistence in terms of surviving reboots, but even persistence in terms of ongoing behavior of a computer, persistent objects have state. They have data that describes what is the condition of that object. And this state helps distinguish one object of a particular type, such as a file or a process, from another object of the same type. And further, the state tends to characterize what's going on with this particular object. What is its condition? If it's a file, is it open? Is there a pointer into the file that says this is the next thing you're going to read? In the case of a process, what to, what other resources in the operating system have been made available to this process. Now, as you might deduce from this, objects tend to change their state. Their state is not usually static. It usually changes as time goes by. And the way that we indicate that the object has changed is by making changes to some or all of its state. So, what's in a state? Well, it depends on what kind of object you're talking about. Different types of objects have different types of states. The more complex the object is, the more complex its state tends to be. However, one thing to be aware of when we're talking about computers and objects and computers is that every object in a computer has state that is representable by a set of bits. Somewhere there's a set of bits that represent the state of that object. Now, this has an important characteristic for the purpose of running an operating system, which is if we make a copy of the bits that represent the state of an object, we can get rid of that object temporarily, saving its bits somewhere, and we can restore the object later on because the state, the bits we saved, would represent a complete description of everything that's important about that object. We can make a copy of the object. We can have two copies of the same kind of object, which have identical state. So state is very, very important for many purposes in an operating system. And it's important to remember as well that when you get down to the brass tacks, it's just a bunch of bits. When you come right down to it, everything in a computer is just a bunch of bits. So it's also the case that within a complex object, you tend to have pieces of state that are used for particular purposes. And other pieces of the state are used for other purposes. So you have subsets of the state. for example, within a context of an object that represents a process, we could have a subset of the state that represents scheduling information. So what do we have? What are the elements of an operating system's state for a process? Well, if it's a process, we might have the scheduling priority. And of course, we want to know what program are you running with this process. So that's part of its state. If it were a file, how far into the file have you read? Have you read the first 503 bytes? Is the next byte you'd like to read 504? If you are talking about a state that represents an I/O operation, I have requested that this message be sent to an off-site machine. OK, has the message been sent or not? We might very well maintain an operating system object that says, this is keeping track of what's happening with this message. And we can look and see, has it been put into the network card? Has the network card sent it off node or not? One thing we have with a process that we'll talk about in much detail later in other classes is we have a memory space. Pieces of memory that are available for the use of that process where it can put things like its stack. Now, those are in pages, which we'll talk about more what pages are in the future. But clearly, you're going to have multiple pages because the pages aren't all that big that represent almost any process. Therefore, one part of the state of a process is, well, what memory pages has it got? Where are they? Now, generally speaking, if we're talking about an operating system object, most of the state of the object is going to be managed by the operating system itself, but not directly by user code. As a rule, the application must ask the operating system if it may change the state of these objects. So if you want to say, I would like to change the state of this file so the pointer isn't at byte 503, it's at byte 700, you have to ask the operating system to do that for you. You can't do it directly. If you would like to say, I would like to end this process, you have to ask the operating system, please end the process. There are some elements of state, especially when we're talking about processes, that actually are directly under the user's or the program's control. They can change it without asking the operating system. We'll talk about that in a little bit more detail in a few minutes. Now, as I've already alluded to, one very important element of the state of a process is its address space. The address space is the collection of addresses that the process is allowed to use. It can issue these addresses, and we're talking here about RAM addresses, or at least that's what it looks like to the process. These are addresses where you can say, here's the next instruction I would like to execute. Here is a piece of data in my heap that I would like to bring in and work with. Here is where my stack is located. That's the kind of thing that we keep in the address space, all kinds of things, really. Now, the entire collection of addresses that is legal for a process to issue is called the address space. There are some addresses that we just don't allow the process to issue. We'll talk about what happens if you try later. But within a given process, at any given moment, there's a set of addresses that it can issue and a set that it can't issue. The set of addresses that it is permitted to issue is typically called its address space. So it's very important to know which of those addresses are within the address space and which possible addresses are not in your address space. So we're going to have to keep track of that. So generally speaking, if it isn't in your address space, your process cannot access it, not legally. If it tries to, a bad thing will happen. Now, generally speaking, in a modern operating system, like Windows, we are likely to say it's at least potentially possible that your address space for your process could contain every single nameable address. How many are those? That depends on the size of the word in the memory, because the word can be used to address a piece of memory. So if it's a 64-bit word, which is common in modern hardware, then you could have up to 2 to the 64th different addresses. And in principle, at least, your process could be able to name any one of them. So that's going to be difficult because, of course, 2 to the 64th is quite a lot of memory. Nobody's got that much RAM in their machine. So there's going to be an issue there if you try to deal with a very, very large address space when you don't have a whole lot of RAM. Addresses in the address space are supposed to refer to a piece of RAM. They aren't supposed to refer to something else, like a location on a disk, different thing. So we are going to have to do something to cover up these little issues. There's also the case that many, many, many different processes are running simultaneously. They all want to have a big address space. What if all of your processes want to have an address 10,000? How can they all have an address 10,000? We'll talk about that later. Now, leaving that aside, we have two important entities related to code that people want to run in order to get things done on their computers. There are programs and there are processes. Each of them actually has its very own address space. Let's take a look at the difference. We'll start with the program. A program sits on disk somewhere. It's sitting on the flash drive, for example, typically. What's out there in the file that represents the program on the flash drive? Well, in most computers, the very first thing you're going to see in that file, if you look at it in detail, is what's called an ELF header. This is something that is going to say here, in the first place, this is an executable program. This is something you can execute. And in the second place, here's some important information about trying to execute it. The very first thing is, what ISA is it? Because if you take a that was compiled for, let's say, a PowerPC, and you move it onto an Intel processor, it's not going to run. It was built for the PowerPC ISA, it's running, or trying to run, on the Intel ISA, and the instructions aren't the same. So the machine language instructions you'd have in that program would not successfully run on the Intel chips. Well, I mean, if you try to run them, there's going to be obvious errors, but why even try to run them? So typically, the operating system, if it's told I'd like to run this program, would take a look at the ELF header and would say, "Is this the right ISA? Is it compiled for the correct ISA for what I'm currently running on, for the ISA on the machine that I, the operating system, am running?" If the answer is no, it's going to reject that. It's not even going to try to run that program. Assuming it's the correct ISA, then there's going to be other things in the ELF header. It's going to say, here are things that you are going to need to load into memory somewhere. Various things that are important for you to load into memory in order to make this program an actual runnable process. Also, there may be other areas sitting around in this program, sitting out there on the disk, that are not actually going to be loaded into memory in order to run the program, but they may be useful for other purposes. Those are called info sections. So ELF stands for executable and linkable format. If it's an executable program, we're not talking here about scripts, those are treated differently, but if it's an executable compiled program, you have linked it together and now it should be ready to run. It should have everything in that file that will allow it to successfully run. So it has been set into executable format, the instructions are in machine language form, and it's linked. All of the pieces that make it up, including libraries and such, will all be properly linked into it so that when it tries to call one of them, that will be successful. So we're going to have various sections out there on that file, and what are those sections going to be? Well, one section is going to be the code section. Now, every one of these sections has a header. This header describes, here's what this section is like. What kind of section is it? This one's a code header. Where does it start? This is the That's the length. And what's going to be after that header? Well, the compiled code in this case, because this is a code segment. All the code in machine language form is going to be compiled here. So there's not going to be anything here that is source language. It's all going to be compiled code. We might have another header that says, OK, here's something else you're going to need to load into memory in order to make this program into a process-- data. You can define, and maybe some of you have defined in programs you write data sections. This is a set of data that is going to have locations in memory and possibly, depending on how you set things up, initial values at the moment at which you load them up. And you specify, here's what I want. Here are the variables I want. Like, I want a big table that's a megabyte long. And oh, what would I like in that megabyte table? I'd like the following values in that megabyte table. That will be in this section, section 2 in this example. As you can see, section 2 is of type data. It starts at some particular address, which is specified. And it's some number of bytes. And then within that section-- this is all still in the same file-- within that section will be the actual variable values you want to have initialized. OK, now as I said, there's also info sections. One such info section is called the SIM table. What's the sim table? Well, when you wrote your program in, let's say, C, you had variable names. You had foo, you had bar, you had x, you had y, you had whatever you had. Those names have been stripped out at the point at which you did the compiled code. We don't keep track of that when we're trying to run the code. Instead, we've said, OK, x is located at this memory location, foo is located at that memory location, and so on. Then we forget that it's x and foo. Now, for the purpose of actually running the program, that's fine. we don't need those names in order to run the program. But what if there's a problem? What if there's a bug in the code? If there's a bug in the code, you're gonna wanna go in there and debug the code. And if it turns out that you've lost track of where x is and where foo is and where bar is, that becomes rather tricky. So what we have is a symbol table that is not actually loaded into the process that we're gonna run, but it is associated with that process. It keeps track of the fact that, when you said you're going to have these initialized data variables, x would be here, y would be here, bar would be there, main, the main function, that would start here, et cetera, et cetera. That's going to be in the symbol table. The symbol table is used for debugging. If you're running a debugger, then when you run the code within the context of the debugger, it's going to use the symbol table to figure out where did the operating system put the variables, the memory locations, the locations of routines within the program, the locations of libraries. Where did it put those? So that you can follow that throughout your running program, you can stop it in the debugger and say, OK, what's the value of foo now? Foo should be 5. What is the value of foo? Oh, it's 12. No wonder I have a bug. So this is used-- the symbol table is used pretty much only when you're doing debugging. Otherwise, it is ignored. It does not result in anything being loaded into the processes address space. Programs, that's what we're talking about so far. Programs are located on disk. Programs are not located in RAM. When you wanna run something though, a program on the disk, it does not run. You only can run things when you have the necessary pieces of that code in memory, in RAM. Okay, so if we want to run a program, we're going to have to put a bunch of stuff into RAM. We create a process for that purpose. The process, as we've already said, has an address space. So we're going to take all this stuff that's in the program, and we're going to map it into particular memory locations in the address space of the process. So for example, here is an example of a process set up within RAM. The lowest RAM address might be 0. The highest RAM address is FFFFFFFFF, not a 64-bit machine. So we are going to have to have certain things that this process has available to it in order to run the process. It's got to have code. It's got to have those machine language instructions. They have to be in RAM or you can't run them. It's got to have a stack. The stack is going to of course, keep track of the current state of execution of the process. What routine is called what routine is called what routine, et cetera. What are the local variables that are set up within the context of each of these routines? What is the return value here, et cetera. All that's going to be kept in the stack. Stack two must be kept somewhere in RAM. So it's going to be somewhere within the address space of the process. If you have these initialized data values, or for that matter, if you have dynamically allocated memory and say, I'm going to have a bunch of memory available to set up dynamic variables when I run the program, so it can choose to create data records, structures, whatever it may be doing, those are going to be kept in an area called the heap. That's going to be private data. So that has to be somewhere in memory as well. Pretty much everything you're going to do when the process is running is going to require locations in memory that are going to be used to do whatever it is you're doing. OK, and we might very well set things out in the following way. The code's in one place. The private data's in another place. The stack's in another place. If you're using shared libraries, libraries as we talked about in the previous lecture that are used by more than one process simultaneously, Those are going to be sitting somewhere too. So shared libraries may be sitting around in these various places. Okay, now the difference between programs and processes are programs are on disk. They are inert, they're not doing anything. They're just sitting there in the form of persistent data. Processes are running. They are in RAM, they are going to execute code and they are going to change their state as they run. Now, if we are building an operating system and we're going to have processes in our operating systems, we pretty much always do. We're going to have to lay them out somehow or other. We're going to have to have some way by which we say, here's where we put the code, here's where we put the stack, here's where we put the heap, here's where we put the shared libraries and so on and so forth. And those locations are locations within the processes address space. So there's going to be addresses that the process is allowed to access. Some of those are going to be used to hold code. Some of those are going to be used to hold the stack. Some of those are going to be used to hold the heap. If there are shared libraries or dynamically loaded libraries, some of those will be used to hold those libraries. If it's a static library, it's part of the code. All right. Now, we thus have a bunch of different things that are going to be sitting in a processes address space. We have code. We have stack. We have data areas. We have shared libraries, et cetera. So those various different things have different properties in terms of how we can use them within the context of the memory. For example, we don't do self-modifying code in modern computers, which means when a process is running, it may not change the instructions that it has currently got set up. You can't do that. You can recompile, stop the process, run a new process with a newly compiled version, but you cannot change the code in the middle. So we don't need the place that we don't even want, the places where we've stored the code in the address space to be writable. They should not in fact be writable. Obviously they need to be executable. Maybe they need to be readable too, depending. Okay, on the other hand, the stack. The stack had really better be readable and writable because otherwise we cannot create new stack frames if we can't write, and we can't figure out what the value of a variable declared within the context of a routine is. You know, you declared variable X within the context of routine foo. What's the value of x? You've got to be able to both write and read that variable. So the stack's going to have to be readable and writable. On the other hand, hard, unpleasant experience has shown it's a very bad idea for the stack area to be executable. So that won't be executable. So we're going to have to have control of the memory segments, the pieces of memory, to say this memory segment should be executable, not writable. That memory segment should be readable and writable, not executable. And that means that we're probably going to want to have big segments that are holding all the code, big segments that are holding all the heap, big segments that are holding all the stack. And we're going to have to put them somewhere. They've got to go somewhere in memory. Somebody wants to start up a process, we're going to have to give it some memory to work with, and we're going to have to put its various pieces in those places within that address space. Different operating systems have different strategies for how they do this. The standard way of doing this in Linux, for example, would be more or less something like this. Assuming that you have the lowest address being zero and the highest address being F, the old fashioned Linux strategy would say, well, we'll put the code starting at zero, following wherever the code is. And the code is static. Once you set up the code, that's how big it is. It never gets any bigger. Right after that, let's have the data segment. That's that heap, the area where we can dynamically allocate memory. And then, got to put the stack somewhere. Standard Linux strategy would say, well, put the stack at the far end of the memory space, as far away from other stuff as it can be. So this is what it would look like in memory as shown in this diagram. So the code segment is static, it's not going to grow. But the data segment and the stack segments, they can both grow. So whenever you have a new stack frame, for example, you need to put a new piece of memory in. And when you have another one, because it's called another routine, you have to put another piece of memory in, so it can grow. So data segments are going to grow too. You dynamically allocate more memory. Perhaps you're going to need a bigger data segment. So you're going to have to have those grow. The way we do it in Linux is we say the data grows up. It goes from low addresses to higher addresses. The stack grows down. It goes from higher addresses to lower addresses. So the code is static. The data is there, and it grows up. The stack is there, and it grows down. and we better not have the data and the stack hit each other. They better not both get to the point where they are trying to share a single address because you can't share a single address that way. So we don't allow them to meet. The operating system ensures that the process will not allow that to happen. All right, so code segments. We'll talk a bit more about those. We start with a load module. That's what we have out there in that program sitting on the disk. Where'd it come from? It came from a linkage editor. Now you may say, oh, didn't it come from a compiler? Well, the compiler created pieces of executable code, but it did not necessarily create an entire program that had all the pieces put together. The linkage editor puts all those pieces together. Now, one thing that is required as a result of the linkage editor putting all those pieces together is you know where every routine is. You know where every variable is. You have all of those references to other elements of the process that you're going to run. already said here's where it is. You've loaded the following shared library. Here's where that library is. We're going to know that at the point at which we do the linkage editing. And then we're going to combine them into a few segments. Now if you've written complex programs, you probably have written programs in which you didn't have one say.c file or.cc file. You had 12 and you compiled each of them individually and then you use the linkage editor to put all of those compiled versions, the.o files, into a single executable. So once you have it in a single executable, you can say, well, here's all the code and here's all of this data that is defined ahead of time, and there's going to be a stack somewhere. So at that point, we can, when we run the program, the operating system can look at that and say, ah, the linkage editor has told me how big things are, what I need to wear. But I do of course have to then load it into memory. So there's all this code that I created and it's sitting out there in the program on disk. The operating system, when I say I'd like to run that program, please give me a process that's running that program, it's going to have to copy that code from the disk into the place in memory that it is allocated for this particular process to use for its code. And it's going to have to of course also say say this is now an executable segment that is associated with this process. So it would have to actually have memory, RAM, that is devoted to holding the code for this particular process. And it would have to say, OK, I'm going to keep track of the fact that this process owns that particular piece of RAM that holds its code. And I will have to set up that piece of RAM so that it's execute only. So you could have it read only. That's a choice of the operating system. You don't necessarily need to be able to read your code segment. What we're going to do with a code segment ordinarily is we're going to say, OK, we're going to jump to the following instruction or this-- at any rate, this is the next instruction that your program counter points to. That just means that the CPU has to go out, grab whatever's in there, and run it. It doesn't necessarily have to read it. OK. So there are many-- let's say that you are running a compiler. And let's say that you are actually running a compiler on several different programs. You've got three or four different programs you're working on. Totally different. They're not going to be linked together, but they have to all be compiled. You can, if you want to, run all those three or four compilations simultaneously. You could be using, say, GCC for all those computations simultaneously. And if each one is going to take half an hour to compile, they're all-- all four of those processes are going to be running at the same time. They're all going to be running GCC. We do not need multiple copies of GCC sitting in memory. Because after all, it's not writable. It can't be changed. The processes can't change it. Nobody else is going to change it either while it is running. So all four of those processes, each of which is supposed to be totally separate, could share the same code. We could put the GCC code in one place in RAM and then just make sure that that place is accessible to the address space of all four of those processes. We can share code segments safely. That's how we do shared libraries as well. Now another thing, of course, we're going to have in the process is a data segment. The data segment is going to have to be initialized. Some of the data segment is going to be defined in your.h files or whatever it may be that you've got that says, here's the data I'd like. Here's my one megabyte of data that is going to be used as input to this program. All right. So that's going to be used as part of it. Now, what if you don't have any of that data? You usually have at least a little. But what if you don't have any? Typically, the operating system is going to say, I think you're going to need some data. So it'll give you some number of pages that you currently have not actually said I'm going to use as part of your data segment. So some of the data segment is going to be whatever you defined ahead of time. Other parts of the data segment are going to be empty space that you can use to hold dynamically allocated data when you choose to do so. Now, there are going to be in some of your programs what are called BSS segments. That stands for blocks started by symbol. It doesn't matter what that means. All it really means for practical purposes is zero it out. When you start the program, zero it out. What if that one megabyte data structure that you've got doesn't have initial values? you're going to do in the program is you're going to read a bunch of data. You're going to get messages in your file or whatever. And then you are going to set records within that one megabyte data structure to the various values you've read in. It's desirable, helpful to make sure that that data starts out as zeros. So you don't have a bunch of junk sitting in there that might corrupt your overall view of what is happening in your program. In many cases, hardware, RAM hardware, is capable of zeroing out a big chunk of data very, very quickly without saying copy zero into this word, copy zero into that word, copy zero into that word. If your RAM can do that, and if you know you've got this one megabyte segment that all needs to be zero, you can use that cheap operation to zero out the whole megabyte. Now, once you have the data segment, it's going to be read/write because the process is going to use it to store data, to read data, etc. And moreover, it is going to be data that, unlike the code, is private to that process. You got those four compilations running. Each one of those compilations is, as a result of trying to do compilation, going to be making use of its data segment to store stuff. But each one of them may be storing something separate because you're compiling four different programs. You don't want compilation one interfering with the values of compilation three or vice versa. So the data segments of those four processes, all running the same compiler, all sharing the same code segment, the data segments will not be shared. They will be private. And not just kind of private, but as private as the operating system can make it. Even if there is a bug or malicious code in one of these processes that you're running, it should not be able to look at the data being held by any other process that's currently That data should be private to each process. Now, another issue for the data segment is how big should it be? We don't know, because after all, you may dynamically allocate a whole lot of data. You may have a program that sometimes is only going to need a few tens of kilobytes of data, and sometimes is going to need 500 gigabytes of data. Well, given that we don't know ahead of time which of those it's going to be, we're probably going to start with a guess, And if it turns out that we don't have enough, we'd like the program to be able to request more. So that means the data segment might grow while we are running this process. In Linux-style systems, we use a call called sbreak for that purpose to say, let's grow that data segment. Now, the third thing that we have that's going to be very commonly used is the stack. Modern computing uses stack-based languages. That's how we do our programs. We use stacks. Every time that one routine in your program calls another routine in the program, the result of that call is that you're going to put a new stack frame onto the stack. When you are running code, you are going to be running in the context of whatever stack frame you are currently in. So you jumped into a routine called foobar. You're going to be running in the context of foobar. Variables defined within the foobar routine are going to be accessible to you. You are going to continue working in that context until either there's another call to something within foobar to yet another routine, or alternately, the foobar routine ends. If the foobar routine ends, you're going to pop the stack frame off of the stack and go back to whatever called foobar. If, on the other hand, foobar calls the XYZ routine, you're going to push a new stack frame for XYZ on top of the stack, just on top of where the foobar frame is. Foobar frame's still there, but now you're to be working in the context of the XYZ routine stack frame. OK. I hope that this is something you're already familiar with. If this is a brand new concept to you, then you really haven't had what you need to have in order to really understand what's going on in this class. It's not a class that's supposed to teach about stack frames. But in modern languages, all our modern languages are stack-based, every one of them. So what this means is that processes, as set up by the operating system, have stacks. The operating system expects every process to have a stack. And so it will set up a stack segment for you when you say, I'm going to start running this process. And it'll give you a stack frame for the initial routine that you're gonna start running in, the main routine if you're writing in C, for example. And what are you gonna keep there? Well, you're gonna keep there any local variables. So within the main routine, for example, you've defined 27 different variables, some ints, some character arrays, some pointers, whatever it may be, they're going to be kept in the stack frame. They're not kept in the data segment. Anything that is specifically defined within the routine is in the stack frame for that routine when that routine is running. OK. Also, you get parameters. When you're in the main function and you call foobar, you may say, I'm going to call foobar with variable i, the value of i. OK. We're going to have to keep that eye in the stack frame for foobar. And then you're also going to have to say, we're going to reuse registers. And when we finish running this routine, we're going to go back to the previous routine. And we want all of our registers to be set up properly for that previous routine, which means when we call the stack, when we call foobar, we're going to have to save all of the registers that were in the routine called foobar before we start doing anything with foobar. We'll save those in the stack frame as well. And then when we finish with foobar, when we return from FUBAR, we'll use those saved registers to restore their values so that whoever called FUBAR will see things as they were. They will also get a return value from FUBAR, which they're expecting to see, minus one or whatever, or zero or whatever it might be. So that too will be kept in the stack. So because stacks are ubiquitously used in all of our programs, modern CPUs have support, have hardware features that make it more efficient to perform stack operations. So for example, creating a stack frame. Now, stacks clearly, obviously, are part of the state of the process. What's going on in the process depends on the state of the stack. So that is going to be something that has to be stored in the memory space of the particular process. So we have a stack segment in every process. How big is it? Well, it depends on what the program is doing. You can have a routine that calls a routine that calls a routine. If we have recursive routines, the routine can call itself and call itself again and can call itself again. Every time a routine is called, you put a stack frame on the stack. Every time a routine ends, you pop that stack frame off the stack. So this means that we don't necessarily have the ability to predict the size of the stack at the moment at which we create the process. It will all depend on the dynamic behavior of the process. However, once we pop something off the stack, and it was using some amount of memory in the stack segment, we can reuse that stack, that amount of memory for another stack frame. So as long as we are working within the segment we currently have available for the stack, it can grow, it can shrink, it can grow again, it can shrink again, it can grow again and shrink again, and we can keep reusing the same memory locations for that purpose. So as long as we have enough stack space, the operating system doesn't need to do anything very special with it. But if it turns out that we don't, we're going to have to do something. And clearly when you create the process, you're going to have to set up some area that will hold the stack. You'll set some default size to that. The operating system will determine what that size should be. And it will allocate that much memory at the moment it starts the process. All right. And you can clearly, dynamically, in most modern operating systems, increase the stack size. You set it to a default value, it becomes clearer as the program is running, as the process is running, that isn't big enough. We're going to need more stack space. The operating system can dynamically allocate more space for the stack. It really has to be kind of contiguous though in terms of addresses, because if it isn't contiguous, it becomes very difficult to handle the stack and we aren't really prepared to deal with those difficulties. As a rule, as I've already stated, stack segments have to be read-write. You have to be able to read them, you have to be able to write them. And again, as with the data segment, they are private to the process. They belong to the particular process that is running with them, that is using them. They should not be visible. They should not be changeable by any other process. So they're process private. But they're not executable. Now, one other thing that we're going to have to do with the address space of a process is make use of space for libraries. Static libraries we don't have to worry about. Static libraries are built into the code segment sitting on the disk in the program. They're already in there. The instructions for those static libraries are sitting in that code segment. So we just don't even worry at the point that we're starting to run a process based on that program. We don't even worry that they are a library. They're just more code that's sitting in the code segment. We just load it up. Of course, this means that if the library changes, if somebody updates the library, then we're gonna have to recompile or at least relink our program, because it's got the old version of the library there. If we want the new version of the library, we'll have to relink to get the new version of the library. If we have shared libraries, then they aren't in the code segment. They are specified as being a shared library. We have that kind of marker that I showed in the previous lecture saying, hey, here's a shared library here. It's this shared library, 5c, for example. They're not going to use space up in the code segment, obviously. So that's good. Also, if it truly is a popular library, there's a very good chance that when this process starts running, when we first create the process, the shared library is already sitting in memory somewhere. All the operating system will have to do is say, well, it's over here. And it will have to then say, OK, I will put into the address space of this particular new process, here's where the shared library is, and I'll make sure that works. Now, this is good because it means if you have 300 processes running and they're all using libc, you only one copy of libc sitting in RAM, not 300. It means also the program starts up faster. When you start up the program, you have to load the code of the program into the processes address space. If some of that code, because it's in a shared library, is already sitting somewhere in an appropriate address within memory, you don't have to load it. It's already there. And this also makes it easier to upgrade libraries. You don't have to worry about relinking your programs. OK, now, address spaces of a process contain a lot of the processes state, but not all of it. So for example, there are the registers. At any given moment, a process is using a set of registers. Now, the registers are actually in hardware. They're in a core of the CPU. And there's a value in those registers. And as the program continues to run on that core, it will use those registers to keep values temporarily. We use registers instead of memory locations because registers are a lot faster. Also, CPU instruction sets, the ISAs, typically say, you can do things with data that's sitting in a register that you cannot do with things, with data that's sitting in a memory location, such as add together two numbers. You can't say I'm adding together this number that's in location 1,000 and location 50,000. What you can do instead is say load location 1,000 into register 1, load location 50,000 into register 2, add register 1 and register 2. So registers are going to be very, very widely used. And what values we have in them is very, very, very important. There are also special purpose registers, one of which I've alluded to before is the program counter. That's something that points to an address in the code segment of the process saying, here's the next instruction to execute. That is the next instruction that will be executed if the process continues to run. There's also the processor status, typically called a processor status word or PSW. This keeps track of various bits of information about what the process is currently doing. What is its status? One of those pieces of information is, are you in privileged mode so you can run the privileged instructions? Or are you in normal mode so you can't run the privileged instructions? There's a bit in the processor status where it covers that. Other things in the processor status word are like a bit that says, on the last instruction, did you have an arithmetic overflow? If so, we set that bit. If not, we clear that bit. Did you have a negative result from the last number? OK, if so, we set the date. If not, we don't set the date, et cetera, et cetera. Also, of course, we have to know not just where your stack is. We have to know where that is. So we're going to have a stack pointer saying, here's where your stack is. That's the location, the overall location of your stack. But also, we'd like to know, yeah, yeah, you've got a stack. But you got 33 different frames on the stack because you've been calling a lot of routines. Where is the current frame that you're working with? we'll have a frame pointer that keeps track of that. All of those will be in hardware registers on the core of a process that's running. Now the process also is ask the operating system to do various things for it, such as, for example, open the following file. I'm going to read the following file. The operating system will set up information that says not only to have that file open, but have it open for this process. This process is the one that's working with that file. And what working directory I'm in, You're familiar with the concept, I imagine, of current working directories. When you're doing something in a computing environment, when you're using a windowing environment, or when you're using a command line environment, there's a current working directory, and things are done in the context of that current working directory. You want to open file foo, it's going to look in the current working directory for foo. We have to know where that is. That's part of the information that specifies the state of a particular process. what is its current working directory. We'll talk about locks later. Locks tend to be, in certain cases at least, specific to processes. We may keep those in the processes state, but it's managed by the operating system. All of those things, the open file information, the current working directory, the lock information, that is specific to the process, but controlled by the operating system. And then there's other stuff that is controlled by the operating system that is not really directly used by the process itself. So for example, we keep track of how long the process has been running. How many CPU seconds has it expended? Well, in most cases, the process itself doesn't care how many CPU seconds it's expended. We, on the other hand, from the point of view of running an entire computer, may care about that quite a lot. So that's kept track of by the operating system, even though the process itself will probably not bother to look at that. So, there's a whole lot of operating system specific information that we are gonna have to keep track of for every single process. That suggests that we should have a data structure that keeps that information, that organizes all that information, and indeed we do. Generically, it's called a process descriptor. Every modern general purpose operating system has some kind of data structure that the operating system keeps on a per process basis that organizes the information associated with that process. So everything that is associated with that process is going to be kept in this data structure, or at least pointers to other things. So for example, the address space, the entire address space, the entire list of all the pieces of memory you've got, we don't keep that actually in the process descriptor. We have another structure called an address space that we are going to have associated with the process, and then there'll be a pointer in the process descriptor to the address space for the particular process. We do that with various other things. So for example, all the information about the file you have open. We don't keep all of that information in the process descriptor. We have a separate operating system object that says, this describes this file that's open. And within the process descriptor of the process that's using that file, it has opened that file, we have a pointer to that other structure. All of this information in the process descriptor is managed by the operating system itself. A process cannot directly make any changes to any information in its process descriptor. In certain cases, it can ask the operating system to do that on its behalf, but it cannot directly do anything. In fact, it can't even directly look at its process descriptor. Again, it can ask the operating system, show me my process descriptor. It probably will. But it has to ask the operating system to do that. It can't directly get at this. All right, so we use the process descriptor for many, many purposes related to process management within the operating system. Scheduling, determining when should we run the process. Security decisions, may the process do this thing it wants to do or may it not. Allocation issues, he wants more memory for his data area. He's running that S-break call. Okay, I got to figure out where his address space is so I can figure out where to put the memory and so on and so forth. Now, there are various different names and different operating systems for their process descriptors. In Linux and Unix systems, it's commonly called a process control block or a PCB. It's a form of process descriptor. And it's got all the kinds of things that we're talking about. This diagram shows some of the information that's in the process descriptor of a Linux system. It's got a process ID, which is just a unique number. It's got a state thing, pointers of various kinds, information about how important is this process. It's got something about what is the current program counter of the process. What are the values of the CPU registers? Here's information about ongoing operations for I/O. Did it ask to send a message, et cetera, et cetera. Counting information, how many CPU seconds is this process used up, and so forth and so forth. So we're going to keep track of all kinds of things in this process control block. Now, let's talk about how the operating system handles processes. What's it going to do? It has a very limited number of things that it does with a process. It creates them. When it needs to run a new process, it must create one. It destroys them. Under various circumstances, we don't want the process around anymore. We need to get rid of it. And of course, we run them. The reason somebody wants a process is because it wants to run a program. In order to do that, we have to create the process and start the process running. So creation first. Where do processes come from? Processes are always created by the operating system. The operating system creates processes. Programs can ask the operating system to create a new process. They can't create one themselves. They must ask. So somehow or other, they are going to, when they ask the operating system to create a process, or when the operating system chooses to create a process on its own, we are going to have to have some way of initializing that process. Effectively, we're going to have to say this process is supposed to run that program. Therefore, we're going to have to bring in the code for that program, set up a data segment for that program, link to whatever shared libraries that program uses, and set up a stack segment for that program. Then we're going to have to do things like say, who owns this process, and so forth. So the most common way that processes get created is an existing process asks to create the process. A process is running, it says I want to create a new process. Now usually when you do that, you're somehow or other, either immediately or shortly afterward, you're going to have to indicate, okay, I was running program X, I want to create a new process with program Y, I must specify this new process should run Y. And there are other things you're going to have to specify. For example, it may be some initial parameters associated with that process that you wish to be supplied to the process as soon as it starts running. Now generally speaking, since practically all processes are created this way, processes will tend to have a parent process, which is the ID of the process that created them. And on the other hand, parents, processes that have created other processes, will know about child processes, the processes they created. So somehow or other, we are going to run a process, say I'd like to create a new process. So what is that going to involve? Well, we have to ask the operating system to do that since processes are a data structure, an object that is controlled by the operating system. We, an individual user process, cannot on our own create a new process. We can, however, ask the operating system to do so. So the operating system, when it is requested to do so, must create a process descriptor. So it's got a process descriptor. Now what? What's it going to do with it? Well, generally speaking, we have a lot of processes running on a computer simultaneously. So we need to keep track of all of those processes. What we do in modern operating systems is we create a table. This table contains an entry for every single process that is currently running. So when we create a new process and create a process descriptor for that new process, we will put that process descriptor in the table. Or more commonly, we will put a pointer to that process descriptor in the table. Now maybe this isn't really a table at all. It might be a linked list. It might be a tree. It might be whatever data structure the designers of the operating system think is appropriate for keeping track of all of their processes. But it's most commonly called a table, even if that isn't really the kind of data structure it is. This table will contain one entry for every process that is currently active. Processes that have been completely completed, that have gone forever, no longer have entries in the process table. So we got a process descriptor, we put it in the process table. What else do we need before we can start running the code associated with that process? Well, it's got to have an address space. It's got to have some memory in order to put the things it needs, which we discussed earlier in this lecture, into RAM. So we're going to have to get hold of some of that. Now, the operating system controls all the RAM, as we will discuss in future lectures. So it's going to somehow or other decide this is the RAM that is going to be made available to this new process. Now, typically, we are going to have to have a object that itself describes that address space, a per-process object that describes all the addresses the process is allowed to issue, where, what code is allowed to issue, where its stack is located, what parts are legal parts of the stack, what parts are legal parts of the heap and so on. So this is a data structure in and of itself. The operating system, when you create a new process, will thus need a new data structure of this type for the new process. Further, it's going to have to fill in portions of that data structure to say, Here are the pieces of memory I've used to hold the code. Here are the pieces of memory I've used to hold a stack. Here are the pieces of memory I've used to hold the data area. Then of course, I have to actually put things into those pieces of memory, for example, I have to put code into the pieces of memory. I said, here's where the code is. I get that code out of the program, out of that file that contains the ELF. Then, of course, I have to initialize the stack of to set that up, so it's ready to go. And then I'm going to have to, if I'm going to run a program, I'm going to choose a core on my CPU, and I'm going to set the program counter, the processor status word, the stack pointer, the frame pointer, assuming it has one, to the appropriate values, and then it'll be ready to go. OK. Now it turns out there are two different ways that modern operating systems can create a process. And these are actually used in popular operating systems, so it's important to go through them. First, the obvious thing to do. You want a new process, you create a blank process. This is a process that has nothing in it, except that when you say, I'm going to create this process, you provide a bunch of information that says, here's how I want the process to be created. And the operating system looks at that information you provided and it fills in everything it needs to fill in. This is what Windows does. The other operation is to say, well, you know, I already have a process. The one, the parent, the one that called it. Let's make a copy of the parent. So now we'll have two processes, the parent process, the child process. The child process will look almost exactly like the parent process. It'll have the same code. It'll have the same program counter. It'll have a copy of the same stack, et cetera, et cetera. This is what we do in Linux and in all Unix-based systems. This is how we do things. Now that sounds a little weird and it is a little different than what we do in the Windows system. So let's go back to the Windows system and talk about the obvious way of doing things first. That's starting with a blank process. You have nothing, run this request to the operating system, create me a new process. And it says, okay, fine. I'll allocate a process descriptor. I will fill in the process descriptor. How will I fill it in? Well, I need to know what program you're gonna run. I need to know various other things, some of which are related to whatever process it was that called it, like who's calling this. Others may be very specific to this, such as, how big should its data area be? How big should its stack be? Things like that. Either I will have defaults, or you will have specified in your attempt to create this new process, here's how I'd like it to be set up. Okay, so fine, the operating system will look at all that information, and we'll set up the brand new process properly, changing the different fields in the process descriptor, setting up an address space, filling the address space as necessary. Now, you might, if you're thinking about this, say, okay, process A creates process B, process B creates process C, process C creates process D. Who created process A? Well, maybe process A was created by process X. But what happened when you booted the computer? When you booted the computer, there weren't all processes. There was nothing. You loaded up the operating system and started running the operating system and there were no processes. The operating system at that point will create one or more primal processes. These aren't processes that belong to an individual user. They're processes whose purpose is to allow creation of all the processes users eventually will want to have. Now, generally speaking, this approach of starting with a blank process is what's done in Windows. And they have a particular system call called the create process system call, which obviously is what it's supposed to do. It creates a process. Now, the Create Process System call is extremely flexible, in the sense that it has many, many different parameters, most of which are optional, that say, here's what I'd like to do to specify what this new process is going to be like. And there are different values you can have for the different parameters. Usually, one way or another, you specify, here's the program I would like to run by naming a file that contains the code for that program. And there are actually a couple of ways of doing this. Windows tends to be rich in different ways of doing things. They have lots and lots and lots of different ways of doing things, which gives you a certain amount of power, adds a certain amount of complexity, good points and bad points. So effectively, you either use the defaults or look at the parameters that have been provided to this create process system call to figure out as the operating system, what should I do to create this process? That's one way of doing things, a very obvious way. The other way is the Linux way, which is less obvious, but actually around a long time. That's called forking, process forking. This is what the Linux and Unix style systems use to create new processes. If you've ever created a new process in the Linux system, you know about forking. If you haven't, then you might not have, not knowingly at least. You don't really know how this works and you would be a little surprised to hear how it does work. Here's how it works. I'm running a process. I decide I want a second process. I want a new process, probably something around a new piece of code. What do I do? I run a system call. What's the system call called? Fork, F-O-R-K. What are the parameters? There are no parameters, no parameters at all. It has a return value, which I better keep track of, but no parameters. What's going to happen here? I'm going to, when I run the fork system call in the Linux system, clone the existing process. I'm going to use the same code. I'm going to make a copy of the stack. I'm going to do something about the data segment. Okay, now, if it turns out that what I really wanted to do was have two copies of the same program running in separate processes, it's a great way to do it. It's going to be very easy to do. Now, the reason that things were, if I don't want to do that, it's going to be a little odd. I'm going to have to do something more. Why did it get set up this way in the first place? Years and years ago, back in the 1970s. So this has been around for 50 plus years. Well, it has some advantages. Originally, it was set up because they thought they were going to do parallel programming this way. We don't do parallel programming this way. But it does have other advantages for things like creating pipes. You're probably familiar with using pipes if you've used a Linux system at all. However, the most common thing you're going to do after you've created a new process in a Linux system is say, I don't like the fact that the new process I've created is running the same code as the old process. I wanted to run a different piece of code. That was the reason I created the process, after all. I wanted to run a different piece of code. How am I going to do that? Well, when I have performed a fork, I have two processes. They're running the same piece of code. They're at the same program counter, so they're pointing to the same next instruction. But they have a different ID number. Otherwise, they're almost exactly the same. So what do I do? I've got two programs running. One of them will run the parent. The other one will run the child. OK. Got the same code, same program counter. What do I do? If I've used this properly to do what I intend to do, which is to run a new program, I'm going to then say, OK, fine. I will figure out, am I the parent? In which case, I'll do one thing. Am I the child? In which case, I'll do another thing. So I could have a big piece of code that does all kinds of the different things I might need to do. And depending on whether I'm the parent or the child, they go one way or another. We'll talk about what we really do in a moment. But there is an obvious question here related to our memory. So here we have our parent, which is sitting in memory somewhere. It's a running process, not just a program. And in the RAM, that means it's got its code somewhere. So there's its code. It's got its data somewhere and its stack somewhere. There's a stack and there's its data. We perform the fork system call. F-O-R-C-K, open paren, close paren, that's it. Nothing more there, no parameters, no nothing. Okay, that results in the child process being created. The child process is another process, so it's obviously got a code segment. Same code segment, so we have a pointer to the same code. It's got a stack segment. Now, even though we're gonna start with a stack segment having the same bit values, we want a different stack segment because the parent of the child probably are gonna go off and do different things. So we've got a copy of its stack, not the same thing. Same bit values, but not the same location and memory. What about the data? Well, if it were the case that we were going to go ahead with this, maybe we'd want a copy of that data. But maybe not. We will talk at one point later about what we might do instead. So the parent and the child are sharing the same code segment. That's fine. They're not sharing the same stack. That's fine. How about its data segment? Well, really, it should have its own data segment. Because data segments, remember, are supposed to be private to particular processes. Each process has its own data segment. Other processes should not be able to affect its data segment. The parent should not be able to affect the child's data segment by writing into the parent's data segment, or vice versa. The child should not be able to affect the parent's data segment by writing into the child's data segment. However, what if the data segment is really, really big? Megabytes big? Are we going to copy megabytes of data from the parent to the child so they have two separate data segments? That could be expensive. Now, if that's what we have to do, that's what we have to do. But what if this isn't really what I was planning to do in the first place? What I was planning to do in the first place is say, here's my parent process running. I want to run a totally different piece of code, different program. I don't want the same code. I want to run a different program. Well, I can use fork to create a new process to hold that program, but it's running the wrong code. So what do I do? I use a second system call provided by Linux called exec. Exec says, remake the process. You got a process running, it's running program A. If you'd like, exec and run program B instead. Change the code, change the sack, change the data segment. Now we're running B. Okay, now given that you have that ability from fork, say, am I the parent or the child? You can say, well, if I'm the parent, I do the fork and then I go on and continue as the parent and do whatever the parent ordinarily does. If I'm the child, I don't do that. I say, oh, I'm the child. That means I should do an exec. And you do an exec. And then you change content of the child. You change its code, you change its stack, you change its data segment. This is how we actually create processes effectively in Linux. The fork is always followed by an exec in the child process and not by an exec in the parent process because you don't want to change the parent process. What is the parent process? Commonly, it's going to be one of two things. It's going to be a command shell. So one of those windows you see when you log into a remote server, that's command shell. You want that command shell to continue as you're running other programs. Or alternately, it's going to be a window manager. So when you're sitting in front of your laptop computer that's running a Linux-style system, it's going to have a window manager process. And when you create a new process out of the window manager, the new process should do something different. It should run, for example, your Word program. But the parent process continues to be the window manager. OK, so the child is going to have an exec. The parent's not. So what's going to happen? It's going to replace the child's code with new code. for example, the Word code instead of the Window Manager code. And then you're going to say, well, I want to start at the beginning of the Word program. You'll change the program counter. It's going to need a stack that's appropriate to the Word program, not the Windows Manager program. OK, fine. Get rid of the Window Manager's data segment, and let's add the stack. Now, you might-- so what are we actually going to do in the operating system point of view when we see this? Well, the operating system doesn't really care whether you do an exec or not. You did a fork, fine, you got two processes. Now I got two processes. I'm going to run sooner or later the trial process. When I run the trial process, it's going to do whatever it wants to do. What does it happen to want to do? Well, it happens to want to do an exec. Fine, that's the system call, no problem. I, the operating system, will do what I'm supposed to do. When there's an exec, what am I supposed to do? Well, get rid of the old child's code. That's simple enough. It just was a pointer to the parent's code. Get rid of the pointer. Point to a new piece of code instead. And what about the stack? Well, I have a stack. I have a stack that's appropriate for the old code. It's not appropriate for the new code. Throw away the old stack, set up a new one. How about the data area? Well, I can't use the old data area. So let's set up a new data area. Throw away the pointer in the old data area, set up a new set of memory to hold the new data area, put a pointer to that. OK. And once I've done all that, it's as if this was always what was running in this particular process. I've got a new process, running a new program. Now, if you think about the fact, remember I said something about, oh gee, what if we do a fork and we got a gigabyte of data in the parent? Are we going to copy the gigabyte of data? Well, if the very first thing that you do after running a fork in the trial process is always an exec, which it usually is, having copied that gigabyte of data is exceptionally wasteful. Because the very first thing you're going to do After you do that exec is throw away that gigabyte of data. Well, we know that's what's going to happen. Maybe that isn't what we want to do. Copy on write will be our friend here. We'll talk about copy on write later. OK, now on the other end of the life cycle of a process, we terminate processes. We destroy them. This is commonly going to happen, of course, for almost all processes. Ultimately, for all processes, when a computer stops running, all the processes go away. But during the lifetime of the computer, the typical thing that happens is you start running a program, you run the program for a while, the program completes. This means that the process that is running that program will run for a while using a process. And then when it finishes, that process will be discarded, will be thrown away. You don't just throw it away and forget about it. The operating system reclaims all of the resources associated with that process. There are other cases where you don't actually reach the end of the program, you do, for example, Control-C when you want to stop a program that's running and terminate it. Or the operating system may say, I, the operating system, because I think it's the right thing to do, will terminate that program. So you can do that too. But what's going to happen, regardless of why it happens, what's going to happen when you terminate a program? You need to get rid of, reclaim all of its resources. You need to take back any RAM you gave to it. You need to clean up the fact that the files it had open opened by that program. If it has locks, you need to get rid of the locks and anything else that you were doing that you kept data structures floating around in the operating system specific to this processes behavior, you need to get rid of those because they're not around anymore. So you reclaim any resources that may be holding you the operating system, reclaim resources that may be holding memory locks, access to hardware devices. I sent a message, I'm going to send a message on the network interface, oh look the message hasn't been sent, but on the other hand and the process is terminated, forget about it. You need to get rid of that. And it may be that other processes were communicating with this process. At the very least, the parent probably should know that this child has died. Or if it's the parent that died, maybe the child should know that the parent has died. So we may need to inform other processes that this process has ended. Not everybody, but only processes where there is some reason to believe there's a interaction between the terminated process of this other process. And then, of course, once you've done all of that, we really don't need the process descriptor anymore. And that's taking up space, taking up memory. So let's get rid of that and remove it from the process table. Now, in between creating and destroying, of course, we run the processes. We run the code. So we have to execute code for a process to do its work. That's what the process is supposed to do. It's an interpreter. And this means that we have to somehow or other make one of our hardware cores on our CPU execute the code associated with that process. Now, we have on modern computers a number of cores on our CPU, perhaps four, perhaps eight, perhaps 16, whatever number. Each of those cores is capable of running a separate stream of instructions, regardless of what the other cores are doing. If you have a 16 core processor, you can run 16 streams of instructions at the same time. You could run 16 different processes, one on each core, at exactly the same time. Each core will do its own work. That's great, but on a modern computer, you may have 300, 400 processes that are capable of running. Some of them may not be ready to run at the moment. But at any given moment, there may be a very large number of processes that you could run. in particular, more processes than you have cores. So you got 300 processes ready to run. You got 16 cores you can run them on. You run 16 of the 300, but the other 284 cannot currently be run. So what this typically means is in the first place, we're gonna have a lot of processes that are sitting around waiting. They can't run because they aren't on a core. And in the second place, it means we probably are every so often going to have to switch which process is running on which core. Core 12 is currently running process A. At some point in the future, core 12 will stop running process A and will run process B. Maybe process A will terminate, then we'll run process B. Maybe something else will happen and we'll switch from running A to B. May run A later, maybe on a different core. So this means that we are going to have to do something periodically to say we have a core we would like to run a process on. It isn't running on that core at this very moment. How do we get it running on the core? Here's what we do. It's called loading. We load a process onto the core. This involves going to the core, which is one of the elements of the CPU, and initializing the hardware associated with that core. If it's the very first time this process has ever run, we just created the process, never run an instruction before, then we are going to initialize the hardware on this core into the initial state of that process. If on the other hand, the process had run in the past at some point, but we stopped running it for whatever reason, and now we want to run it again, and this is something that happens all the time on our computers, then we are going to have to say, whatever state that process was in when it stopped running, we are gonna have to restore that state on the core that we are about to run it on again. Now remember, the process state is just a bunch of bits. There are all kinds of registers on this core that hold bits. So effectively, this is going to be a matter of saying, let us load up the necessary registers with the bits representing the state of this process we'd like to run on that core. Okay, so we load the core's registers. Now, of course, we also have to make sure the stack is available. That's sitting in memory somewhere, presumably. So if we set the register representing the stack pointer in the frame pointer properly, then it's just out there and we can use it. And if there's other issues, when we talk about memory management, we'll discover that things are a little bit more complex than you might think. And we may have to fiddle around with the stuff that's handling our memory. We'll have to do that as well. And then of course, what we're going to do is say, this core is currently not running anything, but we're loading stuff up on it. Once we load a value into its program counter, bang, The core is going to go ahead and start running instructions. So the last thing we're going to do is load up the program counter. What are we going to load it with? Whatever is the next instruction that this process should execute. If it's a brand new process, never been executed before, it's going to be the first instruction the process should ever execute. If it's a process that was running and stopped running for whatever reason, it's going to be the next instruction it should have executed if it hadn't stopped. OK, then what? From this point onward, for a while at least, we are going to run the process in a mode called limited direct execution. What's that mean? What that means is that we have a bunch of instructions that this process would like to run. It would like to run instruction x, then x plus 1, then x plus 2, then x plus 3. Maybe it'll loop back to x, whatever. At any rate, it is defined in its code that it's going to run the following set of instructions. If we load up the program counter with x saying, "Run instruction x next," what's going to happen? The core is going to go out to memory, fetch in the content of location x, which will be an instruction, and run it. Then it's going to increment the program counter, assuming it isn t a jump instruction or something like that. And what's going to do that? Well, it's incremented the program counter to x plus one. It's going to go out the location x plus one. It's going to grab the content of x plus one. It's going to bring it in. It's going to execute it. Then it's going to go to x plus two, bring it in, execute it. then go to x plus three, bring it in, execute it. What it's not going to do is ask the operating system, may I execute instruction x? Oh, thank you, operating system, please allow me. May I execute instruction x plus one? Thank you, operating system, please allow me. It's not doing that. It's just running instructions. There is no OS intervention at this point under ordinary circumstances. It's just gonna keep going and going and going. Instruction after instruction after instruction. Now, that isn't always going to work out. What happens, for example, if the process needs to get some help from the operating system? It needs to have something done by the operating system, such as opening a file. But it cannot do itself because it doesn't have the ability to run those privilege instructions that will allow it to open the file. What's it going to do? It is going to do something called a trap. We'll talk about traps in more detail later. the trap is going to say, you know, yeah, I know I'm in limited direct execution. I'm supposed to run instruction X plus 12 now, but X plus 12 indicates I need to do something with the operating system. Hey, operating system, you take over. Now, let's talk a little bit more about limited direct execution. This is what we're doing almost all the time when our computers are running. They're almost always running processes, not operating system code, in limited direct execution mode. And occasionally, every so often, each process will ask the operating system to do something, and that will require us to go to operating system code instead. And until we get back from the operating system code, the process won't be able to do anything else. So it'll be waiting. Occasionally, even though the process does not want help from the operating system, the operating system will occasionally say, "Hey process, you're running on core seven, stop. You may not run on core seven anymore. We'll pull you off core seven. We're putting somebody else on core seven." This is typically done with a type of interrupt called a timer interrupt. And we'll talk about how that works later. What we are trying to do when we are running processes in an operating system is maximizing the amount of time we are in limited direct execution. So this means that we are running processes, user mode processes. We are not running operating system code. We want to avoid running operating system code whenever we can avoid running operating system code. We always would prefer to be running user code. We can't always run user code for many reasons, but we always prefer to run user code because this is the performance that users expect to see. They want their code to run as fast as possible. How fast can it run? It can run at the speeds of limited direct execution. It can't run faster than that. If we don't run at those speeds, if we run at a slower speed, user performance degrades, people get unhappy. Okay, so this is true when we're doing OS emulation, when we're pretending that we have a Linux machine on top of a Windows machine, when we do virtual machines, which we do for many purposes, we'll talk about those in a late class in this quarter, we're still gonna want to try to do as much limited direct execution as we possibly can. We do not want to go to the operating system, to trap into the operating system unless we absolutely have to. Okay, we are trying to enter the operating system as seldom as we possibly can. And when we do have to enter the operating system, we want to get out of it as quickly as we possibly can. Limited direct execution is the key to good system performance. If your operating system does not achieve a high percentage of limited direct execution, you're not going to have very happy users. People are going to be very unhappy with your operating system. They're going to say, "Oh, this thing's slow as a turkey. We're switching to something else." Okay, now. Even though we want to avoid running operating system code as much as we possibly can, sometimes we have to, we just have to. So, there are other situations where we're not going to continue unlimited direct execution, we're going to do something else instead. Any of these situations where we stop running a piece of code that we were currently running and do something else instead is called an exception. This means essentially you're running on a core, something's running on a core, probably process code, maybe operating system code, something's running on a core, but the next thing you do should not be the next thing pointed to by the instruction pointer, by the program counter. And it should do something else. You should make an exception to moving on to the next instruction. Now, there are various reasons exceptions occur. Some of them are pretty routine, such as if you have a integer and you keep adding to the the integer and add and add and add and add, and you keep doing that in a tight loop for trillions of times, sooner or later, you are going to overflow that integer. The integer is capable of holding at most a particular number. If it's an unsigned integer on a 64-bit machine, 2 to the 64th minus 1, depending on whether it's on a different size word or if it's a signed integer, it's a somewhat different number, but there's a maximum number. However, if you keep adding one to it, what happens? You get to the biggest number, you add one to it, and it goes back to zero. That's called an arithmetic overflow. There are also conversion errors where you say, it's in one format, a floating point format, you're converting to an integer, it doesn't work well. So those things occur because your program, the code you were writing, tried to do something and it caused this problem. So these are kind of routine things. And you can either ignore them or you can test for them and say, well, if this happens, then I would like to take the following action. Oh, I had an overflow. Well, let's do something about that. A more common one is you read a file and read a file and read a file, and maybe you haven't really been keeping track of how much data was in the file. So you've read the last byte of the file. You don't know that yet. You try to read the file again. It comes back and tells you, hey, you're at the end of the file. You should be testing for that. And if you do, you'll be told, yes, here's an exception, end of file exception. And then, you know, you can't really effectively read any more from the file. Now when you are doing the kinds of things that could cause these exceptions, adding to a variable, reading data from a file, whatever you're doing to do that should be checking for these conditions, saying, "Do I have an arithmetic overflow? Okay, do the following thing. Have I reached the end of the file? Oh, move on and do something else." You check for those. Other exceptions you can't really check for in the same way. These occur unpredictably. They are happening outside of the context of what the process itself is doing. So for example, let's say that you were following a pointer in Linux, and you haven't been too careful and your pointer is set to zero. This is called the null pointer. You cannot follow the null pointer for reasons we'll discuss at some point. If you do, you get what's called a segmentation fault, and there are other ways you can get segmentation fault. It's not just dereferencing null. If you do that, well, you got a problem and something must be done. There are other types of things. So for example, you've got a program that's running. You're on a command line processor working on one of these servers and your program's running and running. You think you have a bug and it's running far too long. What do you do? You hit control C. That interrupts the program. That's an exception. That is something where we have caused the process that is running and running and running to stop running. There are other things that could happen there. So typically on a modern computer, particularly something like a laptop computer, you have some hardware in there that says, "Oh, we have just lost power. There is no more power for this computer, almost no more power for this computer. We need to shut everything down because we're about to die because we have no more power. You will send exceptions to all the processes that are currently running saying power failure. And if they can, they may do something about that. If not, they will just terminate. These kinds of exceptions are things you can't predict that well. They're called asynchronous exceptions. They're unpredictable, so your programs typically are not written to check for them. they're not written to check for them on every instruction. There are some programming languages that say we are aware of the possibility of asynchronous exceptions. So we are going to support something that's called the try and the catch operations. So basically, the catch operation says if this particular asynchronous exception of the following character, power outage, occurs, run the try code associated with it, and that'll tell you what to do when that exception occurs. Also, the hardware and the operating system, not the processes, support a kind of thing called a trap. This will say, okay, well, if this exception occurs, I'm going to do some code in the operating system. So most commonly, if you have one of those segmentation faults, what happens? You go to the operating system saying, we got a trap representing a segmentation fault. What shall we do about that? The operating system has code that says, let's not crash the whole machine because this guy dereferenced null. Let us instead take some action that's suitable, with suitable action, most commonly being kill the process, inform the shell, invoke the process, hey, this process died because of a segmentation fault. So when you get a segmentation fault and you see that message on your screen saying segmentation fault, well, that came because of a trap instruction. There are other kinds of things that cause trap instructions. In particular, there's one very important thing we do with traps. We have in operating systems for many, many years, use the capability of a trap to perform system calls. Somehow or other, if your process is running along and running along and running along, and it decides, for example, it would like to send a message. Well, sending a message is something it cannot do itself. It needs the operating system's help. How do you get the operating system's help? You want to make what's called a system call. You call on the system to do something. The way that we do that in modern computers is by using a special instruction that is called a trap instruction. How's that work? Well, in modern CPUs, in the modern CPU chips that we use, they have built into their instruction set one or more instructions that are designed in for this purpose, typically called a trap instruction. They may have another name. So this is a privileged instruction. A trap instruction is privileged. Ordinary programs, ordinary processes may not execute a trap instruction themselves. All right, so what are we going to do with that? Well, we'll see in a moment. Basically, the trap instruction is intended to perform a system call. To perform a system call, you have to be in the operating system. So the trap instruction is going to take us in the operating system. Now, of course, if we're doing a system call, there may be hundreds of different system calls available. Some open files, some create new processes, some increase the amount of memory that you've got available to you. They do all kinds of things. So you probably want to specify when you are trying to get the system's attention to do a system call, this is what I'd like you to do. Of the many hundreds of things you could do for me, here's what I'd like you to do. And many of those, of course, have parameters associated with them. You want to open a file? Which file? Do you want to open it for read? Do you want to open it for write? Do you want to open it for read/write, et cetera? So there may be some parameters associated with many of those system calls. Okay, so typically, you're going to have associated with a trap instruction, a few conventions, linkage conventions, things that will link up what your process wants to do and what the operating system is gonna do for you. For example, you may say, every system call will have some number. Great. Before I do the trap instruction, I will put the number for the system call I care about in R0. If there are parameters, if there are arguments in this system call, I will put the arguments in a particular place, and R1 might contain the pointer to those arguments. And of course, system calls can have different outcomes. The system may say, yes, I have done this for you. Here is your result. The system may say, you may not do that. Here is my indication that you can't do that. So you're going to get a return value in many cases. The return value would perhaps be put in R0. OK, so in order to do a system call, You load up whatever registers are used for these linkages, which may not be R0 and R1. It depends on the particular architecture. And then you call the instruction, the trap instruction that you're not permitted to call, permitted to execute at least. What's that going to do? Well, what always happens if a non-privileged process tries to run a privileged instruction is you trap to the operating system and say, what should I do about this? It doesn't matter which privilege instruction you run, you're always going to trap the operating system, say, what should I do about this? The operating system is going to say, okay, this process tried to do something he's not allowed to do. He tried to run a privilege instruction. Which one? Oh, it was the trap instruction. I know what that means. That means he wants me, the operating system, to do something on his behalf. What should I do? Well, R0 probably contains a number saying what I should do. R1 probably contains a point or two arguments of that system call. Now I know what to do. So what's going to happen here is we are going to, when we run a trap instruction from our process, bang, it's going to instantly go to the operating system and do some work. It's going to figure out what to do. And based on exactly which system call we want to perform, it's going to make use of what's called a gate into the operating system. Gates into the operating system are pieces of code in the operating system that handle particular system calls. After the system call is complete, we're going to come back to the process. Let's take a look at this in a little more detail. So here's an application process. It's running along. It's got a bunch of instructions. Because it's an application, it's running in user mode. It's not running in supervisor mode. Supervisor mode is the mode in which you can run privilege instructions, typically only available to the operating system. So we run an instruction, run another instruction, ordinary instruction, just do them, limited direct execution, then we hit this trap instruction. The trap instruction is a privileged instruction. We may not, in user mode, perform privileged instructions. We have generated an exception. What happens? Well, there are many, many types of exceptions, and depending on which particular type of exception it is, we're going to want to do different things. If it's a segmentation fault, that's different than a trap instruction. Both, however, are different kinds of exceptions, so we're going to need to distinguish which it is. The trap instruction will cause us to go to what's called a trap vector table. Trap vector table contains a bunch of processor status words and program counter values. They say, "Here's what you should do for this particular kind of exception. If it's a segmentation fault, here's what you should do. If it's a trap instruction, meaning you want to do a system call, here's what you should do. So let's follow what happens if it is the trap instruction. You go to the trap vector table, find the entry for the trap instruction. This is done in hardware. You don't have to write code to do this. It's done by the hardware. And it says, ah, OK, here's the program counter and the processor status word. Oh, by the way, I'm now in supervisor mode. I can run privilege instructions. So it loads up the program counter processor status word on the core in which you try to do the trap instruction. Then it goes to a piece of code that's called the first level trap handler. Now, the trap instruction, of course, could have been called in order to call any one of hundreds of different system calls with very many different parameter values. So we don't know which one it is yet. All we know is that it was a trap instruction. So the first level trap handler is going to look at that and say, OK, fine. I know I'm in the first level trap handler. That's what the program counter of the processor status word has taken me to, that's the code I'm running. What's that code going to do? Well, it's going to try to figure out which system call are we trying to perform here. How's it going to do that? It knows that, for example, in this particular hardware, r0 contains the number of the system call, r1 contains a point of the parameter. So knowing what the number of the system call is, I will go to a second table. This table is going to say, I figured out it's a system call. Therefore, I figured out of the system call number 17. I will go to the 17th entry in the system call dispatch table, different table than the trap vector table. And this will say, OK, oh, this is an open call. He's trying to open a file. Fine. I need to go to the code in the operating system that handles open code. That is the second level trap handler. This is going to do all of the code that is necessary in order to perform the open system call. That is the trap gate, the thing that says, Here's the way into the operating system. All right, so now after that, you're gonna do all that code. You may do a whole lot of operating system code, you may do very little, whatever. You're going to return then to user mode because you're finished. And what are you gonna do then? You're gonna go back here and say, "Oh, I run the next instruction, and then I run the next instruction and the next instruction." Now, this whole process is called trap handling. Trap handling is done partially in hardware and partially in software. The trap instruction is built into the hardware. It does what it does. It does what it's designed to do in hardware. It takes you to the trap table. It changes you to supervisor mode. It looks up in the trap table, the fact that, oh, this is a trap instruction. It then goes to the first level trap handler, which is a piece of code at this point. Now, it's already got the processor status word and the program counter. It has to have saved, of course, program counter in the process status word of whatever process called the trap instruction, because you're going to want to go back there eventually. You need to save that information somewhere. Where do you save it? Put it on a stack. Great. And then after you put it on a stack, you're in supervisor mode. You then say, okay, I need the new program counter and processor status word to get me to the first level trap handler. All right. So that's the hardware portion. Now we are dealing with the software portion. From this point onward, we're doing ordinary software, except we're running in supervisor mode. First level handler says, well, there were a bunch of other registers. There's R0, R1, all the other registers. We need to save those registers, because when we go back to that process that performed the trap, it needs those values, those registers sitting around. So let's save them. It'll put them on the stack. It'll figure out, OK, this was system call 17, and it was for a parameter, oh, it's a parameter slash temp slash foo, it's open file for temp foo. It gets that information available, and then it's going to call the second level handler, the one that is appropriate for 17, the open call in our example. Second level handler is then going to say, okay, he wants to do an open. What does he want to do an open on? Temp foo. All right, fine, I'm going to start doing everything I need to do in order to perform an open of the file temp foo. Now, there could be a whole lot of stuff going on here. Many, many, many different things could be happening. So it could be a lot of code. Now, let's talk a little bit about that stack. I already said we're going to, when we do a trap instruction, throw things onto the stack. Where am I going to throw them? What stack am I going to throw them onto? Well, the code that handles the trap is just code. It runs in privileged mode. It's just code. So it requires a stack to run, because all the code, including the operating system code is stack-based. Therefore, there must be a stack somewhere that's gonna run this code. And this is important because ultimately, we're gonna go to the second level trap handler and that's gonna go off who knows where. It may go through 73 different routines in the operating system code and who knows what it's gonna do. So we'd really like to have a whole stack that we can use to hold everything that's gonna have to be done to handle this system call. Great. All right, so we're gonna have a stack somewhere. Well, we have a stack already. The process that we're currently running, the process that performed the trap instruction, it's got a stack. Great. Now, we don't want to lose track of what's happening on that stack because we're going to go back there. So we're not going to reset it. We could, of course, just push stuff onto that stack, but we are probably, as we do these 73 or whatever it is, different routines within the operating system, we're going to perhaps get at a bunch of data that is really not suitable for the user seeing. Once we put something onto the user's stack, his ordinary stack, it's on his stack. And later, if he has malicious code, he could try to look at that stack, but it's readable, and he could get some of the data that we put onto his stack, even though we've, in theory, at least popped it all off. When you pop a stack, you typically do not zero out the values of the frame that you popped. So we'd prefer not to have that happen. So what do we do? What we do is we say, let's have a second stack for this process. Let's have two stacks per process. One, the user mode stack, two, the supervisor mode stack. As long as you're in limited direct execution, you just run the user mode stack. That's it. And just keep doing that. However, sooner or later, you hit one of those system calls and go into the supervisor mode. Now we will start using the supervisor mode stack for this process. So we'll save his user mode program counter and processor status word. The trap hardware had to do that somewhere. Well, okay, here's where we're going to save it. We're going to save all the other registers, the user mode registers. We talked about how the second level track handler was probably going to do that. And then we're going to have to have the parameters to the system call handler put on the stack because we're about to call that. And we have to keep track of where do we return to. The return value is not going to be at this moment, the return value of the process itself, not the one we just saved in user mode there. It's going to be, hey, we're in the middle the second level trap handler. And then we call the system call handler, whichever particular piece of code, whichever gate we go to. And then we set up a stack frame for that. And we go off and we do our 73 things. And gradually as we finish all 73 of those routines, we pop off their various values from the stack, and we'll eventually say, oh, time to go back to the second level trap handler. Let's save all that information. Let's restore the user mode registers. let's restore the user mode PC and bang, we go back to the user mode, making sure of course, that we get out of supervisor mode before we do so. So, the return is just the opposite of the entry. Second level handler returns to the first level handler, the first level handler restores the registers from the stack. It uses the fact that at that moment, the first level handler is still running in privileged mode. In privileged mode, you can change to unprivileged mode anytime you want to. There's a privilege instruction that does that, so you can do that. So it returns to privileged mode and it restores the PC and the other registers and bang, back you go. Once the PC has been restored in non-privileged mode, then you can go back to the code that the user is running back to limited direct execution. Okay. I hope that's clear. This is fairly important to understand. So if this isn't clear, this is a good place to ask questions. ask questions of me, ask questions of the TA, put questions on the Piazza page, whatever. But if you do not understand what I just went through, I strongly recommend that you ask some questions. The readings also cover this, so you can also go back to the readings and figure out what's going on here by carefully going through the readings. OK, now something else. There are various events that we ask for the operating to perform, some of them the operating system can do instantly. So we want to lower the priority of our process. What's that mean? Well, that basically means you go to the process descriptor for this particular process, you change value. That takes a very little amount of work on the port part of the operating system. So let's do that and it'll return. On the other hand, what about that open file call? Well, the open file call probably under most circumstances is going to require us to do some IO. we're going to have to go out to the disk. The disk is slow. The disk is very, very, very slow compared to our CPU. We do not want to sit around waiting forever for that to happen. So the process may have to wait. Once it's opened the file, it can't do anything useful till the file is open. The operating system does not have to wait. And in particular, if this process can't run because we're waiting for an open call or later for a read call where we're going to read data from the open file. We don't want to just sit here in the operating system spinning our wheels just saying, "Wait, wait, wait, wait, wait." What we'd like to do in the operating system, sometimes even in the context of processes themselves, is say, "Okay, we know this is going to take a long time. Let us keep track of the fact that this has been requested. We've asked the disk to get us this data. And we know sooner or later it will. We know it's going to be later, a lot later. Meanwhile, let's go do other stuff. So we'll go off and do other stuff. However, sooner or later, it will be finished with whatever we asked it to do. And we want to be told, I'm done. Hey, you wanted to read this block of data off the file. Here's your block of data. It's available. Now the process that wanted to read that block of data could run again, because now it's got its data. So what we want to have here is an event completion callback. We want the operating system in this context to say, "Okay, this guy wants to read this data. I will ask the disk to read the data. I will keep track of the fact that process A wanted to read this data, and that I'm waiting for this data to occur before I can run process A again. So when that data comes in, hey, I would like to be told the data you asked for has arrived. Do whatever you're supposed to do," which in this case is probably run process A again. That's called an event completion callback. Now this is very, very commonly used in operating systems and is also reasonably widely used in programs themselves, in applications. Basically this works because we have what are called interrupts. Interrupts are in some ways similar to traps. Traps occur because of something happening on the CPU. Interrupts occur because of something happening on a peripheral device, such as the flash drive that you wanted to do that read of the data from, finished reading the It will know that it was supposed to read this block of data, it read the block of data, it made the block of data available to the system in the way it should. Now it wants to tell the system, "Your block of data is available." How does it do it? It generates an interrupt. The interrupt is a hardware signal. It goes across the bus. The CPU is capable of accepting that interrupt. What we mean by accepting the interrupt is it says, "Oh, I am not going to perform the next instruction I was supposed to perform. Instead, because I got this interrupt, I'm going to go to a routine in the operating system that says, here's how I handle the interrupt. What kind of interrupt is it? Oh, it's an interrupt from the flash drive telling me that a block of data has been read. Well, all right, fine. Now I need to go to the event completion callback related to that particular block of data and figure out what to do with a block of data. Provide it, for example, to the process that asked to read it. So this is most commonly associated with I/O devices, but also with one other very, very important thing. Every CPU, every modern CPU has built into it a clock. The clock ticks and ticks and ticks and ticks. This is not a wall clock that says, it's 12.03 PM on Tuesday. This is a clock that says, how much time has expired? It's essentially like a timer, but it's running all the time. The CPU and the operating system on behalf of that can request that the clock generate interrupts. When? It can say, I'd like to have an interrupt in 10 milliseconds from now, 20 milliseconds from now, 2 milliseconds from now, whatever it wants. And after that amount of time has passed, the clock will generate an interrupt. This is an interrupt like any other. It goes to the CPU. The CPU says, I will stop doing whatever I was doing. I will instead deal with the interrupt. What interrupt is it? It's a clock interrupt. I do whatever I do with a clock interrupt. We'll talk about what we do with clock interrupts later. Now, the operating system every so often needs to tell a process something. Something's happened. So it's got signals, abilities to send a signal to a process saying this happened. So processes, to some extent, are able to control what they do when they get these signals. There are defaults. If you write a process that doesn't say specifically what to do when a particular kind of signal arrives, the default happens. But if you do write such code, you can do something about it. Now, if you do something about it, what choices do you have? You can ignore it. You can say, oh, sure, fine. It was an exception fault. It was an arithmetic overflow. I don't care. Or you can say, because this is a signal indicating that we're about to lose power in a few milliseconds, I need to do some vital cleanup. I'm going to do the vital cleanup. I got code that does that. Or I can just say, well, whatever it is. If this is a fatal thing like a control C, fine, kill the process. I don't even do anything. I don't even have to say what I do because it'll do the default. Now, these are a little bit like hardware traps and interrupts, except again, they are not generated in the case of interrupts by necessarily a peripheral device. They're not generated in the case of a trap by the code of the process itself. So for example, I can from my process that is, let's say, my window manager, I can generate a signal that says kill this other process. I was running the following program and it appears that that program is in an infinite loop, it's not responsive, I'd like to kill that process. I, window manager, can then ask the operating system kill that process. If that's one of my child processes, I may be allowed to kill that process. Then the operating system will send a signal to the process saying die. Unless the process has been written to do something special, when it gets a die signal, it'll terminate, cord up. So these are things that the operating system can send, as signals, to processes that the processes must either handle by default, or perhaps in some cases, may have special code that has been written for that process to handle that particular signal. OK, now processes, as we've said several times, have state. Who manages the state? Well, it's a shared responsibility. The stack. Essentially, the stack is the processes own responsibility. Now, if it just does ordinary things in ordinary programming languages, it's going to have default code built into the programming language that says, here's how you handle your stack. However, the stack is read/write, which means the process can figure out, this is where my stack is located. This is what I currently have as my frame pointer, point of the current frame. If I want to, I can rummage through my stack looking at all kinds of things. I can change whatever I want in my stack. I can screw up my stack so it's no longer a proper stack. I can do that. Of course, I can do the same thing as a process with my heap, with my data area. Those are my responsibility as the person who wrote and is running this process to handle properly. If you fiddle with your stack, if you fiddle with your heap and you're not careful about what you do, you are permitted to screw them up any way you'd like to screw them up. Now, if you do so, in particular if you start fiddling around with your stack in improper ways, the chances are excellent that you are going to do something wrong and your program is going to crash or is going to exhibit vastly unpredictable behavior, which is probably not good, but you can do it if you want to. So you have to be reasonably careful about such things. As a rule, you do not want to go into your stack segment and do anything with it. Allow the stack management code that's been built into your programming language, and of course, the hardware support for it, to do the right thing with the stack. That's what should happen. Now, on the other hand, there are things that are not even under the control of the process itself, but are specific to the process. For example, most of the contents of the process descriptor. You cannot go to your own process descriptor and say, change this field. You can't just change it. You can ask the operating system to change some of those fields. Perhaps the operating system will do so. It's a system call. But you cannot do it yourself. So things like, what's my memory? You can ask the operating system to change what memory is allocated to you. You cannot change it yourself. What file have I got open? The operating system has a bunch of files open. You can ask the operating system to open new files. You can ask the operating system to close files you have open. You cannot go in there and say, oh, now this file is open by just changing a value in the process descriptor or an associated data structure. You can't touch the supervisor stack. The supervisor stack is specific to you, but it is not readable or writable by you. It's only readable or writable when you were in the privileged mode, which means only when the operating system is running. And there are a bunch of other things that are in that data structure. So for example, you can't say, well, I know I've actually used 5,000 CPU seconds, but I'll pretend I've only used two by changing that value. You can't do that. OK, now one important aspect of processes is are they blocked or are they not blocked? Now, what do we mean by that? Well, sometimes a process is ready to run. It doesn't mean it's running, necessarily, but it's ready to run. The operating system could put that process on a core and it could start executing instructions and limited direct execution. On the other hand, sometimes a process is not ready to run. So for example, at a file open, I said I'd like to read 100 bytes of that file. Commonly, I don't want to do anything until I've got that 100 bytes. I want to not move on to my next instruction until I have my 100 bytes. And as we just discussed, that could take quite a while. So I can't run that process till I have my 100 bytes. And it would be nice if the operating system knew that. So typically what the operating system will do in such circumstances, it will say, is that it will say this process is blocked. And what we mean by a blocked process is a process that should not be run. It should not be loaded onto a core. This is a piece of state information. In the process descriptor, there will be an indicator of is this process blocked or is this process not blocked? If the process is not blocked, it is eligible to run on a CPU core. It could be run in limited direct execution. If the process is blocked, it is not eligible to be run on a CPU core. It should not be run, it can't be run because it can't do anything useful. Okay, so there are various reasons processes get blocked. It could be waiting for IO. It could be waiting for a message to come in from an outside thing. It could have said, I'm doing inter-process communication with somebody else and I'm waiting for the guy to tell me what he needs to know. There are various reasons a process could be blocked. The operating system keeps track of whether a process is blocked by making an indication in the process descriptor. So why do we block processors? And what is the effect of blocking a processor? Well, we block them because they can't run, which means, of course, we don't want to run them. How do we prevent them from running? We make sure the scheduler, the element of the operating system that decides what should run on each core at each moment, we tell the scheduler, do not run this one. There'll be an entry in its process descriptor saying, it's blocked. And the scheduler will then know, don't run that one. Don't choose to run it, choose somebody else. Of course, who gets to block it? Anybody can block it. Well, not anybody. You can't block somebody else's process typically. Processes could request that they themselves be blocked. More commonly, the operating system says, I'm doing something on behalf of this process. It cannot run until it's complete. It's not complete yet. I, the operating system, am blocking this process. As I said, a system can-- process can block itself by using a system call saying, please block me. It is very necessary to make sure that if you block yourself, somebody's going to unblock you. Because as long as you remain blocked, you're not running. And if you're blocked, you can't unblock yourself because you're not running. You can't run another system call say unblock yourself because you're blocked. So somebody better unblock you. You better have a way of making sure that you get unblocked. So generally speaking, what really typically happens with blocking is there's a resource manager, something that handles, for example, the flash drive, the network card, the microphone, the speakers, whatever it may be that you're interacting with. And the code that handles this says, OK, somebody asked to do something and it cannot be done yet. I will tell the operating system, block this process. Well, actually, at this point, we're talking about code that's in the operating system. So it can directly block the process if it chooses to. And at that point, it is going to say, I now, the resource manager, take responsibility for this blocked process. I've blocked it. Presumably, I had better unblock it eventually, because chances are if I don't unblock it, nobody else will. So if I'm resource manager handling the flash drive, when that data that was supposed to be read from the flash drive is read and is ready to go, it is important that this code that represents the resource manager for the flash drive says, "I remember I blocked process X. I must now unblock process X." Okay, and typically at this point also, whoever it was who did the blocking will say, "If I unblock something, then maybe we should be doing something a bit different in terms of who's running than what we were doing a moment ago. This process that just got unblocked a moment ago, it couldn't run. Maybe it's the most important process to run, but it couldn't run. Now it can run, and maybe it's still the most important to run." So in that case, maybe I better ask the scheduler, "Given that I just unblocked this guy, do you want to do something different? You want to make different choices about which process is running. So generally speaking, if a resource has been requested, it becomes available. Whoever notices that it becomes available, the resource manager most commonly will unblock the process that was waiting for that resource. OK, so this is the basic information I intend to cover on processes. In conclusion, processes are a fundamental interpreter abstraction provided by the operating system. It is an abstraction. This isn't built into hardware. Things that support it are built into hardware like the trap table, but the process itself is a operating system software-provided abstraction. The operating system creates these. Typically, almost all processes are created because an existing process asks for them to be created. Once we have created a process, the operating system will have a process descriptor that represents that process, and all of the further interactions between the process and the operating system will make use of this process descriptor to do the right kinds of things. We can create processes in different ways from scratch with the create process call in Windows, for example, using the fork mechanism that is typical in Linux. Generally speaking, processes try to do as much of the work themselves in user mode as they can when they require services that cannot be performed in user mode that require the use of privileged instructions, most commonly, though not always, because they're going to require interaction with a hardware device, such as a flash drive or a network card or the screen or whatever, they're going to make a system call in order to obtain that access. The system call will transfer instructions, will stop the limited direct execution, and will ask the operating system to do something on behalf of that process. The operating system will typically block that process until whatever it was that needs to be done is done. Okay, in the next class we'll start talking about something I've alluded to a few times in this lecture, scheduling.