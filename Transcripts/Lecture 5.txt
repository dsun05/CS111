Today, we're going to start talking about memory management, one of the most important activities performed by an operating system. We'll talk about what we're really trying to do with memory management. What's the purpose of it? How do we have to go about achieving it? What the goals are? And then we'll start talking about a number of strategies that have been developed over the decades on how to manage memory for a computer system. We talk about fixed partition strategies, dynamic partition strategies, buffer pools, garbage collection, and memory compaction. In the next lecture, we will move on to modern memory management techniques involving demand paging and virtual memory. Okay, but what are we talking about in terms of memory management at all in the context of operating systems? Well, the memory we're talking about managing here in this lecture and the next lecture is RAM, the random access memory. Now, according to the von Neumann model of computing, which is how all of our typical computers are built, you have to have RAM. You store your instructions, the things that are to be performed in RAM, they are fetched from RAM and run to the CPU. You store various types of data in RAM. You keep your stack in RAM. So every process that wants to run, and for that matter, the operating system itself, has to have RAM available to it in order to properly run. Without RAM, it can't run. Okay, now, if we had vast amounts of RAM in a typical computer, more than anybody could ever possibly want, then it would be an easy problem to deal with. But we don't. RAM is expensive. It's expensive in terms of it being hardware you have to buy. It's expensive in terms of taking up space in a computer. It's expensive in terms of burning power. So you can't really afford to have all that much of it. Now, over the decades, of course, How much you can afford to have has increased as we've been able to shrink the size of RAM cells, lower the cost of purchasing the chips and so forth. But still, we are stuck with a limit on how much RAM we can have. And in particular, for any given computer that you've got, that computer has a certain amount of RAM in it. It's got that much. It doesn't have one byte more than that. That's what it's got. Unless you open up the computer and start fiddling around with its innards and its hardware sense, you're going to be stuck with that much RAM. That's how much you have to work with. Now, on the other hand, as I've also said in other lectures, we are running a whole lot of processes at the same time, hundreds perhaps, on a single computer. Every single one of them needs RAM, as does the operating system. So what's probably going to have to happen here is we're going to have to have some effective way of sharing the limited amount of RAM we've got with all of these different processes that have their requirements for RAM. Now, what we'd like to see happen here achieving certain goals within the context of memory management. First, we want to try to maintain the process model of how things are being run in the computer, how we do interpretation of instructions, how we provide services for the user. That model says, well, every process should have its own segregated address space. It should be able to do what it needs to do within its address space, but it should not be able to interfere with other processes' address space, and other processes, in turn, should not be able to interfere with its address space. To the extent that we are sharing all the memory that we've got in the computer with various processes, we want each process to be pretty much unaware of what is happening in terms of memory management with the other processes. It wants to have access to its memory. It doesn't want to know or worry about what's happening with the other processes. We need this to be efficient. If we are going to make decisions on a fairly frequent basis about which process gets which piece of memory, then that's going to have some overhead costs associated with it. And we want to have those costs be low. We don't want to have a whole lot of costs for running allocation and deallocation strategy, saying you get this piece, you lose that piece, that kind of thing. Further, because we only have a limited amount of this memory and we have a lot of needs for it, we want to make sure that the parties that we give the memory to are making effective use of their memory. We don't want to devote a significant quantity of our RAM to a process that really isn't using it, just says it wants it, but isn't doing anything with it. That would be inefficient. That would be a poor utilization of our limited resource. And of course, we also want to have protection and isolation. We want to have mechanisms we can rely on, mechanisms that we aren't saying, well, your process shouldn't look here, don't look there, and hope it doesn't. We want to prevent processes from looking at each other's RAM, from looking at the operating system's RAM, and particularly from changing other processes or the operating system's RAM. That data should be only accessible by the party that should access it, either the operating system or the process that it has been devoted to. OK, so we're talking here for the moment, and we'll be talking primarily throughout this lecture, about physical memory allocation. By physical memory, we mean the actual chips that hold the memory cells, the RAM chips. So this black box, in some sense, represents all of the physical memory in a particular computer. And we are going to use parts of that physical memory to hold different things for different parties. So the yellow block here is the OS kernel. We're going to have to devote some space in our memory to operating system needs. That will include the instructions of the operating system itself, stacks for the operating system, and any heap or any other data areas. It doesn't really have a heap, but data areas that will be used by the operating system. Then, of course, we're going to run some processes. In this simple example, we have three processes running-- process 1, process 2, process 3. Each of them is going to have to have a data area and a stack. And those should be private to those processes. They should get it. Nobody else should get it. Now, if it turns out that two of these processes are running the same program-- let's say process one and process two are both running segment A, then they could share the same piece of code because remember, we're not going to have the code be writable. The code is going to be execute or perhaps read execute only. So if it turns out process three is running a different piece of code, then we need that. But if both of these, that would be segment B down there in the lower right. But if it turns out that both of these programs, the ones being run by one and two, the one being run by process three, uses the same shared library. We only need one copy of that shared library. Call it shared library x. That can be shared because it's execute only. So we're going to be using different pieces of the physical memory to hold different things for different parties. Now, as I said, we're talking here about physical addresses. Mostly. We will eventually, at the end of the lecture, to start talking about virtual addresses. It's important to know that there is a distinction between these two. They are both important. They're different. So what is the difference? What's a physical address? That is an actual location on a RAM cell, somewhere on one of the RAM chips that has been installed in the computer. Location one million on the particular RAM chip that holds the one millionth word. So that is a physical location. It's genuinely physical. You know, of course, in actuality, it's electronic on the chip. But you can go to the chip and you can say that's the chip that is holding location one million. And you're right. It is if it's a physical address. Now, originally, when computers were developed in the 1950s, we use physical addresses only. Those are the only types of addresses we have. And so when we said to a process, you have address 1 million available to you, we meant address 1 million, the physical address on that particular chip. That isn't what we do anymore. We're going to, when we get to the second lecture on memory management, the next lecture in particular, start talking about a different type of address, a virtual address. The virtual address is not a physical address on the chip. If your process is storing some data in virtual address million, then of course your process will use virtual address one million to refer to that data. But that will not necessarily, probably will not be location one million physical address on that particular chip. It's perhaps on another chip at a different physical address or perhaps not even on one of the RAM chips at all. Maybe it's stored somewhere else. So that should imply to you that that somewhere along the line we're going to do some translation, virtual to physical. So the reason we are going to do that, as we will see when we go through these lectures, is virtual addressing gives us a tremendous amount of flexibility that we don't get from using purely physical addresses. But it has requirements. In order to do it properly, with good performance, we're going to have to do things in a particular way. We're in fact going to need some special hardware support. But as I said a couple of times already, that's not really for this lecture. That's going to be for future lectures, except at the very end of this one, we'll begin to talk about virtual addresses and the translation mechanisms. So what in designing the memory management system for our operating system do we have to consider? What are the aspects of the problem? Well, every process needs a memory, but unfortunately, as a rule, processes cannot perfectly predict how much they're going to need, which implies that either they're going to ask for a whole lot more than they will usually need to be safe, or alternately, we have to, during the run of the process, add more memory to their allocation if that is what the process needs. It's also the case that if we tell some process, you have a particular variable, variable x, located at this location, this memory location. If that process says, "I want to see what's in that memory location," we have to give him whatever he put into that memory location the last time. We can't say, "Well, somebody else came along and used that memory location. Somebody accessed your memory and changed it. Oh, you know, it used to be somebody else's memory location, so you get to see the old thing they've got instead of what you want to see." No, we can't do that. If we give an address, if we tell a process, "You have this address," and it writes something in in that address, or it just has something in that address like an instruction, which it didn't write itself, it has to be there when it goes back. Another aspect we have to be aware of is if we added up the requirements, the true memory requirements of every single process that we were currently running, plus the operating system's own requirements, the total amount of RAM required would almost certainly be greater than the amount of physical memory that we have available. OK, so that's going to lead to some tricks that we're going to have to worry about, and we'll talk about those later. Further, at any given moment, we have perhaps on old computers one core, on modern computers several cores, which means one or perhaps a few processes are currently running, but as we talked about in the scheduling lecture, we're going to change which processes are are running on which cores. A process running on a particular core is going to need to have access to its own memory. If we switch to a different process on that core, the new process needs to have access to its memory. And because we are going to be shuffling things around, shuffling things around kind of implies you're copying data from one place to another. OK, well, you can do that. That's something the operating system can and will do. But it has performance costs. there are overhead costs associated in copying data. You are going to perform an instruction, potentially, for copying every single word to another location. We can't afford to say, oh, when we start up a new process, you're going to have to copy a whole lot of data, and you can't start that process, can't actually run that process until all that data has been copied. That would be too expensive. So when we switch between processes, we must not run into a whole lot of delays because suddenly we have to copy a bunch of new data into places. Further, we're going to run some algorithms to do memory management. Something is going to be done to say, here's how we allocate the memory. Here's how we shuffle things around, however we do that. And we don't want that to be a high overhead operation. We want that to be relatively cheap, because we always want to limit the overhead that is being performed by an operating system. OK, let's talk about a few of the very basic early memory management strategies. That's the first thing people thought of when they said, oh, we've got computers, we've got memory in the computers, we need to use the memory in the computers, how do we manage it? Talk about fixed partition allocations, dynamic partitions, and a little bit about relocation. So perhaps the simplest thing you can do in terms of managing memory in a computer is fixed partition allocation. So you have n processes running. What do you do? You say I've got m bytes of data, n processes running. I will divide up the m bytes of physical memory that I have among the n processes. I will perhaps choose one partition, one contiguous range of addresses within my physical memory, and that'll go to process one. I'll choose a different one. That'll go to process two. I'll choose a third disjoint partition of memory. That'll go to process three. Each partition is a contiguous range of addresses, and each partition will be given to one process. Maybe I'll have more than one partition per process. Perhaps I'll have two or three. Probably a fairly limited number, but I might have more than one. Now, obviously, if I'm going to say I've got these N processes, I need to make sure that I divide up the memory in such a way that everybody gets what they need, assuming I have enough RAM to do that. So I'm going to then say, What is the largest possible process that I have to allocate? What's the biggest one, the one among the end that requires the most? I'm going to have to make sure that I have a partition sufficiently large when I come around to allocating memory for that one process. I've got to have a partition sufficiently large to give to him. So even if I have in principle enough memory, but I keep dividing it up in such a way that by the time I get to that process that has a big need, there aren't any contiguous partitions anymore, I got a problem. So I can't do that. Now, how do I do this with a fixed partition allocation? Well, what I do is I say, "Okay, I've got this many megabytes or gigabytes of memory. I'm going to divide up the overall amount of memory I've got into pieces of a fixed size." How big? Maybe 4K, maybe 16K, maybe 4K or 16K. I'll divide up the partitions into two sizes. Basically, I will in some fairly static way say I'm taking all the memory that I've got, all of the RAM that I've got, and I'm creating a pool. And that pool will consist of blocks of memory of this fixed size, 4 megabytes, 16 megabytes, whatever it might be. I can have one size, I can have two sizes, maybe three or four. I'm not going to have too many. But what am I going to do when it comes time to say, here's a process, it wants to have partition of memory. What I'm going to do is say, "How big a partition?" And then I'm going to find a partition that has not yet been allocated to any other process that is at least big enough to hold its requirements. It says it needs three megabytes. I've got a four megabyte partition. I can allocate the four megabyte partition to that process's three megabyte requirement. Each partition that I allocate will belong to one and only one process. I'm not going to split up the partitions, I'm not going to reuse or otherwise reallocate portions of a partition that a process isn't using. So, if I give a four megabyte partition to a guy who only needs three megabytes, I will not take the one remaining megabyte in that partition that he isn't going to use and allocate it to somebody else. The four meg is his even though he only needed three. Now, there are advantages to this approach. It's really simple. It's a very, very simple thing to implement. So back in the very, very old days, 50s, early 1960s, you might very well do this because you were doing what was called batch processing then. You were basically saying, run this process, when you're done with this one, run the next process, when you're done with that one, run the next process. You didn't actually have a whole lot of processes running at the same time. That made it very easy to use this kind of strategy to allocate memory. Also, the actual allocation and deallocation process was very, very cheap. You'd basically have a bitmap representing all of the partitions that were allocated and all the partitions that weren't. If the bit for that partition was zero, it wasn't allocated. If the bit was one, it was allocated. Easy to find a partition that was not yet allocated. Easy to change its status from being unallocated to allocated or back again when you deallocated. Also, if you knew exactly what was going to happen on on your computer, and that was what was going to happen. Nothing else was ever going to happen, because perhaps it's a special purpose computer designed to do one set of things, such as a set-top box on top of your television. Then you could usually figure out, OK, here are the partitions that I'm going to need, because I know which processes I'm going to run. I know how much memory they need. I know ahead of time exactly what everybody needs. I can use a fixed partition strategy. So if I do this, I do need to enforce the partition boundaries. I gave a guy a 4 megabyte partition, and he said he only needed three, but I've given him a 4 megabyte partition. What happens if he actually starts running through that partition, and he gets to the end of the partition and he tries to access the next byte, the next word, after the 4 megabytes I gave him? I didn't give him the partition that comes right after the one that he was supposed to use. That belongs to somebody else. I must not allow him to access that other partition. He should only be able to access the partitions he has been granted by the operating system. OK, so I got to enforce the boundaries. I got to enforce the top, the low address, and the bottom, the high address within the partition that has been granted. So how do I do that? Well, the good way to do that is to use hardware. Because if I use hardware, if I build it into the CPU, then it will happen relatively quickly. I won't have to run multiple instructions to say, Should I give him this memory or not? I won't have to do access to a whole lot of stuff that's off of the CPU chip. Maybe I can build this into registers on the CPU chip. In fact, that's what I would do. I would set up-- and this is what they used to do in these old systems-- I would set up registers on the chip that say, these registers represent the high and the low address for the partitions you have been allocated. You may use these addresses. Since it's built into hardware, if the operating system is careful to say whenever process A is running, I will set the registers for its partitions, one or perhaps a few. And every time it issues an address, the hardware will check the address that was issued against the partition registers to see if it was within the partition range that this process is supposed to use. If it isn't, I will reject his attempt to access that particular memory address. Now, this is all done still in the basic scheme we're talking about with genuine physical addresses, no virtual addresses yet. So in a little bit more detail, here's what it looks like. So down at the bottom, we have our actual computer. It's got a processor, it's got a disk, it's got RAM and memory, it's got a network, it's got other things I'm not showing here. Now, the process one here that's running is under the impression, we try to maintain this impression, we try to give it this impression and make sure it certainly continues to appear that way. Process 1 is under the impression that it has exclusive use of this computer. It thinks it's got exclusive use of the processor, exclusive use of the memory, exclusive use of the network and the disk and the other devices. It doesn't, but it thinks it does, and we want it to look that way. OK, so what are we going to do with the memory? Well, perhaps we have decided that we're going to allow this process to have three disjoint partitions, indicated by the blue here. So this is memory that the process is allowed to access. All right, so these three partitions. In order to do this using the partition concept and using partition registers, we would have three sets of partition registers. Each partition register would have two values, a start and an end of the partition. So each of them for one of the three partitions would have a pointer, an address saying, here's the start, here's the end, here's the start, here's the end. This is built into the hardware of the CPU. So any time that the process, when it's running on the processor, issues an address, there would be a check against these three sets of partition registers. And we would say, is the address within the range of one of the partitions we have actually allocated? We know what they are because they're what are in the partition registers. When we start as the operating system to run this process, we say it's your turn to run based on the scheduling, as we discussed in the last class. One of the things we're going to do is load up the partition registers of the CPU with the appropriate values for the particular process we choose to run. OK, so simple enough. It's got some serious problems. In the first case, it presumes that you know ahead of time how much memory you're going to be using, because there will be a fixed number of partitions possible, like three. You might have each process can have three partitions. You're going to have to set at the time that you start the process. when you first create the process, you're gonna have to set the particular sizes of the partitions that you want. If you want more later on, if it turns out that you underestimated what you needed, well, that's too bad. You can't get more while you're running. Also, of course, in order for this to work, because we are using physical addresses and we're not doing anything spiffy, anything exciting, you're going to be limited in how many processes you can support. There's a certain amount of RAM. Each process is gonna require one or more partitions. You add up the total amount of memory for all the partitions, for all the processes you're trying to run, it cannot be greater than the amount you've got. In fact, it'll have to be a lot less because the operating system itself will require some of that memory. It's not an easy thing to share memory this way and fragmentation causes inefficient memory use. Remember, we want our memory use to be efficient because we have a limited amount, we need to make the most efficient use we have of this critical but limited resource. What do I mean by fragmentation? Well, all memory management systems have some degree of problem with fragmentation. Fixed partitions have a very, very bad problem with fragmentation, but all of them have something. So what's the problem with fragmentation? Fragmentation causes memory that you have not yet allocated to be unusable. For one reason or another, nobody can use the piece of memory because of how you have chosen to do your memory allocations, because there are inefficiencies in the way you do it. If you have a very, very high degree of fragmentation, then you are not going to be able to support nearly as many processes as in principle you could. You could add up the actual memory requirements of all of this set of processes, and you could say, oh, I've got that much RAM available. But because of fragmentation, you will not be able to allocate the requirements of all the processes. There will be portions of your RAM that are not being used by anybody, but that cannot be used by anybody else. So how could that happen? Let's take a look at an example in the context of fixed partitions. So let's say we have three processes, A, B, and C. And A requires 6 megabytes, one partition of 6 megabytes. B requires one partition of 3 megabytes. C requires a partition of 2 megabytes. Great. Now we're doing fixed partition allocations. Let's say that in this particular computer we have two sizes, eight megabytes and four megabytes. And as it happens, based on all the other requirements that we've already been meeting for other things, like the operating system, what partitions do we have available that are not yet allocated that we could use to meet the needs of these three processes? Well, we have one eight megabyte partition and we have two four megabyte partitions. At the moment, this looks pretty good, because you could easily examine this and see, yes, we can manage this. So there's partition 1 with 8 megabytes. Let's put A in there. A requires 6 megabytes. This is the only partition we've got that's big enough to hold A's requirements. So we'll put A into this partition. And there he is with his 6 megabytes in an 8 megabyte partition. Then we have partition 2, 4 megabytes. fit B, so we'll put B in there. And again, B is using three megabytes of that. And then we have partition C, which requires two megabytes. And we'll put that in the third remaining partition. Two megabytes there. Okay, so what's wrong with this? We've satisfied the needs of all three processes. But we've wasted some memory. Because we've allocated fixed partitions, and we've said, "Hey, A, you can use anything in this eight megabytes. "B, you can use anything in this four megabytes. "C, you can use anything in this four megabytes." Because we have done that, we now are saying, assuming that A actually uses what it says it's gonna use, there's two megabytes that's not available within that partition. Nobody's using it. A isn't using it. We can't allocate it to anybody else because it's part of the fixed partition. It's an eight megabyte partition. We can't chop off a two megabyte portion. Same thing happens with the other two partitions. One megabyte's wasted in partition two, and two megabytes are wasted in partition three. How much are we wasting out of the total amount we had available? Two megabytes plus one megabyte plus two megabytes, five megabytes. We had 16 megabytes available. We've wasted 31% of our memory. So if we have a fourth process come in, and it says, "I would like three megabytes, please." We can't manage that fourth process. we cannot give him his three megabyte partition. Because there are no partitions that have that much space available in them. For that matter, even if it was a two megabyte partition, even if process said, "I only need two megabytes," we still couldn't help him. Because even though we have two megabytes unused in partition one, and two megabytes unused in partition three, we can't give him those two megabytes in either of those partitions, because partition one belongs to process A, and partition three belongs to process C. We don't have any partitions left to give this guy, even though 31% of our memory isn't being used by anybody. So, this is called internal fragmentation. Now, fragmentation comes in two varieties, very different types of fragmentation, but both resulting in inefficiencies, wasted space. This one is internal, the other type's external. We'll get to that later. So what is internal fragmentation? Internal fragmentation will occur whenever you are allocating any kind of memory, which will include, for example, flash drives, any kind of memory in fixed size blocks. So if you say it's always the same size or it's always one of this very small set of sizes, you are gonna get internal fragmentation. How much you get will depend on specifics, but you'll get some. So, and the reason that you're getting some is somebody requested a certain amount of memory. You couldn't give him exactly what he requested, so you had to give him more. But because these are fixed partitions, each partition belongs to one party at a time, the more that you gave him, the extra is not usable by anybody else. OK. And it'll be wasted. As long as that party, that process is using that partition with some portion of the partition not being used by him, nobody else is going to be able to use that memory. that memory will be wasted for that long. If you force allocation in fixed size chunks, you will get internal fragmentation. So it's caused by a mismatch between the fixed block size, which you probably determined at some design time or at most at the time at which you started your operating system, and the actual size that processes need. If there is a big difference between those, there's going to be a lot of internal fragmentation. there's very little difference between them, there will be less fragmentation. So maybe if you understand exactly what your processes need, you can avoid very much internal fragmentation. But if you don't understand, you're taking your chances. And if you don't understand, on average, you might be wasting 50% of every block that you allocate, which would be pretty bad. So that's how fixed partition allocation works, what's good about it, what's bad about it. Good things, very simple. Bad things, not very flexible. It's got a lot of internal fragmentation. As a result of these disadvantages, it's not used in most modern systems. But as I said, in special purpose systems where you know exactly what you're going to do, it may prove useful. Now, I'm sure some of you are saying, well, why do we waste that extra space in those partitions? Somebody requested six, we gave him eight. Why did we waste that two megabytes of space? Why did we say only he could use it? Well, we don't have to if we use a different strategy. We can use dynamic partition allocation instead. This is just like fixed partition allocation, except every time that somebody asks for a particular amount of memory, we give him that amount of memory exactly. We probably will round it up to a word. So, you know, if he asks for a million words plus a half a word, we'll probably give him a million and one or something like that. Maybe a little bit more rounding, but very, very little. So this means that everybody gets exactly what they ask for. No less, certainly, but also no more. And clearly, if we're going to use contiguous physical addresses, as we used in the fixed partition case-- and we'll use that here, too-- then we're going to say every partition we give the guy has a contiguous sets of addresses. It starts at this physical address. Every physical address through the end of the partition belongs to that process. We've given him those addresses, the entire set. OK, and we can share these between processes if it's shareable things like, let's say, code. Every process could have multiple partitions. Again, we can say there's going to be three partitions for your process, not just one. And every one of those partitions could have a different size. They don't have to all be the same size. They don't have to be one of three particular sizes. Each one could be exactly what it needs to be. And further, each one can have a different characteristic. So for example, code, we typically don't allow writing of code segments. So we'll set up that particular partition to be non-writable. Executable may be readable. Stacks, of course, have to be writable and readable, but they don't have to be and shouldn't be executable. So we won't set up execution permission on a partition that you are using for your stack, and so on. Now, we are still in dynamic partition allocation, as we will be describing in the next few minutes, using only physical addresses. So there are still some problems with dynamic partitions. One is a problem that we didn't even bother mentioning with fixed partitions, but it had that problem too. They aren't relocatable, which means that if you put a process, if you've given a partition to a particular process, and it's, let's say, put its data area into that process, we can't change our minds about which contiguous set of addresses it uses for its partition. We gave it that one, And it's done a lot of things based on knowing that it has that one, including things that involve the physical addresses. It's very, very hard to move a partition from one place to another using this scheme. It also isn't easily expandable. If a guy said, OK, I need four megabytes, and then later he said, you know, I really should have had six megabytes. Can I get two more megabytes? We can't, in general, guarantee that we will be able to give him the extra two megabytes he wants, because we'd still need to have the six megabytes be contiguous. Unless two megabytes are available right after the four he's got. We can't give that. We can't move him to a new six megabyte partition if there happens to be one free, because he's not relocatable. Also, of course, one thing we can't do here, we couldn't do this with fixed partitions either, is we cannot support applications that say, I need vast amounts of memory. I need four terabytes of RAM. Can't do it because we're going to be giving you a fixed partition of physical addresses and we only have a certain number of physical addresses. We have four gigabytes of addresses. We don't have four terabytes. We cannot support such things. And of course, if we have several different applications, none of them have such immense requirements, but we add up their memory needs for all of them with basic dynamic partition allocation. We can't support more processes than there is physical memory to manage them. And you might say, well, how could you do that? We'll talk about that later. But you certainly can't with this scheme. Now, also, you might say, well, hey, we've gotten away from that fragmentation problem. You know, we're no longer going to give a guy a six megabyte partition when he only needed four. He wants four, he gets four. See, no internal fragmentation. Well, yes. But unfortunately, you remember the second kind of fragmentation? This one's going to have that kind. Now, first of all, let's talk a little bit more about the relocation issue, which is related to the expansion issue. I've got four megabytes, I want six. Can you give me an extra two megabytes? Partitions in this scheme are still using physical addresses so they are tied to those addresses. What you did in the partition, such as what's happening in your stack, it's going to have some pointers in it, like pointers to the next stack frame, the stack frame that you were called from. when you exit the routine you're in, the stack frame you're in is going to be discarded and you're gonna go to the previous stack frame, which means you need to know where that is. So you're gonna have a pointer to that. Okay, that pointer is a physical address. It's a physical address within the partition holding your stack. Well, you can't move that partition somewhere else because if you did, the physical address in the stack frame would be wrong. It would point to the old location, not the new location. You can't do it certainly during execution be difficult to impossible to do it in general. We'll talk more about that. So the reason, fundamentally, is that when we get right down to things, there are going to be a lot of pointers sitting in data areas, in code, in stacks, everywhere. Everywhere's going to have pointers. Now, you may say, oh, I'm using a language that doesn't support pointers. Well, fine. You can't program your own pointers. But underlying everything, there are going to be pointers. There are going to be things that say, this is a jump instruction. Where do you jump to well to the following pointer? This is the stack frame that was called by the other stack frame. Where's the stack frame that called it? There's a pointer to that. There's going to be pointers to the data structures that you store in your heap and so forth. There are pointers everywhere if you look at a low enough level. If you move a chunk of data bit for bit from one area in memory to another area in memory, all those pointers are going to be wrong. This is going to be difficult. It's hard to figure out where the pointers are. It's hard to figure out how much they point to. It's very difficult to know how you can do this in general. And you can't expand because there might not be any nearby space. So let's look at that problem, the expansion problem. So there's process A. It's got partition that it's happy with. Process B has a partition there. Process C has a partition there. OK. Now, as process B runs, it says, well, gee, I asked for too little memory. I need more memory. I'd like to expand my partition size. You can't do that. You can't just give B the next set of addresses beyond its current partition, because you already allocated those to C. So that wouldn't be possible. You can't move C's partition out of the way, because you can't move C's partition. You can't move anybody's partition. You can't move B's partition. You could move it potentially to the green area if you could move partitions, but you can't. Okay, so that means you're kind of stuck. And basically, B says, I would like another two megabytes of memory. Hey, you got plenty of free memory. In principle, you should be able to give him his extra two megabytes, but you can't, because there is no free space right next to where he is currently allocated. All right, so regardless of these potential issues, how would we go about doing this? Now, this is going to have to be more complex than the fixed partition management. With fixed partition management, you could say there are 48 partitions, each of the following size. I've got a bitmap with 48 entries in it. Just keep track of which ones are allocated, which ones aren't. Fine. You can't do that here, because you have no idea how many partitions you're going to allocate, and you don't even know what size of the partitions are. With fixed partitions, it's fixed. With variable partitions, it isn't. It's dynamic. Every time that a process runs, it might ask for a different size of partition. Now, the way we're going to do this is you're going to say, fine. Let's have a big heap of memory. We have all the memory we might use to support processes in a heap. Some of it will be allocated at any given moment, some of it will not be allocated at any given moment. In particular, let's maintain a list of some sort that keeps track of all of the unallocated pieces of memory. They may not all be contiguous, not like in that diagram I just showed where the green area was all at one end. They might be in various different places. So we're going to keep a list of all the ones we haven't allocated. We'll call it the free list. Great. So this is a structure that is being kept by the operating system. The operating system will maintain the free list because the operating system is the one that's going to allocate and deallocate partitions for individual processes. So what are we going to do, given that we have this free list and got partition processes running? Sooner or later a process will say, I need a partition. I need a partition of this size. It'll give us some size. What are we going to do? We will look through the free list to find a possible place to allocate from. Now, we have to give it contiguous addresses, so we will look for an entry on the free list that has enough memory, enough free contiguous memory, to satisfy the request. Occasionally, we may find something that's exactly the right size, but that's just coincidence. More commonly, we will find something that is bigger than what we need to give this new processes request. Then we will say, OK, fine, we're going to divide up what we got here. It's a big piece of memory. He asked for a smaller piece of memory. We'll give him what he asked for. That will leave us with some free space, whatever was left over from the big free area after we've carved off the piece that he wanted. OK, so what are we going to do with that? Well, put it back on the free list. Here's another piece of free memory, not as big as it was before, but it's free. And whenever a process actually finishes with memory, either the process ends or for whatever reason, it says I'm finished with this partition, it'll tell the operating system or the operating system will deduce it. And the operating system will take back those partitions that are no longer in use by the process, and it'll put them on the free list. So the free list is a data structure. If we're using this strategy, the operating system is going to have to maintain. Now, it was really easy with fixed size blocks, just bitmaps. Here, we're going to have to have some more information. The typical way it's going to be done is with a linked list. And effectively, we're going to say, let's have a linked list of memory partitions. In the simple version, you could say, let's just have a linked list of all the free memory partitions. But it's not enough to say this is a free partition. We also have to know how big is that partition, because things are a variable size. We chop off pieces. We add pieces, whatever. So every entry in this linked list is going to have to keep track of, this is where this partition starts, starting address, and this is how long it is. And probably, since it's a linked list, it will then also point to the next entry in the free list. The way we will do this effectively is to say, well, let's not just have a bunch of separate chunks of memory that we use to hold these descriptors, the size and the next pointer. Let's instead just put those at the beginning of every piece of memory that we've got in the linked list. the descriptor size and next pointer will be in the first few bytes of every free chunk of memory. OK. Now, we have another choice, which is, in addition to having descriptors for allocated memory, we might have descriptors for unallocated memory. We might have descriptors for the allocated memory as well. This is going to be a little bit more complicated, because then we have to say, OK, yes, this is a piece of memory for you. But the first few bytes in here, you can't touch those. because that's where we've kept track of how big this is and where the next one is. But, you know, it's just a little complication. So here's how the free list might work. So let's say that we are going to keep track of all the chunks of memory, the ones we've allocated and the ones that have not yet been allocated. So we'd have a head, the operating system would say, "Here's the head of the free list "of the appointed of the first element." And in that pointer, follow it, it would lead to a chunk of memory. First few bytes in the chunk of memory would have a bit that would say, perhaps this is free. Then it would have a length. This is how long it is. And then it would have a next pointer, pointing to the next element in the list. Now here, because we're going to keep both the allocated and unallocated partitions in this list, here we would say, OK, in this diagram, green entries are elements in the linked list that are free. Yellow entries are elements that have been allocated. So here we see the first element is free. It points to another element in the list. This one, as we see, has a used bit, not a free bit. It still has a length. It still has the next pointer. It's yellow because it's been allocated. Some process is using this partition at the moment. It points to another element in the free list, and that element in the free list is free. Therefore, it's got a length. It's got a next pointer. It's indicated as unallocated, showing green. It has a pointer in turn to the next element, which is in use by some other process, and it in turn has another pointer to the next element, which is free, and then you just go on. And all of the elements of memory, all the pieces of memory, all the partitions, each of a different size, as we can see here, will be in the free list. You could do this instead by saying, I'm not going to keep the allocated segments in the free list. I'm only going to keep the unallocated ones, in which case the free list actually would only have these pointers to the free elements. OK, so how are we going to manage actually allocating from the free list? Well, we're going to use something that's called free chunk carving. Here's how it works. So here's a piece of a free list. So somebody says, I want some memory. I need a new partition. What do we do? We go through this linked list, and we find a partition that is sufficiently large to meet the needs of this request. So let's say we've gone through the free list. We've gone through other elements ahead of this. big enough or they were allocated. Here's one that's free. It's more than big enough. So we're going to say he doesn't need all of this partition. We're not going to give him anything he didn't ask for. We'll give him exactly what he asked for. So fine, we're going to have to divide this partition from the point of view of the free list into two parts. First, we're going to give him the part he asked for. So we'll change the length field here and we'll set up another header in in the remainder of the free list element, the part he doesn't need, the part that's beyond what he requested. And we'll set up a length and a free pointer there. Now we need to fiddle around with our linked list. So we'll change our pointers so that we have our linked list set up. Then we'll say, oh, this is the piece we gave to the process that requested something, exactly the bytes you requested. Now it's used, therefore we will not try to allocate it as long as it is in use. All right, great. Now you look at that and you say, there really should be no internal fragmentation here. And really, there isn't. But you might say, oh gee, but you know, I don't know exactly how much memory I typically need. So I asked for four megabytes, but it turns out I only use three megabytes. Isn't that internal fragmentation? Not from the point of view of the operating system. The operating system is not trying to police the requests made by processes for what they ask for. They ask for it, it gave it, and the operating system assumes that it's gonna use it. He asked for four, he's gonna use four. You do not try to keep track from the point of view of this kind of allocation of what he did and didn't use. It's his. As long as he's got that partition, he's got the full four megabytes, he can use it, he cannot use it. We don't care if he uses zero bytes in that. He asked for the partition, we gave him the partition. From the point of view of the operating system, hey, that's allocated memory, there is no fragmentation. Okay, so have we gotten rid of fragmentation altogether if we leave aside that issue? No, we've gotten rid of internal fragmentation, but we have not gotten rid of external. In fact, we've caused external fragmentation. What's external fragmentation? Let's take a look at an allocation strategy and what happens when we do various allocations. So the green rectangle here represents all of our free memory. Currently, it's all free. Process A says, I need a partition. We give them a partition. Process B says, I need a partition. We give them a partition. Process C gets a partition. Now, process B finishes, and it releases its partition. Great. So now, this middle rectangle indicates the state of what's happening. Process A has a partition. Process C has a partition. We've got a couple of green chunks here, One between process A and process C, one after process C. Process D asks for a partition. We look through and say, "Oh, it'll fit nicely into that area that was just deallocated by process B." B doesn't want the whole thing. We'll give him what he wants. Process E then says, "I want some space." We find some space for him. Process A ends. Process A deallocates his segment. The operating system says, "Oh, it's free." Process F comes along and says, "I need some memory." He wants that much space, we say, oh, that'll fit in there. We fit it in there. Okay, leaving a little tiny bit of green, unallocated memory after that, between process F's and process D's partitions. What's gonna happen here over the course of time, as our system runs, as processes start, processes end, they request partitions, they release partitions, experience shows, simulation shows, analysis shows, that what's gonna happen if we do this, is we are gradually going to build up small, unusable chunks of memory, such as those two. They may be too small to ever meet the needs of anybody who ever wants a partition. They aren't big enough for anything. If there are a lot of them, then there's a whole lot of memory that isn't allocated, and we know it isn't allocated. We could allocate it, but nobody needs that amount of memory. And if we add up all of these pieces of small, unusable pieces of free memory, we could end up with a very, very large percentage of our overall available RAM not being usable, even though nobody has requested its allocation. So the cause here is because every allocation you perform in this kind of system is going to leave some leftover free chunks. Over time, the free chunks get smaller and smaller and smaller. Eventually, they get small enough that they can't be used for any practical purposes. They never are big enough to satisfy any request. They're in the list. We know they're unallocated. They're not of any use to anybody. This is just as wasteful in terms of memory that's free but not in use, not actually being used, as internal fragmentation was. So we don't like external fragmentation. What do we do about that? Well, there are a couple of things we can do. One, don't create tiny fragments. If you don't create tiny fragments, if you can be successful with that approach, then you won't have these unusable little bits of memory floating around in your free list. Also, you can try to recombine fragments into bigger chunks. If you have two fragments that are next to each other and they're both small, why not have one bigger fragment instead of two small fragments? Let's start with that first strategy, the first approach. Let's not create small fragments. How? Well, you might say, clearly, if I'm smarter about what free chunk of memory I allocate to satisfy a particular request, maybe I can do better. Now, there are issues here. In the first place, this means that what was a relatively simple allocation approach-- look through the linked list until you find something-- becomes more complicated. And more complicated means you spend more instructions doing it, which means there's higher overhead. But nonetheless, you could do this. And in fact, people have looked into doing this quite extensively. They have come up with a number of algorithms. We'll go through all of these in a moment. So first, let's consider a generic strategy. And we'll see how this works with various algorithms to choose what to allocate. So let's say that we have here our memory. We're in a current state where the green elements are allocated, the red elements are free. Excuse me, the other way around. Green elements are free, red elements are allocated. All right, so that's free. That's free. That's free. That's free and that's free. As you can see, we already have to some degree some external fragmentation here down there at the bottom. These are used. We cannot allocate them to anybody else until they're free. Okay, so we're going to start allocating chunks of memory. When people ask for a new partition, we're gonna give them something. How are we gonna choose it? Well, one algorithm for choosing is called best fit, which sounds really good. It's best, isn't it? So what do we do? we search for the best fitting chunk. And what is the best fitting chunk? Well, it's the smallest size greater than or equal to the requested size. OK, so we're going to look through the-- a guy requested 2 megabytes. We're going to look through the list of free partitions. And we're going to give him a piece of memory from the smallest partition that will hold the 2 megabytes. So if there's a 2.1, we'll give him something from the 2.1. If there's a four, we'll give them something from the four, et cetera. And we're going to find the best one. Okay, now, if it turns out that you have a perfect fit, this is great, because if you have a perfect fit, there is zero external fragmentation due to allocating that request. He wanted two, you found two, you gave him two, everything is great. Okay. There are disadvantages, however. One, how are we going to find out which of the free elements in the free list is the best fit. We're going to look at every one of them. We're going to look at the first one, the next one, the next one. We're going to follow every pointer in the linked list until we've gotten to the end of the list. And we say, ah, out of all of the ones that we considered, this is best. We have to search the entire list every time. If there are a lot of elements in the list, that isn't so good. Also, it turns out that this actually runs counter to what we were hoping to do. We wanted to prevent external fragmentation. This causes external fragmentation. Remember I mentioned, OK, you're looking for 2 megabytes. You go through the list, you find a 4 megabyte partition that you could use to hold the 2 megabytes. You find a 2.1 megabyte partition you could use. Aha, let's choose the 2.1. That's the best fit. You end up with a 0.1 megabyte partition, which probably will never be usable. That's not so good. Here's how best fit works in action. So we start with our situation of some free, some allocated. We want to partition the size of this yellow one on the right. What do we do? Well, we check there, it could fit there. We check there, it could fit there. We're not even gonna look, of course, at the red ones. We check there, it could fit there. We check there, ah, that's not quite big enough. It won't fit in there, so that's not feasible. And then we check the last one, and that one's also too small. Won't fit there. So what do we do? We say, ah, the best fit is this one. This is the one that's closest. Hold on for a moment here. Okay, I'm sorry for the interruption. Going back to the example of best fit, how it would work when we tried to actually use it. Here again, we have the initial state of the memory, some partitions free, some partitions allocated, and we have a request for a new partition, the yellow one over here on the right. In using this particular algorithm to determine What's the best fit? What will we do? Well, we check against each of the free partitions to see if it will fit. And we'll see how closely it fits. So it fits those first three in various ways. It's a little bit too large, as we can see, for the next to last one. And it is much too large for the very last one. So they aren't even candidates. We cannot satisfy this request with those two. According to best fit, which one is the best fit? Which one should we use? if we're using this algorithm to satisfy this request. Here, that one. So we allocate, and what do we have left over? We have left over that little green area. Now we have another request, and it turns out the best fit for that one is there, leaving aside another little green area. So we've ended up with a couple of very unuseful free fragments. They are not going to be helpful. All right, so that was kind of the opposite of what we were trying to achieve. to prevent external fragmentation. We achieved external fragmentation. So best fit isn't gonna work out too well. What else might we do? Well, we could do worst fit. Now worst fit sounds like it's worst. How could the worst thing be best? But let's take a look at what's gonna happen. What are we actually doing with worst fit? We're checking the requested allocation against all the possible free partitions. One of them is the largest possible partition we've got that would have enough space for the allocation. If there is no such partition, we can't meet the allocation. Okay, but if there is one, we choose not the smallest, the closest, but the furthest from what is requested. We allocate from that one. Now, as we will see when we go through the example, and as you might think about, gee, this is gonna tend to create some very large fragments, might it not? Yes, it might, for a while at least. But if we do this using the free list that we've talked about before, we're still going to have to search the free list every single time. Here's how it would work using the same example. Free partitions, allocated partitions. We want to allocate space for this one. We try it there. It'll fit. We try it there. It'll fit. We try it there. It'll fit. Won't fit there. And it's not going to fit there either. OK. So we have three candidates. And we say, all right, which one is the worst fit? That one's the worst fit. So we allocate there. And as you can see, we're left over with, after carving off the piece we need, a fairly large leftover partition. When this one comes through, what's going to happen? Well, we try there, won't fit. Try the next partition down, it'll fit there. Try the next one down, it'll fit there. Won't fit in the other two. So, where do we put it? We put it there. Okay, now as we can see, we are left over with two potentially useful partitions and only one little tiny new area that's not going to be too useful. So best fit looked like that, worst fit ended up looking like that. Now this of course was for one particular choice of allocations. Over the long haul, many, many different things will happen. So which one is going to actually result in more external fragmentation? Experience, simulation, analysis, all say best fit will be worst. Worst fit will work better than best fit. Now of course worst fit does have the requirements still, you check every partition. There's another option, first fit. Okay, first fit is again you start looking through the free list for this, this, this, this, this, go through elements of the free list. As soon as you find one that actually will fit the allocation requested, it's big enough, you allocate that one. Now the advantage you get here is short searches. As soon as you find something that'll work well, okay, great. And because you're not necessarily choosing the best fit, you're not necessarily choosing the worst fit, you get sort of a randomized selection of how closely things fit. Now there are disadvantages here. Things at the front of the list are obviously going to fragment pretty quickly because you're always going to choose something from the front of the list if you possibly can, which means if it's a bad fit, best fit type, it's going to leave aside a little tiny fragment. So everything at the front of the list will fragment quite a lot. And that means you're going to get a whole lot of little tiny fragments at the front of the list that are not useful, but you're going to have to check every one of them every single time. take a longer time over the course of time. As operation of the system goes on, as it runs for a long time, searches become longer. Ultimately, it fragments just as badly as best fit. Okay, so what other options do we have? Next fit. Next fit is a little bit like first fit. It always says I'm going to start looking through the list. The first thing I find that is of sufficient size, that's what we're going to go with. However, there's a difference. With first fit, we always started at the head of the list. With next fit, we don't. We start at various different places in the list. After each search, after we've done a search and we've allocated for something, we set a special pointer, a different pointer, not the head pointer, but a special pointer. All right, so for next fit, what's going to happen is we are like with first fit going to look through the list, finding the first free chunk that is of sufficient size to meet our new request. But we're going to start looking at different points, not always at the head of the list. How do we choose what point we look at each time? Well, the last time that we allocated something, we found a particular place in the list. We found a good place, or at least an acceptable place, from which we allocated. We carved something off. With next fit, we will set a special pointer to point to the free element after the one we just worked with. So it'll be a bit further in the list. And that's where we will start our next search. So this guest pointer will be going through the list. It'll keep moving through the list over and over, gradually sort of moving down to the bottom of the list, then being a circular list. It'll go back up to the top. we'll always start looking at a different place in the list every time that we do an allocation. Okay, so what's going to happen if we use this algorithm? Well, we're trying to get advantages from first fit short searches and advantages from worst fit, spreading out fragmentation. So if we're lucky, we're going to get short searches because unlike first fit, we're not always going to be looking from the same place and building up a lot of useless free segments that we have to look at even though they are no use, we're going to be jumping into some random place in the list. Maybe it's better in the random place in the list. And fragmentation will not all occur at the front. It'll occur close to wherever we are currently looking. And maybe that'll spread it out pretty well. So that sounds good. Now, one thing I want to point out that's not just about this particular algorithm, not really about algorithms involving memory management particularly, is that guest pointers are a general technique we use in system software and in application software in certain places. So basically, the concept is have something, a pointer, that is at something you're going to look at, and it might be right. It might not be right. If you have arranged your list and your search algorithm in such a way that it's going to eventually find right thing anyway. It wasn't where your guest pointer pointed to, but it'll find the right thing eventually because it'll circle around to find the right thing, and that's great. If you have a high percentage probability of finding what you're looking for close to the guest pointer, it means you don't have to search as long. So your searches can be shorter. So we end up using guest pointers in many, many different problems. We'll see another example in a later class. All right. Now, despite that, next fit tends to over the long haul pretty much work like first fit eventually over the long term. But we have to do something to try to combat external fragmentation beyond saying let's make good choices about what we allocate from. Another choice, of course, and something we should do because it's obvious, we're eventually, in many cases, going to deallocate partitions. Process ends, releases its partition. At that point, that partition becomes free. Now, if we are lucky, the range of addresses in the newly freed partition is right next to a range of addresses in an already free partition, in which case we can create one bigger free partition by adding together the two that are right next to each other. So this is a good way to build up bigger fragments. It works against external fragmentation. So whenever we can do this, we're going to do this. Now you may have wondered, why do we keep the free list in the order that we keep it in, which is essentially the order of addresses, low addresses at the front, high addresses at the back. This is why. We could have kept it in another order, such as, for example, big fragments up front, small fragments at the end, and that would have different properties. One unfortunate property that would have is it would make it difficult to coalesce. It would make it difficult to figure out when you freed up a fragment, whether that fragment was next to an already deallocated fragment. You'd have to look through the whole list to find that out. So typically, we're going to probably keep things in order of address for the purpose of making coalescing easier. So that means that, of course, when you look through the list and you say, "Oh, I'm deallocating this fragment," you will find the right place in the list to deallocate it, And then you'll be able to tell whether there's a fragment right next to it that could be coalesced because they're at the right place in the list. So here's how it would work. Let's say we have a free list here, a portion of the free list. Some are free. Some are allocated. And whoever held that partition says, I'm done with that partition. Perhaps my process ends. I release that partition. It can now be free. OK, now the easy thing to do, of course, is just go to that partition in the linked list and say change the used bit to the free bit. Great. But now we can do something else, because look at what's happened here. That partition is right next to, right after, an already free partition. We can combine them. And we can now say, oh, now we have one bigger free partition. And in fact, in this particular example, we were very lucky, because the following partition is also free. So we can combine that and say now we have a very big free partition. Great. Now instead of having three small partitions that we could allocate from, we have one really big partition, which combats fragmentation. We have reduced fragmentation. So essentially, we have allocations that are causing sometimes external fragmentation going on. People allocate new partitions. cause new external fragmentation. And then we have coalescing going on more or less at the same time. People release partitions. They, to some extent, work against external fragmentation. They build bigger fragments. Okay, which of these two processes, or these two activities, getting external fragmentation because you're allocating, reducing external fragmentation because you're coalescing, which of them is going to dominate? Well, it depends. If you have a lot of free space already, then coalescing tends to work pretty well, because there's a pretty good chance that whatever you're freeing is close to something that's already free, because a lot is already free. If there's very, very little free space, it's less likely. Now, one reason that you also might see coalescing work well is if you have a lot of processes that allocate free, allocate free, allocate free, or very, very short processes that don't hold on to their partitions for very long, you'll get a lot of opportunities to do coalescing. On the other hand, if you have processes that run for days, weeks, months, chances are they aren't going to release their partitions ever. So if their partitions are somehow or other involved in a case of external fragmentation, you're never going to be able to coalesce with them because they're not going to stop running and they're not going to release their partitions. It also depends on how variable are the sizes of the chunks. This is something that isn't quite so obvious, but simulation shows that this is going to be the case. Because if you have highly variable chunk sizes, then you tend to get highly variable external fragmentation and you get less benefit by coalescing. So generally speaking, fragmentation will get worse over time. The longer the program runs, your average program, the worse the external fragmentation will be. So, we've talked about fixed partition allocation. Here we're now talking about variable partition allocation, variable sized allocations. It gets rid of internal fragmentation. There is no internal fragmentation here because you're giving me exactly what they asked for. It's more expensive to implement. You're gonna have to have a free list instead of a bitmap. You're gonna search through the free list, which will have overhead associated with it. You're gonna have to build in code that will do the carving, that will divide up a free fragment into a used and a free fragment of smaller size. You're going to have to have coalescing code that will put together free fragments. This is code you didn't need when you had fixed partition allocation. You're going to get external fragmentation. It's going to happen. There's no question about it. You can try to work against it, but that's too bad. So are we forced to choose one of these two choices, the fixed partition with internal fragmentation, the variable partition with external fragmentation. Now, here's one case where we can avoid that issue entirely. What if we have a whole lot of allocations that are all the same size? Everybody's asking for the same size thing all the time. So let's say that we have somewhere a piece of code that is allocating memory. And it's allocating small pieces of memory, let's say. So it's allocating something between, let's say, 0 bytes and 4 or 8k bytes. And we look at how frequently are those allocations requested. You might think it's going to look something like this. In real systems that really do this kind of thing, this is not what really happens. This is what actually happens. You get a few sizes that are very, very popular and others that are less popular that don't occur nearly as often. So for example, a 4k allocation might be something where your program has a need for a 4k buffer of a particular size, a whole 4K of something. We'll see cases where we might want to have a lot of 4K buffers in the near future. OK, so at any rate, if you want that, well, then gee, is there anything we can do to avoid any kind of fragmentation, at least associated with those particular requests, those 4K requests, those 128-bit requests, those 64-bit requests? What could we do? Well, the reason this happens at all is that in the real life, in real systems, we don't have a system that randomly asks for some size of allocation. We have a system that says, I do work on things of this size. I send messages, therefore I need a buffer that holds a message. I read blocks of data from the disk, therefore I need a buffer that holds a block of data, size of the disk block, et cetera. And this happens in applications as well. So we do this all the time, certainly internally within operating systems. You also will end up doing it in some cases in applications, complex applications that do sophisticated memory management of their own. So if that's the case, then you not only expect to see a whole lot of these allocations, but normally for this kind of thing, the allocations aren't going to be for very long. You'll allocate something, you'll use it for a short period of time, you're done using it, you're free. OK, so fixed size, always the same size, Not for very long. Happens a lot. What could we do to make life simpler there? We can use buffer pools. What's a buffer pool? We figure out popular sizes for allocations for the particular software that we are working with. And then we say, let's grab a chunk of memory, fairly large chunk of memory, and let's divide it into buffers of exactly the size I need. You need 1K, we'll divide this chunk of memory into 1K buffers. They're exactly 1K. This is going to be a fixed allocation size. Anytime that anybody says, I want memory, one of the things we do very early on in dealing with a request is say, does it match the size of one of our buffer pools? We've got a 4k buffer pool. We've got a 1k buffer pool. Is it a 4k allocation? Is it a 1k allocation? If it's one of those things that we have a buffer pool for, we don't go through the ordinary allocation strategy. We don't do the variable partition size on them. Instead, we go to the buffer pool. We say, I want one of your 4K buffers, and it gives us one of the 4K buffers. Now, this is gonna be much simpler because it's effectively for a subset, not everything, but a subset of what we're doing. We've now said, you don't need to do all this free list stuff. Remember, we can use bitmaps for fixed allocations. This is a fixed allocation. It's always 4K, therefore we just have a bitmap keeping track of the allocation status of all of our 4K buffers. This is a lot simpler. We can just do a few bit manipulation things instead of doing searching a linked list. We don't have to carve. We don't have to coalesce. We don't have external fragmentation. OK, now the issue, of course, is you have to know what size the buffers are. You can't have a buffer pool of every particular size in the world, because ultimately, the buffer pools will be helpful for the particular size that they're helpful for, but they're not helpful for anything else. So if one particular size is a very, very common allocation size, having a buffer pool for that will be a tremendous benefit. If another size is very, very, very uncommon, there is no benefit. There is, in fact, a cost to having a buffer pool of that size. What we're going to do with buffer pools is we will only match perfectly sized requests. 4K buffer pool, do you want 4K? Go to the buffer pool. You want 3.9K? Sorry, we can't help you with a buffer pool. Go to the other allocation strategy. OK, and the reason we do this is because we don't want internal fragmentation. If we allowed the buffer pool to be used to satisfy smaller requests than the buffer size, we'd get internal fragmentation. So generally speaking, you're going to have a process or some operating system code say, I need this piece of memory, and it's this size, and it's one of these sizes. It's popular. Make a lot of requests of that size. You go to the buffer pool and say, OK, fine, give me the buffer. The system says, here, it is from the buffer pool. Typically, what will happen is whoever asked for it will use it and free it pretty quickly. Now, this may be an explicit free, explicitly allocate, explicitly free. In some cases, depending on what you're doing with a buffer, it may be kind of implicit. So for example, if you allocate a buffer to build a message and you're going to internally in the operating system send that message across the network interface, when the network interface tells you the message is sent, you can deallocate that buffer. You don't have to go to a process saying, are you done with the message? The message was supposed to be sent. The message was sent, fine, forget about it. Okay. One question of course is, okay, fine, you're gonna have a buffer pool. The buffer pool will contain a number of buffers of some single particular fixed size. Great. How many? How many buffers in the buffer pool? Well, the right answer to that usually is say, watch what's happening and resize to the appropriate size of the buffer pool based on actual use. Use a lot of buffers, have lots of requirements for buffers, we need more buffers. Don't use very many buffers, we need fewer buffers. So what you can do is you can keep track simply of activity. How widely, how often is a request made that is satisfied by the buffer pool? And if it turns out that we don't have enough buffers, we keep going to the buffer pool because what's gonna happen when you don't have a buffer in the buffer pool? Somebody wants 4K, you go to the 4K pool, You've already allocated all the 4K buffers. Clearly, you didn't allocate enough buffers. Well, if that happens very often, add some more buffers to the buffer pool. On the other hand, if it turns out that you've got a lot of buffers in the buffer pool and they're practically never used, well, then you don't need that many buffers. Take some of the space that you've used in the buffer pool, return it to the system to use for more general purposes. And this doesn't have to happen just once. You don't have to just figure out, This is how my system works. You can keep track of this dynamically. As things are allocated and deallocated, you can have statistics kept on what's happening with my buffer pool. If it turns out that you need more memory based on the fact that your buffer pool is very, very heavily used, then you can ask for more memory. There's another question, of course. If I ask for more memory, how much do I ask for? On the other side, if my buffer pool is underutilized and I'm going to free up some memory, how much do I give back? So you've tuned that as well. So you basically say, here's a low space. If I have this much space in the buffer pool left and it isn't nearly enough, I'll ask for more. If I have this much space in the buffer pool and it's far more than I need, give some back. This is nice. We like in our system software, things like operating systems, to have dynamic adjustment without human intervention. All this is done by software. This is not done by people. We like to have dynamic adjustments of what our systems do to try to match ongoing operations, both because this means we don't have to have people watching, but also because it means we can work at speeds and at times where there are no people watching, where it's not possible for people to keep track of this kind of thing. Okay, now, we do have one issue with buffer pools that we tend not to have with other things as much, called memory leaks. So, what's a memory leak? Well, this is easiest to think of in terms of user processes. So what happens in a user process is you typically say, I'm going to grab a big chunk of memory that will serve as my data area. Then within the user process, you're going to say I'm going to now manage that memory area. I have to do my own memory management. The operating system isn't going to do it for me. It gave me 16 gigabytes. I have to decide how to use the 16 gigabytes. So you're going to have an allocation and free approach within the application code. This is typically done in libraries. So what'll happen there? You're gonna ask for a piece of memory. Now, if you are well-behaved, if your process is well-written, when you're finished with that piece of memory that you asked for, you're gonna release the piece of memory. If you have a bug in your program, which, you know, in many applications, we have bugs in the program, maybe you forget that you allocated that memory and you never deallocate it. And if you don't tell your memory management system, deallocate this memory, It thinks the memory is still in use. It can't take it back. You haven't told it it's safe to take it back. That's what happens with a memory leak. So this kind of thing is fairly common in complex applications like web servers and web browsers. They allocate lots of pieces of memory. They deallocate lots of pieces of memory. Sometimes there's a piece of code somewhere in there that under some circumstances doesn't properly deallocate what it allocated. That piece of memory is from the point of view of the memory management system of the application forever lost. It is gone. This is called a memory leak. So this happens primarily, of course, with complex long-running processes. Web servers, web browsers, are complex long-running processes. You know, a web server may not stop running for months on end. A user application web browser may be running for days or weeks without changing. You may have noticed on some of your web browsers that if you've had your computer up for a long time or just go into a sleep mode and then start it up again, it may be the case sometimes that your web browser gets slower and slower and slower, just keeps slowing down. What's going on there? One reason it might be doing that is because of memory leaks. It used to have a bunch of memory, but it didn't deallocate memory when it was finished with it. So it's got all this memory that in principle it could use, but it doesn't know that it should be free. Therefore, it's going to have to do more expensive operations management. And that problem is typically solved by saying, well, if you end the process, then all of its memory gets returned to the operating system. You start it up again, and it starts afresh. Of course, if we've got the same code in there, and you do the same kind of things, eventually it's going to get into the same state. Okay, now, how could we deal with memory leaks of this kind? Well, garbage collection is one approach. What is garbage collection? Basically, we can say we don't think our programmers are smart enough to release memory when they should release memory. They should, they don't. So, let's solve the problem for them. Let's figure out what pieces of memory in this area of memory that we're working with are actually in use, and which ones probably should have been released, but they didn't bother releasing them. So, in particular, let's not do this constantly, because it may be expensive to do, but when we think we're in trouble, when our memory allocation is low, we don't have a lot of memory to work with, things are beginning to slow down because Let's start a garbage collection process. So what are we going to do? We're going to go through the memory that we have allocated and try to find the pieces that should have but weren't deallocated. What are those? Well, basically, there's some kind of object, not object-oriented necessarily, though actually we do this in object-oriented languages, but some kind of data structure representing something, like this is a message we're building, or this is an element in a larger data structure, one element in a table or something of that nature. Let's figure out, can anybody still access that element? Can anybody still get to that buffer containing that message? If they cannot, then maybe we should have deallocated it. How could we tell? Well, it's got an address. In order to get to it, they have to get to the address, now don't they? They'd have to follow the pointer to get to the address, saying here's the buffer. If there is no pointer that leads to that address, well then, maybe we can free that. So let's look through memory for all the pointers, and let's figure out what's pointed to and what isn't pointed to. And if it isn't pointed to, fine, free it up. And if it is pointed to, don't touch it. OK, so then you get free memory back if you can do this properly. How do you do this? Well, returning to actual object-oriented languages, This is something you can probably do in an object-oriented language. Why? Because in an object-oriented language, the memory that you're talking about here is in particular sizes. Sizes that are the size of the object. And we know how to get to objects. Objects are allocated. Objects should be deallocated. Maybe they haven't been deallocated. But if there is no remaining way to get to this thing that we know is of the appropriate size object-- we probably even kept a record saying this is an object of type foo. And there's no way to get to that particular object of type foo. We know it can be deallocated in an object-oriented language. That's because we have descriptors for every object built into the language. Application programmers don't have to do this. The language does it for them. We have tags saying this is an object reference. So we can then figure out what's not currently live and reachable anymore. We can deallocate them. Within an operating system, for at least certain portions of the operating system's own memory, we can do the same kind of thing. So if, for example, we have a buffer pool in the operating system, and the buffer pool holds messages, let's say 1k buffers holding messages, and if we know everywhere in the operating system that we ever allocate buffers from this buffer pool, which we can know because of course we can look at the code, we can then say, all right, let's look at all the places where we might've saved a pointer to this buffer. Did we save a pointer to this buffer? No, it's not in any of those places. Well, it can't be anywhere then, delete the pointer. How about for the general case? General case is hard. How would you do it in the general case? Well, you have to find the pointers in allocated memory. I have to say, here's pointers, here are live pointers to live things. If there's a thing out there that doesn't have a live pointer to it, maybe it's not, maybe it should have been deallocated. So you have to know where the pointers are. And moreover, it's a pointer, but is it a pointer to a byte? Is it a pointer to a thousand bytes? a pointer to a 4k to a megabyte? Who knows? You have to figure that out too. In object-oriented languages, that's taken care of. In buffer pools, that's taken care of. But in general, you have to figure it out somehow. Then, having figured out, here are all the pointers and here is all the set of data of varying length that they point to, then you can say, all right, here's the other stuff that isn't pointed to, and then you free that. The problem with this is that, Well, gee, what's a pointer? In general, you can look at a number and say, well, this is probably a pointer. It's a big, weird integer that doesn't seem to match anything else. Maybe this is a pointer. But you don't really know that. Maybe it's another data type. You know, maybe somebody's been doing arithmetic calculations on large numbers or something and it's the result of one of those arithmetic calculations. That's not a pointer. And if they are a pointer, fine, it's a it's a pointer. Can you get to the pointer? If you can't get to the pointer, then really it's a dead pointer, so you should be deallocating it. So if you have dynamically allocated data structures and you know exactly what they are, maybe you could figure it out, but maybe not. But there are cases that are very, very, very hard, and you don't really get, in a general case, anything that says this is how much is pointed to. If you know what the data structure is like, as you would in an object-oriented language, then you could deduce. But if you don't know that, if you just say, look, it's a pointer, how much is it pointing to? There's nothing there to tell you. All right, now, the reason we cared at all about garbage collection is it's a way that potentially we can free memory. It doesn't really help or hurt with fragmentation. It just says, you know, we can maybe do a little bit more freeing of memory. Maybe that will lead to coalescing. So if we have ongoing activity, then it can be very, very hard to do much coalescing because things aren't being deallocated. We could just, if we run into problems, say, well, let's stop accepting new allocations. But then, of course, that would mean effectively the process that requested the allocation is probably going to be blocked or we'll get an error condition that is likely to lead to its failures. It's not a good solution. What we'd like to be able to do is take everything that's in memory and pack it all into one tight space, leaving a whole lot of free memory at the end. That's called memory compaction. How might we do it? Well, let's say that in addition to having the RAM, which is in the condition we see here with the blue areas here being allocated, the green areas being free, in addition to that, we have a swap device, something like a flash drive or a hard disk drive. It's got a lot of space on it, a lot more space than we got in RAM. And in particular, let's say it's got a lot of free space. So what could we do? Well, we could say this is the largest free block we started with. It's too little. We are having too much fragmentation. What can we do? Let's compact. We're going to keep that indication, that little brace showing how much was the largest free, so we can see what happens after we finish compacting. All right, so let's compact. What do we do? We copy that to the swap device. We copy that to the swap device. We copy that to the swap device. We copy that to the swap device. Now all of our memory is free, but of course, all of our partitions are on the swap device where we can't use them. So we're going to bring them back. And maybe we bring them back in the following way so that they're all right next to each other. We have compacted our memory. How much space have we got? What's the largest free block we've currently got? That. Now we had exactly the same amount of free and allocated memory in both cases. It's not any new. But the largest free block, which means the greatest flexibility, is much, much bigger after we've done the compaction. So this is great if we can do it. Now, how can we do it? Well, we have to relocate. Because if you remember what happened on that slide, we moved partitions from one place to another. That would mean that it was currently sitting in this particular range of memory addresses. Now, after compaction, it's sitting in a different range of memory addresses. What's hard about that? Pointers. All the addresses in the partitions that we moved will point to the wrong place when we're using physical addresses, as we are up to this point. Which means every time that we have in the code a jump to someplace, every time that we have a stack segment, stack frame pointing to another stack frame, all those pointers are going to be wrong. And this is going to be kind of difficult. Now, let's look in a little bit more detail about what's hard about this to make sure you understand. So let's say that we have a partition here. This is physical addresses. This partition is sitting in locations 10,000 through 20,000 in memory. Those are real physical addresses. And we have in our partition an pointer to an integer named foo. And where is foo? Foo is sitting there in that partition. Foo's current value is 11,000. So what that means, of course, is that it points to that location, which happens to contain 18,000. OK, let's move the partition. How would we move the partition? We'd find a new place for it, and then we would copy it, presumably, bit for bit. Every single bit that we had in the old location will appear in a new location in exactly the same bit values. So we copy it, great, and we moved it to 23,000 to 33,000. Maybe that's a better place for it. Where's foo? There's foo at this location, somewhere a little below 33,000. But what's foo contain? It still contains 11,000. We copied it bit for bit. What does that point to? Well, to something not in the partition. If we try to follow foo, we're going to run into a problem. Next time we access foo, we're probably going to have the program fail. So, if we use the techniques we've talked about so far, you really can't relocate a process. You can't move it. You might be able to relocate references to code, if you understand the code well enough. You can say, well, I'll go back to the code and I'll look at the code and I'll see, oh, this is what should be here. This is what that should be there. You could do that. You would, of course, have to go back to the code in the binary form. So the program that's stored on disk that you used to create the process could be done, but not very flexible. How about references to data, though? How are we going to do that? Well, we really can't, because we don't know what happened dynamically before we did our relocation. Who knows what values we set? Who knows why we set them to that value? We can't just say, oh, I'm going to add a few thousand bytes because I moved it 10,000 bytes, you'd have to know first of all which ones are the pointers. You don't want to add something that isn't a pointer. So it would be effectively impossible. Effectively, it's just like garbage collection. You could never find and fix everything. So if we want to do relocation, and really we would love to do relocation, it would make our lives so much simpler, we'd have to say we don't care where the process lives in memory. We can move it from one place to another place. We can just copy the bits, and everything still works in the new place. That's what we need to do. We need to make the partitions location independent. How do we do that? Now we're going to start talking about the other form of addresses, virtual addresses. Here is a simple version of virtual addressing, which was kind of the first version of virtual addressing that was developed. So here, we see that we have up here in the left, the box representing this is the process. The process has code. The process has its own data area. It has its own stack. It has some perhaps dynamically linked libraries. OK, now, we're going to say now, this is not the physical address space. The shared code doesn't sit at that address. The private data doesn't sit at that address. The private stack doesn't sit at that address. And the dynamically loaded libraries don't sit at those addresses either. nothing actually lives in those physical locations as indicated here. It's somewhere, but it's not in those particular physical locations. This is a virtual address space. Now, as I said, it's all somewhere. So maybe we're going to have to do something that will say, we'll translate the addresses that the process thinks it has into the actual location. So the shared code is actually sitting down here in this little green box in this yellow area that represents the physical address space. It isn't sitting starting at 0, 0, 0. It's somewhere else. So if we have a magical translation unit, every time that we issue an address, it goes to the magical translation unit, and it says, oh, you think it's there, but I know it's really here. So I'm going to go out to here in the real memory, and I'm going to go get it for you. So it can do that with the data and with everything else. OK, so we just need a magical translation unit. How? How are we going to do this? Well, this is really useful because, of course, what do you really have when you make up a process? Well, it's got a code segment. It's got a bunch of code. That all sits in a contiguous area. It's got a stack segment. We want the stack to be contiguous. That should sit in a contiguous area. It's got a heap segment. Why not a contiguous area for the heap segment, too? That's useful because if you want to build a table, you might just want to keep going through the table by adding things. So you want the addresses to be contiguous. So it makes a lot of sense to say the overall memory needs of a process are going to be represented by three or maybe a few more than three segments, but not many more. So let's say we'll use the segment as the unit of relocation. We will relocate entire segments at a time. So we'll have the code relocatable to somewhere else, not part of the code, but the whole code, the whole segment. We'll have your whole stack relocatable to somewhere else, not part of the stack, the whole stack. We'll have your data area relocated to somewhere else. again, the whole data area. So how are we going to do this? Well, this was something that was developed in the 1960s. The IBM System 360 was one of the early systems, hardware systems, that actually did this. And this continued going on for quite a while. To some extent, it's still in certain hardware. So how are we going to do it? Registers. We're going to have special registers. We will build into the CPU. We did build into the CPU. Not me. I didn't do this work. But the people who designed this back in the 1960s and later said, we're going to have relocation registers. Every process will have its own value for the relocation registers. The process that's running on the core-- back then, they were single-core systems-- the process that's running on the core will set its relocation registers. They're called segment-based registers. So we have a bunch of segments-- data segment, code segment, et cetera, et cetera. When you start the process up, when you schedule a process, and you're going to do a context switch to get the process running, the operating system will set the special segment base registers to the appropriate value for that particular process. Here's where its code is in physical memory. Here's where its data is in physical memory. Here's where its stack is in physical memory. OK, and we'll have a pointer, the base register, to the start of each of these segments. Now, every time that an address is issued, because we're doing this in hardware on the CPU chip itself, We will take the address that was issued. We will add together the value of the appropriate segment base register, and that will result in the actual physical location. We'll do this automatically, every single instruction without the user code trying to do anything. It'll just say, I want address 1 million. We will go to the appropriate base register, add a number to 1 million. That'll be the physical location on a RAM chip of what the process thinks is at location 1 million. Okay, so now we are going to be able to do relocation, and we'll see how in a moment. So we start off with the segment in one place. If we decide we want to relocate the segment, well, we can relocate the segment. We can't do it while the process is running. But if the process is not currently running, we can say, all right, now we're going to take some time in the operating system to copy all of its bits from this place to another place, and we're going to keep track of now we've changed the value it should have in the segmentation register the next time it runs. How does it work? So here's how it works. We've got our virtual address space, which is what the process thinks is going on in terms of addressing, and the physical memory. And we put things related to this process, like its stack, into particular places in the physical memory. There's the stack, there's the code, there's the data, there's one of its dynamic libraries. I haven't shown the other two. OK, so we now will have base registers. Now, these base registers are within the CPU, but we fill them with values that come associated with this particular process. The operating system will maintain these values. The operating system will load the base registers at the time it should load the base registers. Why should it? I want to run this process on CPU, therefore it needs its correct base registers by the operating system. As part of the context switch I'm going to perform to get this process running will appropriately set the base registers. So it sets the base registers. Every time that an address is issued by this process, we take whatever address it issues, which will be a virtual address, not the real address. We add together the appropriate base register, and we get a physical address, which is the real physical address. So for shared code, it would go to the code base register. if we issued a code address, like saying, jump to this location, it would then say, ah, OK, I need to jump to a location, but not the one he specified. That's the virtual address. I will take the code base register. I will add together the content of that and the virtual address he gave. And that'll point to the place in the actual physical code segment that I've stored somewhere that things live. Similarly for the data, go to the data base register, leads to the data area. For the stack, it's the stack base register, leads to the stack area. If I want DLL1, it goes to the aux base register, and that will lead to DLL1. OK, how do we go about actually relocating a segment using this idea? Well, OK, here we have, at some moment, these particular segments for this particular process sitting in places in physical memory. So let's decide that the operating system, for whatever reason, says I want to move that stack for that process into a different location. I don't want it to be in physical memory where it is. I'm going to move it. So what does it do? Copies the bits, bit for bit, into a new location. Great. So there it is sitting on a new location. Fine. Now, every bit has been copied, bit for bit. Nothing has been changed except where they live. What else do we do? We haven't changed the virtual addresses in the stack because they're the same bits that they were before we did the move, exactly the same bits. Therefore, after they've moved, they are still the same bits. Great. Virtual addresses haven't changed. If we didn't do anything else and we tried to use the stack, we'd go to the wrong place. We'd go back to the old location of the stack. But what we do instead is we change the value in the stack base register so that now it points to the new location, not the old location. We can do that because we, the operating system, did the move of the stack. We, the operating system, control what should go in the stack-based register for this process. So we just change something in our process descriptor for this process saying, hey, now your stack-based register should be this value instead of the old value. And when we issue a stack address, it goes to the new location, just like magic. So this is really good, of course. It allows us to solve the relocation problem. Now we can move partitions around in memory. This allows us a great deal of extra flexibility. But that isn't quite sufficient. We also need protection. So we don't want a process to be able to get outside of its own area. So the way we do that is in addition to having the value of this is the start of the segment in the base register for that segment, we also have to have a length. Either we have a length, which we use to determine whether this is within the space, or we say this is the last address. And if the addition that we perform leads us beyond the last address, we aren't allowed to go there. What happens if we do? You probably have seen what happens if you do. If you've been running on Linux systems and you've been writing C code or C++ code, the chances are excellent that sooner or later, one of the pieces of code you're writing, One of the programs we're writing crashed and got a little message saying segmentation fault. It's probably about all it said. But your program crashed. What happened? Well, you tried to access an address that was outside one of the segments that you were supposed to be able to access. Most commonly, it turns out to be address zero. That'll never be in your addressable address space. So what's going to happen then is it's going to say, OK, I added together the address you gave me and the base register that I should be using for this and it came out to something that isn't one of your segments, you're dead. Normally a segmentation fault causes your program to crash. Okay, so how much of our problem have we solved? Well, we can use variable size partitions and we can cut down on internal fragmentation. That means we won't get much internal fragmentation, if any. We can use buffer pools when they make sense. We can move partitions around. This means that coalescing will be a lot more effective because we can move partitions in such a way, as we saw in that diagram earlier, where we move stuff off onto the flash drive. We can have small free partitions coalesced into bigger free partitions. But we still have some limitations. We still have to have contiguous chunks of data. Our segment has to start at this point and have all addresses through the end. This means that we're going to have some external fragmentation problems almost no matter what we do, unless we spend all of our time coalescing. And further, we haven't solved the problem of saying, what happens if you want to have a set of processes running and the total memory requirements of those processes is more than the RAM you've got? Up to this point, we haven't been able to solve that problem. Now, in particular, we need to get rid of the requirement of contiguous segments, And then we also need to get rid of the requirement that you actually have all of the memory associated with people's processes stored in RAM at all times. Okay, that's what we're going to cover in this lecture. We'll deal with solving those other problems in the next lecture on memory management. In conclusion of today's lecture, RAM is a scarce, important resource that we have to be very careful about managing in the operating system. Do a bad job, operating system runs poorly. We could use fixed partition management, but it's very inflexible, and it causes internal fragmentation. We could use variable partition management. It's more complex though than fixed partition, and it causes external fragmentation. Neither of them solves the contiguity problem. Neither of them solves the problem of saying we can't support more memory requirements than we have RAM available. We can help with garbage collection and compaction. Those can do some help. But those require us to relocate partitions. We have a technique for relocating partitions, but it is kind of expensive because we're going to copy a lot of stuff. And if we don't do that, it's going to be kind of hard to relocate partitions. We need to solve a few more problems. And in the next lecture, we will talk about how we solve those problems, which fortunately we have indeed solved.