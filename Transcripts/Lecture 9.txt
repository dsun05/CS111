In this lecture, we are going to start talking about various actual tools one can use to perform synchronization.

We'll be talking about semaphores, a theoretically base but actually available tool for that purpose, and a few other more practical tools.

Further, we'll also be talking about problems that we tend to run into if we are going to use locking to perform synchronization. is pretty much the best way we have to control synchronization in realistic circumstances.

But it's got its costs.

It's got its problems.

And we'll talk about dealing with some of those problems and understanding some of those costs.

So we'll talk here about semaphores, which are a theoretical mechanism for doing synchronization that has particular interesting properties.

We'll talk about mutexes and object-oriented styles of locking.

And then we'll talk about issues with performance problems and achieving good performance with locking.

To begin with, semaphores.

Semaphores are a theoretically sound way to build locks.

They essentially do locks.

They do other things as well, other types of synchronization mechanisms.

They're very, very general.

Now, they can be used, in principle at least, to solve many kinds of synchronization problems that you are likely to run into in your work with programs.

However, they have their limitations.

Now, what's good about semaphores, what's really important about semaphores, is that they are theoretically sound.

They have been carefully designed and investigated to demonstrate that they do things correctly, that they work properly.

They're very, very precisely studied, specified, which means they can be studied very carefully and have been studied very carefully.

So many, many, many theoretical papers have been built around properties of semaphores and uses of semaphores.

They aren't all that good.

They aren't all that easy for a programmer to use, however.

So they typically are not what you're going to use if you're writing programs, even though they are often available should you choose to use them.

It's also the case, of course, that there's a definition of what semaphores do, where you can specify in theoretical terms, this is how semaphores work.

And if they work that way and the implementation precisely that way with no differences, then they are correct.

On the other hand, if the implementation is not quite right, if the implementation doesn't do exactly what it should do, then there is a potential problem.

So because there is this potential difference between what semaphores should do and what a particular implementation of semaphores does do, the mere fact that you are using an implementation of semaphores does not necessarily imply that you are going to get the theoretical correctness you would have liked to get.

So where do semaphores come from?

They were invented.

Semaphores are a concept that has been in existence in human societies for centuries, but in the context of computation and using computers, they were developed in 1968 by a computer scientist named Edsger Dijkstra, very, very famous computer science, one of the leading theoreticians in computer science.

He was interested in the issue of how do we get several sequential processes, each individually sequential, but that want to cooperate with each other and want to achieve particular results in a correct way.

How can we get them to do that?

So he worked through the problems and was able to develop the concept of semaphores that helped him understand these problems and come up with solutions.

Now he was the first person to do this and his work essentially provided the foundation for all future theoretical work in synchronization of computations.

The behavior of semaphores as defined by Dijkstra is very well specified.

There are very simple, very precise rules for how semaphores work.

That's how they work.

They don't do anything else.

They do that and nothing else.

And everyone understands who works with theory of computation that this is what a semaphore is and this is what it does.

There isn't any debate about, well, sometimes it's like this, sometimes it's like that.

Now, as a result of this being an early and provably correct version of synchronization, it has been the foundation, the fundamental bed on which all future theoretical studies of synchronization have worked.

And other mechanisms that people have built, which aren't semaphores, are generally at the theoretical level compared to semaphores.

So this is like a semaphore except this differs from semaphores in the following way.

So understanding how semaphores work at least a little bit is helpful for a computer scientist.

Now you can build locks with semaphores as I've already stated, but you can do more than that with semaphores.

They are more powerful than just a lock in the form that we talked about in the previous lecture.

Their power comes in part because they include a second element, not just a bit indicating locked or unlocked, but a FIFOQ.

Associated with every semaphore is a FIFOQ and a lock.

Now, really, it isn't a lock at all.

What it is is it's a counter.

When you were talking about locks, it was a binary flag.

Locked, unlocked, one of two values.

With a semaphore, it's not one of two values.

It's a counter.

It's a counter that can go up or down.

It's a signed integer counter.

So let's take a look at what semaphores do.

Now, one nice thing about semaphores is that they're really quite simple when you get down to the brass tacks of what can you do with a semaphore.

There's not very many things you're allowed to do with a semaphore.

And you can only perform the allowed operation if you're using semaphores.

So as I said, the semaphore as a data structure has two parts.

It has an integer counter.

When you create the semaphore, you can initialize this counter to any integer value you choose. and it has a FIFO waiting queue, which is initially set up empty in almost all cases.

So that's what you have.

Now there are only two operations other than create and destroy that you can use on a semaphore.

Now Dijkstra was Dutch.

And as a result of this, this was before the era where everyone in computer science wrote in English, he wrote in the Dutch language.

Therefore, the operations he defined on semaphores were defined with Dutch words, not English words.

And the abbreviations were abbreviations of the Dutch words.

Okay.

So, he had two operations, a P operation and a V operation.

P stood for proberen, Dutch word, meaning essentially test.

V stood for verhogen, meaning essentially raise.

Okay, so what do they mean?

The P operation is essentially an operation called wait.

If you were using a semaphore as a lock, for example, before you could obtain, to obtain the lock, you would perform the P operation.

What would happen with the P operation?

Well, one of two things.

First thing that would happen is definitely it would decrement the counter.

You have a counter, it was set to some value, you would subtract one from the counter.

Now, the semaphore code would to see is the count after the decrement greater than or equal to zero.

If it is, you return, and whoever performed the P operation goes on to do whatever they were going to do.

If, on the other hand, the counter is less than zero, then you would not return.

You would instead cause the, whoever caused the semaphore to block, to wait, and add that process to the waiting queue.

Okay, so now the process, if this is the situation, is in the waiting queue and it can't run.

Okay, obviously we'd like the process to run again at some point in the future.

We don't want to block processes forever.

So the v operation is going to be related to having processes start up again.

The v operation is called if you were using semaphores for locks when you want to release a lock you've already obtained.

See around the p operation you got the lock.

When you're done with the lock, you perform the V operation, you release the lock.

The V operation does a bit more than that though.

What does it do?

Well, you remember there's a counter associated with the semaphore.

If you run the V operation on the semaphore, you increment the counter.

Then one of two things is the case.

Either there is somebody in the FIFO queue or there isn't anybody in the FIFO queue.

If there is somebody in the FIFO queue, you wake whoever is first in the FIFO queue. just that one process.

If there's an empty queue, you just go on and do nothing other than having incremented the counter.

Okay, so I've already sort of hinted how you would perform locking mutual exclusion using semaphores.

Let's look at that in a little bit more detail.

So let's say we want to set up a semaphore that will be used to lock some kind of critical section.

Great.

We want to make sure that only one process at a time is going to use that critical section.

Now, as in other cases of locking, we have to assume that everyone who might use the critical section is going to either be forced to or by convention ensure that they do perform the operations related to the semaphore before they try to use the critical section.

So whenever some process wanted to use a critical section associated with a semaphore, we would have to make use of the semaphore to determine if that is allowed to happen or not.

Okay?

So, how are we going to do this?

Well, first of all, of course, we would have to set up a semaphore.

We'd have to create a semaphore.

As you may remember, on creation of a semaphore, you can set the counter to any value you wish.

If you're going to use this semaphore for mutual exclusion, then we would initialize the semaphore count to 1.

And what does a semaphore count mean when we are using a semaphore this way?

The count, as initialized, indicates how many threads or processes are allowed to hold a lock.

Well, for mutual exclusion, it's one.

One process at a time could hold the lock.

Perhaps nobody holds the lock.

Perhaps one holds the lock.

Certainly, you never want two or more to be holding the lock simultaneously.

OK, so how do we make use of the semaphore set up in this fashion to achieve mutual exclusion?

Well, first, we use the p operation to take the lock.

So let's say that we have set up our semaphore.

Semaphore's value is initially one when we create it.

Nobody else has done anything else.

Somebody wants to use the critical section associated with this semaphore.

What do they do?

They perform the P operation.

Okay, now what happens in the P operation?

You go to the counter, decrement the counter.

The counter was one when we created it.

Nobody else has done anything with the semaphore yet, so it's still one.

So we decrement it, zero, okay.

And what's gonna happen then?

Well, according to the rules on the semaphores that we went over in the previous slide, we're going to check to see if the counter after the decrement has a value greater than or equal to 0. 0 is greater than or equal to 0.

Fine.

In that case, whoever performed this first P operation will be allowed to continue.

They've effectively gotten the lock.

They have achieved mutually exclusive access to whatever resource we're controlling with the semaphore.

What happens if somebody else then performs a P operation?

First process has already gotten the lock.

They're holding the lock.

They haven't released the lock yet.

They've only done their P operation.

Now a second process wants to do the P operation on the semaphore.

What's going to happen?

Well, when he does the P operation, we go to the counter.

The counter was set to 0 by that very first process that did the P operation.

So this second P operation by the second process is going to decrement the count to minus 1.

Then we check according to the definition of a p operation.

What should happen?

Well, we should check to see what is the value of the counter in the semaphore after the decrement, minus 1.

Then we check to see if that value is greater than or equal to 0.

Minus 1 is not greater than or equal to 0.

So what's going to happen?

The process that performed the second operation, the second p operation will be put into the FIFO queue that is associated with the semaphore, and it will be blocked.

It will not be allowed to proceed.

OK.

Now, sooner or later, that first process, the one that essentially got mutually exclusive access to the resource, will finish its use of the resource.

When it finishes the use of the resource, it should call the V operation.

OK.

So what's going to happen then?

Well, when it calls the V operation, It says, OK, fine, I'm going to go to the counter, and I am going to increment the counter by 1.

Goes to 0.

And in addition to that, without regard to what the counter's value is-- it's not going to check the counter here-- but without regard to what the counter's value is, it'll go to the FIFO queue, see who's at the head of the FIFO queue, and wake them.

So in the little example we were talking about before, process 1 did the P operation first, got the mutually exclusive access through the semaphore.

Process two tried to get mutually exclusive access, was blocked, and was put in the FIFO queue of the semaphore.

Process one released the semaphore with a V operation.

That meant that the semaphore would then go to its FIFO queue and see who's there.

The only one there in our little example is process two.

So process two would be woken.

It would have, upon being woken, an understanding that it achieved exclusive access to this particular resource.

It called the semaphore, it had to wait till it got its turn, but once it was told you have your turn, it's got the lock.

It doesn't need to do another P operation to get the lock.

It's already gotten the lock.

Okay, now, if there were more than two threads or processes here, you know, if there are three, four, five, they would be gradually put in the queue.

For a semaphore, the queue is always FIFO.

So it's always going to be in whatever order you performed your P operation.

OK, now, one neat thing about semaphores is that while they are fine for mutual exclusion, they can be used for other purposes related to synchronization.

For example, we talked in earlier lectures about how we had a second problem associated with synchronization, a problem where we were doing asynchronous waits.

We were waiting for something to happen.

We wanted the processor thread that was doing the waiting to be asleep, to be blocked until that particular event happened.

And then when that event happened, we wanted it to be woken so that it could go ahead and do whatever it was supposed to do when the event occurred.

We can use semaphores for this purpose.

Now, clearly, for locks, we could do that.

But what about for other types of things?

So let's say that what we're having is a producer-consumer type of thing.

So in that case, what are we going to do?

Well, we'll set up a semaphore that is associated with the event that's going to occur.

Let's say it's not a producer or consumer for the moment.

Let's say instead it's, oh, there's a piece of hardware that we want to make use of.

Multiple processes want to make use of the piece of hardware.

Only one can use it at a time.

So what are we going to do?

Well, perhaps what we could do is we could initialize the semaphore count to 0 and basically say-- I think I'm giving you a bad example here.

Let's go back and say this is something where, oh, let's say there are messages coming in.

We're getting messages coming in from outside, and they are coming to, oh, several threads that are worker threads.

And we want the worker threads to be able to make use of the messages, to do something with the message when it arrives.

Any message is good for any worker thread.

They're all similar.

So what do we do?

Well, we can say, all right, initially, we don't have any events.

No messages have arrived.

So the count is zero. and we haven't done any events yet.

So what are we going to do?

Well, let's say we start up our worker threads.

So what we can do is say, all right, I'm a worker thread.

I would like to do some work.

Let's see if there's work for me to do.

I would use the p operation in that worker thread.

The p operation, as we know, will decrement the count by 1 in the counter for the semaphore.

There's a semaphore now associated with how many things are happening. and it would then say, okay, what are we going to do?

Well, there hasn't been any event that occurred yet.

So what's going to happen?

Well, we're going to go there, we're going to decrement the count from zero to minus one.

We're going to say, okay, what is the count after we have done our P operation?

Apologies, Zoom dropped me for some reason.

We'll start over again on slide seven.

Okay, so we're talking about semaphores, and the semaphore can be used for multiple purposes.

We talked in the previous slide about how we would use it for locks.

Now let's say we're gonna use it for notifications.

We're going to say, all right, we have work coming in.

Example I was talking about at the beginning when we lost the connection before was we have several worker threads that are sitting around, they're waiting for work.

They can't do anything till work comes in.

The work may come in from another process.

The work may come in as a result of a message being delivered to the computer and being sent to one of the worker threads.

Maybe any of the worker threads can deal with any piece of work that gets created.

But there's a certain number of worker threads.

And if there's no work to do, they can't do anything.

So what do we do?

We can use a semaphore to keep track of who should be waiting and who should be going ahead and doing a piece of work, because there's a piece of work for them to do.

So what do we do?

Well, first, we would say, let's set up a semaphore to deal with this situation.

Now, assuming that we're at the start of our computation, we've just started everything up.

There's no work to do.

No work has arrived yet.

We hope some will arrive, but none has arrived yet.

So we're going to have the semaphore count in this case be how many pieces of work, completed events, are there to work with?

How much have we got to do?

Maybe we have a lot to do.

Maybe we don't have anything to do.

OK, so we'll set it initially to 0, because currently we don't have any work to do.

No work has come in yet.

So we're also going to start up our worker threads.

Say we have three worker threads.

So what are they going to do?

Each of the worker threads, in turn, is going to perform the P operation on the semaphore.

All right, so what's going to happen?

Well, the worker threads are essentially performing this operation to say, is there any work for me to do?

If there is, it'll get a piece of work and it'll do it.

If there isn't, well, then the thread should probably just wait until some work arrives.

So what happens?

The thread, looking for work to do, calls the P operation to see if there is work to do.

In the scenario we're talking about, we've just started everything up, there's no work to do.

So when they call the P operation on the semaphore, the initial count was zero.

So there's no worker initially posted, so there's nothing there that they can actually work with. they're going to call the semaphore's p operation, and they are going to decrement the semaphore.

It's going to go down from 0 to -1 for the very first thread that calls it.

If the other two threads call it, it'll go to -2 and -3 and so on.

But anyway, when the first thread calls it, it goes down to -1, that thread would be blocked, it would be put in the semaphore's queue of things to be woken when there's something to do.

Okay, now somebody creates work.

Message arrives, another process produces something that needs to be worked on, whatever it may be.

And at this point, whoever it is who handled that incoming piece of work is going to say, "Oh, we need to see if there's a thread available."

Now, you don't necessarily know until you check whether there is a thread available.

Maybe all the threads are busy.

Maybe there are no threads yet started, in which case you can't do anything yet.

But at any rate, if there is a thread available, a thread that's waiting for work, we would like to have that thread take over this piece of work.

So whoever deals with the incoming work, saying here's an incoming piece of work to assign, it's going to call the v operation.

Okay, what's that going to do?

Well, the v operation does what it always does.

It doesn't matter what you're using the semaphore for, the p operation and the v operation always work the same.

So what the v operation does is increment the count.

Minus 1 goes to 0.

Now remember on the V operation, we do not check what the result of the increment was.

Instead, we simply go to the FIFO queue and see if there's anybody in the FIFO queue.

And in the scenario we're talking about, there is.

There was that one thread that performed the P operation already.

So we wake it, and it gets going and presumably it finds a piece of work and gets to work on it.

Now if there were two or three threads sitting in the FIFO queue, We still, for each V operation performed, would wake one and only one of them.

And it would always be who was ahead of the queue.

So if you had three threads that were waiting for work to do, and they performed P, P, P, three P operations from three different threads in a row, thread one would go into the FIFO queue first, thread two into the FIFO queue second, thread three into the FIFO queue third, none of them would be able to run.

They'd all be blocked because there's no work to do. pieces of work start arriving.

One piece of work arrives, you call a V operation, we wake thread 1.

The second piece of work arrives, you call the V operation, wake thread 2.

Third piece of work arrives, you call the V operation, you work thread 3.

Okay, so now all three threads are off working.

What happens if a fourth piece of work arrives?

Well, you increment the count.

Now, you keep decrementing the count from 0 to -1 to -2 to -3, then you increment it from -3 to -2 to -1 to 0, so now it's 0.

When the fourth piece of work arrives, you increment the count to 1.

Is there any thread sitting in the queue waiting for work?

No, you have three threads, they're all off working.

So nobody is waiting in the queue, so you don't do anything further.

Let's say sooner or later thread 1 finishes the work that it got, so thread 1 is now ready to do more work.

What does thread 1 do?

It goes back to the earlier part of this slide and says, okay, when I have nothing to do, I want to see if there's work for me to do, I call the P operation.

It calls the P operation.

What would happen then?

Well, this fourth piece of work that came in would have incremented the count to one.

Okay, so P operation decrements the count by one.

One goes to zero.

Zero greater than or equal to zero?

Yes, it is.

So thread one would not block, would not get put into the FIFO queue.

It would be able to go ahead and do a piece of work, the fourth piece of work in this case.

OK.

Another way of handling semaphores is to say, let's say that we have available resources.

So let's say that we have somehow or other attached up to our computer several different pieces of hardware that can do various kinds of arithmetic operations quickly.

Something like an NVIDIA card.

Maybe you can somehow or other connect several of them.

You connected four of them to your computer.

They can do four separate operations, but you can only use the NVIDIA card to do one operation at a time, let's say.

So you got four of them.

Okay, now you also have things coming in.

You got pieces of work that pop up where you say, I need to use one of those cards.

You got four of them.

What you gonna do?

Well, you wanna make sure, of course, that you make use of your four resources when they're free, and you don't try to use a fifth resource and you only have four.

So if somebody says, I want to do this operation that requires one of these pieces of hardware, if there's no hardware available, that guy has to wait.

So how could we handle that issue?

Well, the way we could handle that issue is we could use a semaphore to keep track of how many of our resources we have already used.

So if that's what we're doing, let's say that we have a semaphore set up, and we have to, when we set up the semaphore, initialize its count.

What do we initialize it to?

Let's say that we have four of these pieces of hardware.

We would initialize it to four.

Now, in the particular example on the slide, it's three.

So we've initialized it to three because we have, let's say, three pieces of hardware.

OK, now, so far, no work is being done by any of these pieces of hardware.

They're all idle.

What do we do?

Some other process that is running on our computer says, I need to make use of one of those special pieces of hardware.

What do I do?

I have to find out. one of them available for me and I have to do this in a synchronized way.

You want to make sure that if there's one that is currently not being used, you don't have two processes that both try to grab that piece of hardware and interfere with each other.

It's a synchronization problem.

So what do you do?

Well, you say, "Okay, I will call the P operation.

The P operation on this semaphore associated with these pieces of hardware, the controllers of these pieces of hardware, will say, "Fine, I'm using the P operation to say I want to use one of those pieces of hardware if one is available, if they aren't already in use.

And the P operation does what the P operation always does, decrements the count.

So in this case, count was 3, we decrement it to 2.

Great. 2 greater than or equal to 0?

Sure is.

So, whoever called first using the P operation to ask for one of the pieces of hardware would get it, because it's available.

Then the second guy calls and he says I want to use one of the pieces of hardware He would call the P operation for that purpose.

We decrement the count from 2 to 1 Say yeah, fine.

That's greater than or equal to 0.

You too can get a piece of hardware.

We give him the second piece of hardware Then some other party comes in and says I would like to use one of those pieces of hardware We decrement the count from 1 to 0 and say 0 greater than or equal to 0.

Yes Yes, 0 is greater than or equal to 0.

We don't block him he gets to use the piece of hardware.

Now all three pieces of hardware are in use.

What happens if a fourth guy comes in while the three pieces of hardware are already in use?

Well, he can't use pieces of this piece of hardware, any of the three, because they're all being used.

They can only be used for one purpose at a time.

Therefore, we want to make sure he does not try to go ahead and use one of these pieces of hardware.

So what do we do?

He calls the P operation.

Current value of the counter of the semaphore that we are using was zero. 0, he decrements it to -1. -1 greater than or equal to 0?

No it isn't.

Therefore, this fourth guy gets put into the FIFO queue and blocked.

Sooner or later, one of the parties that was using one of the three pieces of hardware, it could be any one of them, it wouldn't have to be the first one, says "I'm done.

I will call the V operation to indicate I am not using that hardware anymore."

Increment the count.

And then check to see if somebody's waiting.

Oh yeah, that fourth guy's waiting. it, allow him to go ahead and make use of the piece of hardware that just got freed.

Again, no broadcasts.

If after the fourth guy tried to use it and he was blocked, the fifth and sixth guys try to use it and they're blocked too because nobody has finished using the hardware yet.

Nobody's performed the V operation.

Now the FIFOQ would contain three parties, four, five, and six.

And when, let's say, process two finished and released his piece of hardware, called the v operation, that would increment the count to zero.

And it would also take the first, but only the first party in that FIFO queue and wake him.

So process four would wake.

Five and six are still in the queue.

Sooner or later, when other pieces of the hardware become free, when the processes currently working with them are finished, they would perform the operations.

Five would get woken, then the next V operation, six would get woken.

After that, we would then say, okay, we've got all of our three pieces of hardware hard at work, our counter is set to zero.

What now?

Well, now what we're going to do is say, fine, let the counter at this point actually be set to zero, yes, minus one.

We're going to say when one of these guys finishes, set the counter to zero.

Great.

And then when somebody else finishes, set it to one and so forth.

And it would count up to the number of resources.

If nobody was using any of these pieces of hardware at the moment, it would go back to three.

Okay, so in a little more detail for the mutual exclusion case, which is really the most common case for using semaphores of this kind, how would we use it to do mutual exclusion?

Okay, let's go into one of these banking operations.

So, we got a bank account.

You can write a check on the bank account or you can possibly deposit money to the bank account.

You need to have synchronization on the total in the bank account to ensure, as we saw in previous lectures, that you don't get into a situation where you lose a deposit or lose a withdrawal.

Okay, so here is part of the code for it.

This is just the part where you write a check, where you take some money out of your account.

We'll use a semaphore to control mutual exclusion to this code.

So what would happen here?

We'd say, okay, fine, there's the semaphore S, we initialize it to 1 because we are using it for mutual exclusion.

When you're using semaphores for mutual exclusion, you start the count at 1.

Okay, so then what we do, when we want to write a check, we'd say we need mutually exclusive access to the balance.

Therefore, let's do a wait.

That's that P operation on the semaphore.

Okay, great.

And that is going to get us exclusive access to the account.

One of two things will happen.

Nobody else is using a critical section associated with this account.

Count goes from one to zero.

We get to go ahead.

Alternately, somebody else is doing something with the balance of this account.

They brought the one counter down to zero.

When we do our second p operation here, we go from zero to minus one, and then we block.

So if we don't block, then we do our business with the account.

We change the balance and so on.

And after we've done that, we perform the post operation, the v operation.

Just one side note here.

It is unfortunate that post, which is what we commonly use in English for this purpose starts with a P.

Because POST is the V operation, it's not the P operation.

WAIT is not, starts with neither a P nor a V, but it is the POST operation.

So, I know this is a little confusing, but you should be able to work it out.

Now, semaphores are theoretically correct, and you can do a lot with them, but there are some limitations.

They do certain things and they don't do other things.

They're simple.

They just have these very, very few features.

There aren't any other operations on a semaphore.

If you have a data structure which has those operations plus a few more, it isn't a semaphore.

And as a result of this, there are things you can't really do properly with semaphores.

Now, from a theoretical point of view, this wasn't a problem since the point was to theoretically work out how you can do synchronization.

From a practical point of view, I'm writing programs, this can be a problem.

Now, some of the features that they don't have that you might like to have in a synchronization mechanism is that you can't check the lock without blocking.

You can't say, "Gee, does somebody already hold this lock?

Oh, then I don't want it.

I don't want to block.

I want to go do something else."

Can't do that with semaphores.

Checking the lock either gives you the lock or it blocks you.

That's it.

No other choices.

You can't do something where you can have multiple readers or one writer.

With some forms of lock, you can do that.

Now, with multiple readers, it's okay, because if you're only reading a particular data value or data structure, you're not writing it, then other people also can read that data structure.

You can, however, have somebody who is writing the data structure and a bunch of readers, much less two parties who are each writing the data structure simultaneously.

So, there are some forms of locks where you can say, I'd like a multiple reader lock.

If somebody else holds the lock, but it's also only for read, fine, we can share the lock.

If somebody else holds the lock and it's for write, I got to block.

If I hold the read lock and somebody else wants to write for lock it for write, they have to block.

Can't do that at all with semaphores.

You get the lock or you don't get the lock and we don't care what you're doing with it.

Read, write, whatever, you got the lock.

Now, also semaphores make it very, very difficult to recover from a wedged V operation.

What do we mean by a wedged V operation?

You got a bug in your code.

You're never going to do the V.

You were supposed to do the V.

You'd finished, maybe even called the V operation, but there was a bug or there was a crash or something bad happened.

And the code associated with the V operation does not get properly performed.

Well, in particular, if the process that did that, that did the V operation, let's say crash, It's never going to perform a V operation because it crashed.

Therefore, you're probably never going to recover from this, and that lock is going to stay there forever until you reboot the whole system, which is undesirable.

For some forms of locks, there are ways around that.

Not for semaphores.

And priority scheduling can cause difficulties here, which we'll get to later.

There are ways around those difficulties, but semaphores don't do those ways.

Semaphores don't act that way.

However, despite these limitations, because they are such a foundational concept in synchronization, many operating systems, including the popular operating systems, do have packages that provide semaphores.

Now, as I said, semaphores aren't the best possible mechanism for performing high-level synchronization, for performing synchronization in your code.

Let's look at what you might want to use instead.

So we'll talk about mutexes and working with object-oriented locking.

Then we'll talk about some problems that we run into with locking and how we deal with those problems.

First, mutexes.

What is a mutex?

It was a mechanism that was developed for locking purposes primarily originally in Unix systems.

It had been built into Linux systems, which, of course, were initially set up to be clones of Unix.

So they are in pretty much all Linux systems and in many other systems that are based on the Unix code.

What do they do?

NewTexas lock code.

They say, this is a section of code that is a critical section.

Perhaps there's another section of code related to it.

They're both critical sections.

One of them is the read code for, one is the code for updating, adding money to your bank account.

The other is code for removing money from your bank account.

Between them, those two are critical sections.

You can't do one while you're doing the other.

So the code in question could be controlled.

Mutual exclusion could be provided if you were doing a Linux implementation using a mutex.

Generally speaking, mutexes are used when you have a single process with multiple threads, because typically they're provided through a threading packages.

Now, what's good about mutexes, from the point of view of working in programs, is that they have a low overhead.

There is not a lot of code associated with them, which is always good.

They're also quite general.

You can do many things with mutexes.

Now, mutexes protect critical code sections.

It's expected that they're going to be of limited duration, that you're going to have a small piece of code, several lines of code.

Not that you're going to lock something at the beginning of your program, run for three hours, and unlock it at the end.

They're not intended to do that kind of thing.

So you're expecting that you're going to hold a particular mutex for nanoseconds or milliseconds, not for minutes or hours.

And you're expecting that there are other threads that are going to operate on the same data.

If there aren't, you probably don't need a mutex.

But you also expect that all of the operations are going to occur in the same data space.

Or in other words, there is one address space associated with one process that is providing all of the access to the critical sections.

All the threads are working within the same address space, which as you should now easily see, will imply that we can implement the mutexes as being a piece of memory in the data area of that address space.

'Cause they can all look at that piece of data, determine if the mutex is free or not free.

Now, there are other things we might wanna lock.

Things that tend to be more persistent than perhaps that are held for longer periods of time than milliseconds.

A good example of that would be files.

We have file systems built into our Windows machines, our Linux machines, our Mac OS machines.

Most other operating systems provide file systems.

And the concept of a file system is that any process running on the computer has potential access to all of the files in the file system.

We can prevent some of them from using it using what's called access control.

We'll talk about that later.

But generally speaking, we're sharing the file system among all the processes.

That means that processes with different address spaces are using the same thing, the same file slash temp slash foo or whatever file it may be.

Now also, what are they going to do with this code?

Well, what are they going to do with this resource, this file?

They're going to open the file.

They're going to read the file.

They're going to write the file.

That's the kind of thing they're going to do.

As we will see when we talk about file systems, and as already has been alluded to, the amount of time necessary to do anything with a file that is stored on, let's say, a flash drive or a hard disk drive, is going to be multiple milliseconds just to do anything, like to open the file.

And typically, what you're going to do with a file in a program is you're going to say I'm going to open the file, and then I'm going to perform several operations on that file.

I'm going to read some of the file.

I'm going to read more of the file.

I'm going to read more of the file Now, every one of those operations, we expect every one of the reads, every one of the writes, is going to have multiple millisecond delays.

This could add up.

You can easily be required to hold mutually exclusive access to the file for seconds or even longer, a lot longer than for mutexes.

So the critical sections are likely to be longer.

There are different programs that do not share the same address space operating on these files.

And what happens if we're talking about a distributed system?

There's a machine over there.

My machine, your machine, another person's machine are all using files on that remote machine.

We'll talk about how you do remote file systems later, but effectively you can then say, "Well, gee, my machine, your machine, the other guy's machine, we sure don't share any address space.

We don't share any RAM.

We cannot look at each other's RAM.

Our operating systems can't look at each other's RAM.

Therefore, how are we going to provide mutually exclusive access?

So we have to think about those kinds of issues.

The way we do this is we lock objects.

Now, so far we're talking only about non-object oriented concepts of objects, more general things like a file is an object.

So in that case, what we'd like to do is say, OK, this is an object that is implemented by somebody, frequently the operating system.

Files are implemented by the operating system.

Therefore, we will tell the operating system we would like to lock this entire thing that you provide, this object, this file.

The mechanisms we're going to use to provide the locks in such cases are very likely to be quite specific to the type of thing we're locking.

So if we're going to lock, let's say, a piece of hardware, we're probably going to do it somewhat differently than if we're going to lock a file.

So here's an example of what we can do with files and locking in Linux.

So with Linux, we can perform an operation called flock.

FD stands for file descriptor.

That's essentially what you get after you've opened a file.

And you can take, in Linux systems, a file descriptor, and you can say, I would like to lock it in the following way.

So how can you lock it?

Well, you can have a shared lock.

This would be one of those locks similar to what I talked about before, where you allow multiple readers but no writers or one writer, but no readers.

So you could do a shared lock.

You could do an exclusive lock, which says basically, I want to have totally exclusive access to this, probably because I intend to write.

Therefore, nobody else should get read access.

Nobody else should get write access while I hold the lock.

And then, of course, you can release lock.

So that's another operation you can have supported here.

Now, if you think about this, you're saying, OK, this is on file descriptor.

What is a file descriptor?

When we get to file systems in a few lectures down the road, we will talk about precisely what we mean by a file descriptor.

But effectively, a file descriptor tends to be specific to a process.

You could have multiple threads in the process, but the file descriptor is specific to the process typically.

Therefore, what this would mean is that if there are multiple totally separate processes, they're not going to have the same file descriptor even if they open the same file. which should tell you something about what's going to happen if you use the flock operation to lock a file.

You're going to effectively lock the file for everybody else who uses the same file descriptor you do, and not lock the file for anybody else.

So if you have totally distinct operations-- process 1 opens file temp foo, process 2 opens file temp foo-- the flock performed in one of those processes will not apply to the other one.

This may not be what you were hoping for, But it is what FLOCK does.

Now also, there's another thing about FLOCK.

It's advisory locking.

What do we mean by advisory locking?

What this means is that we have put a little notation on this file descriptor saying, you know, somebody wanted to lock this.

If somebody else wishes to, they can say, gee, has somebody put that little note on the file descriptor saying they've locked it?

Oh, they have.

I won't fiddle with the file in question with this file descriptor until they release their lock and I get the lock.

But what if they don't do that?

What if they just said, ah, to hell with it.

I'm writing that file.

Well, that's what we mean by an advisory lock.

It is not enforced.

There is no mechanism that is built into the locking or the underlying system that says, oh, you can't do that because it's locked.

You have to obey the rules.

Now, the same thing is actually true with mutexes.

They also are advisory.

Generally speaking, some locks are advisory and some locks are enforced.

Enforced locking says, in order to get to this object, you must go through something-- a system call, an object method, an object-oriented code, or whatever it may be.

But there's something you have to do before you can get to this object.

We can, with an enforced lock, keep track in that code.

Whatever it is that's controlling the thing you want to get to, we can keep track of whether it's locked or not.

You can't get to the thing in question, the object in question, without going through our code.

If we have an enforced lock, when you try to go through the code, we check to see if there is a lock.

We check to see if there is a lock.

Do you hold it?

If you don't hold it, and if there is a lock and you don't hold it, then we reject your operation.

We enforce the lock.

So generally speaking, this is most commonly done at things that the operating system controls that you get at through system calls.

So that means the operating system will have the opportunity to say, oh, this thing is locked. can't do what you want to do.

Enforced locking happens whether the user who is trying to access whatever it is wants it to happen or not.

If another user or some other party of some kind, like let's say some code in the operating system, has put an enforced lock on some object in the system, nobody will be able to get at it without obtaining that lock.

Now sometimes this can be too conservative.

It can be unnecessary to do this, and yet you're doing it.

We have the choice of having advisory locking instead.

This is something where you say, well, you know, we got a bunch of people.

We're all cooperating.

We're all part of the same group.

We all want to achieve the same goals.

And one of the things we all want to achieve is proper synchronization.

Therefore, we agree that we're following the rules.

When we're getting into a situation where synchronization is critical, we are going to make sure that we check to see if one of our buddies has already obtained the lock.

And if he has, we're not going to try to break through that lock.

We're not going to try to access the thing in question, the lock thing, even though mechanically we can do so.

So you then expect that all of your other buddies are not going to use this object, whatever it may be, unless they've acquired the lock.

And if somebody else has acquired the lock, they're going to obey the lock. they're going to do what they should do, which is don't try to use that object.

Now, this gives you a lot more flexibility.

And it also, of course, gives you the opportunity to screw things up royally.

One guy isn't careful, doesn't follow the rules, suddenly your synchronization is screwed and everything goes to hell.

Now, mutexes, which we talked about before, and flocks, which we just talked about in the context of Linux file systems, are advisory locks.

They are not enforced.

Linux, in some cases, has a second type of file locking which is enforced.

This is called lockf.

So flock, lockf-- isn't it nice that they are so close together in name that you can so easily confuse the two of them?

If you're going to do Linux locking on files, it's well advised that you take a good hard look at which form of locking you're using and make sure you're using the right one.

What does lockf do?

Well, lockf actually is in many ways more flexible than flock.

It does work by saying, this is the file descriptor I want to put it on.

This is the file descriptor I wish to apply the lock to, so the particular file.

There are different commands.

You can get exclusive locks.

You can check to see if a lock is in place.

And if it turns out that it's in place, you can say, oh, I'm not going to try to use that file.

I'm going to go and do something else instead of blocking. you could unlock, of course.

But another interesting thing you can do is lockf allows you to lock part of a file.

To say, starting at byte 100, going to byte 300, I would like to lock that byte range.

Somebody wants to do something with byte 0 through 99, go ahead.

Wants to do something with byte 7,000, go ahead.

Can't do anything with bytes 100 through 300 while I hold the lock.

Now, the lock here applies to the file, the file itself, which means any process on a system where you're using this is going to have this lock enforced on that file.

I use lockf on /tmp/foo, and that file is locked.

If some other totally different process that I don't have anything to do with tries to access that file while I hold the lock on the file using lockf, then they will not be given access to that file.

They will not be able to use the file.

The code in the operating system that implements that file that allows people to open the file, read the file, write the file, et cetera, et cetera, et cetera, will not allow them to do so while somebody else holds the lock.

Now, if it is the case that I then close this, it's going to release all of the processes, file descriptors, and locks for that file.

OK.

Now, I said the locking is enforced.

Maybe, maybe not.

As we will see when we talk about file systems, in any given operating system, modern operating systems like Windows and Linux and Mac OS, there is not just one file system.

By which I mean, there is not just one set of code in the operating system that implements the file system.

There can be several that implement the file systems very, very, very different ways.

Now, LockF is specific to Linux.

It is not in Windows and so forth.

But even in Linux, some file systems, some of these implementations of the code that do the open, close, rewrite code, some of them provide enforced locking on LockF.

Some of them don't.

How do you know?

Well, you can look it up for the particular file system you're using if you happen to know what file system you're using.

I suspect many of you run on Linux systems at least occasionally, and chances are good that unless you do system administration type stuff or have a great interest in file systems, you haven't got a clue what file system you're running on.

You'd have to figure it out.

Then you'd have to figure out, well, does that particular implementation of file system code for Linux actually enforce lockf?

Maybe it does, maybe it doesn't.

Clearly, if one cares a whole lot about enforced locking of one's application and access to a particular file, one would want to know.

All right.

Now, moving aside from those kinds of locks, let's talk about some problems that we see with locks.

There's performance issues and there's overhead issues.

And then there's the issue of contention.

There are various problems that we see with lock contention.

That means when multiple people want to use a lock on the same resource.

Convoy formation, then under some circumstances, when you're doing priority scheduling in particular, you get a possibility of a phenomenon called priority inversion.

Let's talk about these issues.

First, performance of locking.

Now, sometimes locking is called in a library.

Sometimes it's called as an operating system call.

And force locking, it's usually an operating system call.

So that means there's going to be code.

You're going to run code.

Now, if it's an operating system lock, you're going to do a system call to get to the locking code, which means you're going to have a contact switch to the operating system.

You're then going to do code within the operating system.

There's going to be overhead associated with that code, as there is for all operating system code for all cases where you perform system calls. if you do this a lot, you get high overheads.

Now, if the locking code is built into, let's say, a library rather than in the operating system, of course, it's only going to apply to the particular process that you're running in because you won't have access to other processes.

But that means that you're not going to have the system call overhead.

But you are still going to call a routine, the lock routine, that does the locking code or does the unlocking code.

You're going to run some instructions in that routine.

Then you're going to return and go back to what you were doing before.

That means there's overhead associated with it, even if it's not in the operating system.

So you should be using locking not all the time.

You shouldn't be locking everything in sight.

You should, if you want to get good performance while still getting proper synchronization, have thought carefully and said, here are critical sections that must be performed in proper order, in controlled order, to ensure correctness of my code.

I am going to hold locks for those critical sections only when I need to.

Now, ideally, if you have designed your code well, taking these issues into account, then the critical section is going to be short.

It's not going to be a whole lot of code.

So if that's the case, that's great.

It's great in the terms of saying, well, I'm not going to hold a lock for very long, we'll see later for contention purposes, we really don't want to hold locks for any longer than we have to.

But it does mean that perhaps you're doing just a tiny handful of instructions while you hold the lock.

But you're also performing code to get the lock and code to release the lock.

It may be under some circumstances that the code you use to get the lock and to release the lock is more code than you use while you hold the lock.

That can happen.

That means that the overhead of performing the locking greater than the code that you were protecting.

All right.

Now, of course, one thing that could happen when you try to get a lock is you try to get the lock and you don't get it because somebody else holds it.

What's going to happen there?

Typically, you're going to block.

Your thread or your process is going to block because it couldn't get the lock.

You can't go ahead with whatever you're doing until you get the lock.

So you need to block.

Now, if you were talking about blocking of something that is understood by the kernel, a process in pretty much any system, a kernel-level thread in an operating system that provides kernel-level threads, then when you block it, you're going to be trapping to the operating system.

The operating system is going to take over, effectively the same kind of overheads as a system call, and it is going to do a bunch of stuff. that it's going to perhaps go back to user code.

But first it has to do a bunch of stuff.

And in particular, when the lock becomes free again, your process is blocked because you tried to get the lock, you couldn't get the lock, you're blocked.

Well, we're going to do extra code because you're blocked.

Like we're going to set in the process descriptor of this process, a bit indicating you are blocked.

We're going to keep track somewhere of why are you blocked so we can know when something happens, whether we should or shouldn't unblock you.

At any rate, you're going to do code to do that.

Then eventually the thing happens and you unblock, which means there's more operating system code.

Operating system code to say, oh, the thing happened.

So we're probably trapping to the operating system to deal with the fact that the thing happened.

And oh yes, is there somebody who was waiting for that thing to happen?

Let's see, yes, here.

Here's a notation that somebody was waiting for.

It's this thread over here.

It's kernel level thread.

Let's go to that kernel level thread.

Let's unblock him.

Let's check our scheduling to see if we should schedule him instead of somebody else.

There's going to be code to do that.

Effectively, this is going to be very expensive.

All the cost of locking plus all of this extra cost because you're blocking.

So it may be microseconds, which is quite a lot of time, and milliseconds if it turns out that there's really bad stuff happening.

OK, so what is the probability that you are going to have a particular cost?

What's your cost going to be, probabilistically speaking?

Sometimes you get the lock, sometimes you don't get the lock.

One cost when you get the lock, a different cost, higher cost, when you don't get the lock.

So what's your expected cost when you perform one of these locking operations and you're not sure what the result is going to be?

Well, here's the cost.

Either you're going to get the lock or you're not going to get the lock.

If you don't get the lock, that was a conflict, meaning somebody else holds the lock.

There's a probability that somebody else holds the lock.

So the cost is going to be the probability that somebody else holds the lock times the cost if you don't get it, if you block.

Plus, of course, the more lucky case, the cost of getting the lock, you have to call the lock operation, times 1 minus the probability that somebody else holds it, the probability effectively that nobody holds it.

You add up those two things, and that's the cost that you expect to pay.

Well, that's the cost you expect to pay if things are going relatively well.

What happens if things aren't going relatively well?

Why might they not go relatively well?

One reason is you have a very popular resource, a very popular resource that must be accessed with mutual exclusion, implying that to use this resource, you must obtain a lock.

So let's say you have a multi-threaded program, you have a 100 threads in your multi-threaded program, they're all making use of a data structure.

That data structure is a read-write data structure, and in order to make sure the data structure isn't messed up by improper synchronization, you're going to have a lock on the data structure.

Any one of the hundred threads that wants to use that data structure must obtain the lock before it can use the data structure.

Once it gets the lock, it uses the data structure, it releases the lock.

Okay.

Now, on a good day, of course, one of the threads might hold the lock at any given time, but the other threads are doing other things.

They aren't touching the lock.

They aren't interested in that data structure.

Maybe sometime later they will, but by that time, the thread holding the lock will have released the lock.

That's a good case.

What happens if in a short period of time, several of the 100 threads all want to use the data structure, meaning they all try to obtain the lock.

Somebody wins.

Somebody gets it first.

They start using the data structure.

The other threads, let's say there are four more of them.

They want to get it.

They try to get it, try to get it, try to get it, try to get it.

All four of them.

None of them can get it because that first thread has got the lock.

Okay.

The other will effectively be put into a queue somehow or other, whether it's a semaphore or some other structure, they'll be put into a queue.

Condition variable, whatever it may me.

Waiting for the lock to become free.

Okay.

So now there are four people waiting for the lock to become free.

The guy who holds the lock releases the lock.

Great.

One of the people who is waiting in line, five, four, let's say, gets to use the lock.

He gets the lock.

Now there are three people in line.

Now, the fact that the first guy was doing some work and then had some overhead for releasing the lock, and then there was overhead for determining who is the next guy who should get the lock, and he has started up again, and he starts running, and he's running code holding the lock, meaning nobody else can use the data structure, and eventually he'll be done, but there'll be some time before he's done because he has to use the data structure.

That's why he obtained the lock in the first place.

So what's going to happen during that time?

Perhaps two more of the hundred processes say, I want the lock.

There are three in the queue.

Now there are two more who want it.

They get added to the queue.

Now there are five in the queue.

Okay, sooner or later, second guy hold the lock.

He releases the lock.

He does all of his work with the mutually exclusive access he requires on the data structure.

He releases his lock.

Wonderful.

Now we go to the queue and we wake up the third guy.

There are four guys left in the queue.

Now the third guy is going to do some work and he's going to eventually release the lock.

And while he's doing the work and before he releases the lock, three more guys show up.

There were four guys in the queue.

Now there are seven guys in the queue.

If this keeps going, we get what's called a convoy.

We get a whole group of processes or threads that are sitting in a queue, waiting for their turn to use a resource that only one can use at a time.

They're all blocked.

None of them are doing any work.

Well, clearly, for the processes that are sitting in the queue and the one that's running, there's no parallelism.

One of them gets to run.

Out of all the threads or processes that are trying to use this critical resource, this mutually exclusively controlled resource, one at a time can use it.

The rest wait.

So in the example that I was discussing when I finished up that example, one guy was using it and seven guys were waiting.

Clearly, we no longer have any possibility of parallelism among those eight threads.

So each one is going to run in turn.

And until the convoy is cleared out, there will be threads sitting around blocked, waiting, not providing parallelism, because they can't get to the resource.

Now if it keeps being the case that more and more of these overall hundred threads keep trying to use this data structure, they too will be put into the queue.

They will join the convoy at the end of the convoy.

Any possibility of parallelism with these other threads also disappears.

In this case, we have what is called a bottleneck.

A bottleneck is something where only one can get through at a time and you have a queue of people who are waiting for their turn to get through.

What's the probability that this will happen?

Well, here's an example that people have worked out from theoretical situations.

This isn't based on, I believe, actual performance measurements, but it's demonstrably true for the assumptions that are made.

So this is a graph that is showing for different numbers of threads that you have a critical section for.

You obtain a lock before you perform the critical section.

So on the x-axis, this is a log scale on the x-axis.

What is the fraction of the total time each thread performs its processing that it spends in the critical section?

On the y-axis, there is what's the probability when somebody asks to obtain the critical section that somebody else already holds it?

OK.

Well, as you can see, if there is 1% of your time, 0.01 of the time for each thread that it is going to spend in the critical section, meaning it needs to hold the lock for that period of time.

For two threads, the probability of the conflict is pretty low.

Not very likely that you're going to run into somebody else holding the lock when you ask for it.

That looks to be down like below 1% chance of that happening.

With 10 threads, it gets to be a little bit more likely.

Now, the y-axis here is not log scale.

So this means that if you have 10 threads, which isn't a vast number of threads, we're not talking thousand threads or anything, you have just 10 threads, there's about a 10% chance when you ask to have access to the critical section that somebody else holds it, even though each thread is only holding the critical section for 1% of its time.

Okay.

Well, not great, but you know, still 10% chance.

How about if you spend one 10th of your time in the critical section, each when it's running 1/10 of its time is in the critical section.

Now, if you only have two threads, you're talking about nearly a 20% probability that when you want the critical section, the other thread has it.

If, on the other hand, you have 10 threads, as you can see here, you're talking about a 70% chance that some other thread, one of the other nine threads, holds the critical section when you want it.

And as you get up to a very high probability of spending your time in the critical section, if threads spend most of their time in the critical section, you get to the point where it is near certainty that whenever you want the critical section, somebody else has it.

Then you are almost certainly going to get into a convoy situation.

This is a curve that should make it very, very clear that if you require several threads to hold a particular critical section in order to achieve proper synchronization.

It is really in your interest from a performance perspective to make sure that each thread holds a critical section for as short a period of time as possible.

OK, what's going to happen with convoys?

OK, well, the probability that you get a conflict, if there is no FIFO queue-- there's not already a queue of people-- the probability that when you ask for a lock, you get a conflict on the lock, is essentially Okay, take the time that's in the critical section over the total amount of time.

The rest of the time you're not in the critical section.

So, take 1 minus that, and then to the power of threads, because every thread is going to do the same thing.

That's the presumption here.

Not always true, but let's say that's true.

Okay, so the bigger this 1 minus T critical over T total is, and the higher the number of threads is, the probability of conflict is going to go down, excuse me, up.

Because 1 minus this T critical over T total will always be less than 1.

More threads you have, the lower the power, the smaller the number of the power becomes, you subtract one, that from 1.

Get enough threads, enough time in the critical section, then the probability of conflict becomes nearly 1.

Now that's if there's no Q.

What if there's Q?

Well, if there's a queue, then of course it is still the case that you may have to wait for the guy who holds the lock, the one guy who holds the lock.

But what if there are a bunch of people in line already?

Now we're assuming we're gonna do FIFO here.

So if there are a bunch of people in line already, let's say three other threads are already queued waiting for this lock, you're gonna be number four in the queue.

So then it's not just going to be the time critical over time total, but it's going to be how long do you have to wait?

before you get your turn.

This can get to the point where the probability of conflict gets to be incredibly high, very, very, very high.

So, T-wait gets to be very, very, very long.

If the time to wait reaches mean inter-arrival time, what's mean inter-arrival time?

The presumption here is that you have threads running and running and running, and sooner or later, they ask to obtain the lock.

What is the average time between two threads to obtain the lock.

That's what we mean here by mean interarrival time.

How long between one guy asking for the lock and the next guy asking for the lock?

If T-weight reaches the mean interarrival time or exceeds it, you got a problem.

Because what this means is the line is permanent.

Now that doesn't mean that your system freezes, but what it does mean is you have no parallelism.

Because ultimately, everybody gets in line.

Everybody except one guy, the one guy who is fortunate enough at the moment to hold the lock.

Now he will finish, assuming you don't have bugs in your code, and he will release the lock and the next guy in line will get a chance to do something.

But if the amount of time that they are waiting is greater than that amount of time it takes this guy to release his lock and the next guy to get it, the line's gotten longer or at At least it has never gotten shorter.

That's bad.

So what performance do we expect to see if we get into one of these resource convoys situations?

Now, this is a curve we've seen in previous lectures.

Offered load, how much work are you trying to get done?

Throughput, how much work are you getting done?

This, you remember, is the ideal curve for offered load versus throughput.

Your system has a capacity.

It can run a certain number of instructions per second, for example.

As you offer more and more load, you get to the point where you've reached the total number of instructions per second you are capable of performing.

No matter how much more load you offer, you can't do better than that.

The throughput you're going to get under ideal conditions is exactly what you can do per second, what your hardware is capable of.

This is what happens when you get a convoy.

Now, the red curve is for a while like the red curve we saw before.

It diverges from the ideal, which is not so great.

But here, when you get a convoy, you hit a cliff.

You've gotten to the point where everybody is waiting in line for the same thing.

One party can use that thing at a time.

Therefore, the throughput you're getting is going to be essentially the throughput of one thing at a time doing its work, everybody else doing nothing.

No parallelism.

This is bad.

OK, so let's talk about another problem that we see when we use locking, in particular, when we use locking in combination with priority scheduling.

You remember that priority scheduling, I hope you remember, was a form of scheduling where you said some processes or threads are more important than other processes or threads.

Let's run the important ones in preference to the less important ones.

Now, of course, we talked about how this could cause starvation, and that was undesirable.

But here's another problem that can be caused. if you also have locking in your system.

So systems that use both priority scheduling and locking may have a problem with priority inversion.

What's the problem?

So let's say you have a low priority process P1, and it happened to be at some moment the next process to run because there's nothing of higher priority to run.

So you ran it.

It acquired a mutex M1, okay?

So it's got M1. a higher priority process comes into the system, P2.

Well, in a priority scheduling system, one that uses preemptive priority scheduling, that means that P1 with a lower priority is going to be preempted by P2.

Therefore, P2 will run in preference to P1.

But P1 still holds the lock on that mutex.

OK, what if P2 wants that lock?

Well, then we've got a problem, now don't we?

This means that for a period of time, P2 is going to be reduced to the priority of P1, which isn't desirable.

P2 is a higher priority process.

We can't run P2 until we finish running P1.

Not desirable.

So this is inconvenient and it can be indeed fatal in some circumstances.

And it can get a little bit more complicated than this, as we will see.

Here's an example, real, well, not this world.

This happened on Mars, literally on Mars.

So NASA, some years ago, sent up a robot, the Mars Pathfinder Rover, which this is an example of what it looked like, not the one that was really on Mars, but same piece of equipment, the example that they used on Earth to test things out.

So it was a little robot running around on wheels, and it was running around Mars once it had been sent up to Mars by spacecraft doing all kinds of scientific tests.

Now, after they had sent it up there and it had been running around for a while, NASA noticed we got a problem here.

Every so often, the entire system resets, shuts down, restarts, gets going again.

And this was a fairly lengthy process, and moreover, since they didn't understand why it was happening, they were worried that this might be something that was going to be truly fatal that would make the rover not work anymore.

So they needed to find out what the problem was, and it proved to be kind of difficult to figure out what this problem was.

It was priority inversion.

So let's talk about the specifics of how this happened.

The hardware that they built for this purpose, for the Pathfinder rover and its control, was special purpose hardware, as one would imagine, you know, build vast numbers of rovers that go up to Mars.

So NASA had built this specifically to be put into this rover.

And they wanted to have computing power here because they were doing many, many kinds of complex things and the rover had to be autonomous.

It had to be able to do things on its own.

Too long a delay between Earth and Mars for signals to get there to do things in real-time control.

So it had to be autonomous.

Therefore it had to have computational capabilities.

Therefore it had to have some kind of system software.

Instead of building their very own custom-built system software, NASA looked around and said, "Well, what do we got out there?

Oh, VXworks."

VXworks is a real-time operating system.

As you may remember, real-time systems are systems that ensure that certain things, computational activities, occur at particular times, often, as in the case here, because you want certain things in the real world to happen at particular times.

Like if you see a rock in front of the rover, you want the rover to turn, and you want it to turn before it runs into the rock.

So VXWorks seemed like a very good choice.

It was a very well-known real-time operating system, respected, did a good job.

So they said, "We'll use VXworks."

Okay.

Now, there were many, many things they wanted this rover to do.

You don't get a chance to send stuff up to Mars very often.

So once you send something up to Mars, you want to make sure it's got everything you want on it.

So there was all kinds of things the Pathfinder rover was supposed to do.

And some of them were more important than others.

Therefore, they said, "Gee, priority scheduling sounds like a good idea.

VXWorks can do priority scheduling, all we have to do is say that's what we want to do.

So let's use preemptive priority scheduling on our VXWorks that we're installing in this rover.

High priority task will get the processor and we'll try to make sure we don't have starvation and other problems by proper design.

Okay, now the rover had all kinds of stuff on it.

It had a bunch of sensors, it had actuators, it had motors, it had communication devices that that would send signals to Earth and would receive signals from Earth and interpret them as commands that it should do the following thing.

So it's got a lot of different components.

Now, these components had to interact, not just the hardware components, but the software controlling them.

So there's going to be information that one component, like the thing that detected that there was a rock ahead of you, would have to send to the component that says, take the right turn so you don't run into the rock.

And the way they did that was to say, let's set aside some portion of the RAM on this hardware as an information bus.

Now, this wasn't a bus in the standard hardware sense of a bus.

It was essentially something where all the different processes running on this robot were able to interrogate the bus.

So it was shared memory.

It was a piece of shared memory that all the processes could look at.

Now, clearly, given that you had a lot of processes doing different things, and you had this piece of shared memory, you really needed to have good control over synchronization of that piece of memory.

You didn't want somebody inadvertently writing over the signal that there's a rock ahead of you before the guy who was supposed to make the turn noticed that there was a rock ahead of him by seeing the information in the information bus.

OK, so they knew they had to have synchronization.

And they said, fine.

We know when we're going to the information bus.

We got no problem with that.

We will make sure that we are controlling synchronization of the information bus with a mutex.

And we'll make sure that everybody who uses the information bus, everybody who reads data from it, writes data to it, holds the mutex before they do that and doesn't hold it for very long.

They'll only hold the mutex for a very short period of time.

Great.

OK, so that's what they wanted to do.

And that's how they built their code.

And they built their code quite carefully, as people building code for NASA have a tendency to do. because, hey, once you put it on the spacecraft, it's on the spacecraft.

As it happened, there were three tasks out of the many that they were performing that were going to cause a problem.

Now, the bus, the information bus, was a limited size area of RAM.

There's only a certain amount of space there.

So you had to keep track of what part of that space have I used and what part haven't I used.

And you needed to clean it up to say, OK, this part that was used, it's not important anymore.

Get rid of it.

So it becomes free.

We can use it for other purposes.

So there's a bus management task that would do that.

And this was a very limited amount of memory.

So they had to run this fairly often.

They would run the bus management task quite frequently.

However, they made sure that when the bus management task was running and it acquired the mutex for the information bus, it would only work for a very short period of time.

And then it would release the mutex.

So this task, let's call it P1, would hold the Mutex frequently because it was being run very, very often, but it would only hold it for a very short period of time.

Since we really had to make sure, they really had to make sure the information bus had free space, they ran it at a high priority.

Okay.

Then, there's a meteorological task.

Believe it or not, Mars has weather.

Limited weather, but weather.

And they had meteorological instruments attached to the rover that would keep track of the Martian weather.

What was the temperature?

What was the wind speed?

All that kind of stuff.

Now this was of some importance.

They wanted to know what Martian weather was like, but they presumed that Martian weather was not changing on a millisecond basis.

Weather tends not to.

So they said this can be a low priority task.

We're going to run it just occasionally.

And as it turns out, it doesn't actually need to do that much work anyway, but it does need to take the information it's gotten from its various sensors and put them someplace so Ultimately, we can send that information back to Earth.

Where are we going to put them?

Let's put in the information bus.

So this task would only run occasionally.

It wouldn't be running all the time.

And it would be low priority.

If there's something more important to do, delay this task.

We don't care if it has to wait 30 milliseconds before it gets to run.

That makes no difference.

But it did, of course, when it was using the information bus.

Have to get the mutex on the information bus, which would help for a very short time.

Okay, then there was another task.

Obviously, if you just sent this thing up to Mars and you never got any information back from it, you kind of wasted an opportunity.

The purpose of putting the Martian Pathfinder Rover up there was to gather information, and that information would only be useful if the information was sent back to Earth.

So there were communication devices attached to this rover that would periodically say, The most important thing to do right now, or at least a very important thing to do right now, is to send information back to Earth.

Now, this turned out to take a certain amount of time.

It was kind of slow.

So it's going to run for a long time.

Therefore, let's not do it too often.

Once a day or something like that, you know, not too often.

And we need to have it happen when it really needs to happen.

So it's going to have a reasonably high priority.

But there are other things that are more important, like making sure the information bus doesn't get congested.

So it'd be a medium priority task.

So we have three tasks.

High priority bus management task P1, low priority meteorological task P3, medium priority communication task P2.

P2 is long, P1, P3 are short.

The priority P1 is greater than P2 is greater than P3.

Okay, now this all sounds perfectly reasonable. went wrong.

Why did we run into a problem here?

Well, let's say that nothing of high importance is happening, nothing much of any importance is happening, and it comes time for the meteorological task to run.

So bang, the meteorological task runs.

Low priority, but it's ready to run.

We run it.

And in order to do its work, in addition to getting the data out of its sensors, it's got to put it on the information bus.

So it acquires the lock on the information bus.

At the very moment that it is acquired the locker shortly after, before it releases the lock, the bus management task, which is high priority, kicks off.

Now the bus management task is high priority.

The meteorological task is low priority, therefore the bus management task preempts the meteorological task.

The meteorological task at the moment of preemption, though, holds the mutex on the information bus.

Now the whole purpose of the bus management task is to clean up the information bus.

Therefore, it has to acquire the lock.

So practically, the first thing the bus management task would do once it started running is to try to acquire the lock.

But it couldn't because the lock was held by the meteorological task.

Therefore, the bus management task is going to be blocked.

Now, if this was all that happened, usually it's not a problem.

You do the little bit of work for the meteorological task at low priority, fine, you do that work. releases its lock, bus management task is high priority, runs next, gets the lock, and it gets to do its business.

Unfortunately, every so often, the following thing happened.

The long communication task woke up before the meteorological task released the lock.

What happens?

Here's what happened.

So, over the course of time on this x-axis here, here's what a single core processor.

So only one thing was running at a time.

So here's the meteorological task.

It started running.

It says, lock the bus.

And since nobody else held the lock, it got the lock.

Bus management task starts to run.

It's more important than the meteorological task.

So it interrupted the meteorological task as soon as it started.

And the very first thing it said to do is lock the bus, but it can't because the meteorological task holds the lock.

Then, shortly after that, but before the meteorological task finishes and releases its lock, the communication task starts.

Now, it turned out the communication task didn't actually need to use the bus at all.

It had already had all of its data stuffed away somewhere.

So it just needs to say, "Okay, just start running and running and running and running and running and running and running.

Feed all that information back to Earth.

Takes forever, but hey, it's important, relatively important.

So let's keep running that task."

Now, what else could we have done?

Well, we could have run the meteorological task.

It was ready to run.

It could run, but it's not nearly as important as the communication task.

Now, is it?

We'd like to run the bus management task.

It's more important than the communication task, but we don't have the lock for that one.

It's blocked.

So why don't we run the one we can run?

We can run the communication task.

So B, the bus management task had a higher priority than C, the communication task, but it can't run because it doesn't have its lock.

It's waiting for the meteorological task to release the lock.

The meteorological task isn't going to release the lock because it can't release the lock till it runs again.

That's a piece of its code.

Therefore, it's got to run before it can release the lock.

But it can't run because it's not a high enough priority.

The communication task would like to run as a higher priority than the meteorological task.

Therefore, the communication task runs.

So the meteorological task will not start again until the bus management task completes.

And the result was that we had a high priority task that doesn't run and a low priority task, communication task here that does.

Now, what was the effect of this?

Okay, now, when you build a robot that's going to another planet, you have to assume that things are gonna go wrong.

So you build your system to say, Let's keep an eye on things.

And we know that we may not have figured out all of the bad things that might happen.

So let's keep an eye on what's happening.

And if it looks like we got a problem, we better do something about it.

Even though we didn't know ahead of time that such a problem might occur.

We don't know why it occurred, but we got to do something.

Now, we all know what we do on a computer when we get into an unknown state where nothing appears to be happening correctly.

We reboot.

That's what they did.

They had what was called a watchdog timer.

The watchdog timer was a special task.

It would go off every so often, not too frequently, but every so often.

And it would look around to say very, very quickly, I'm going to run an extremely high priority, higher priority than anything else.

I'm going to look around and see what's happening.

And if I don't like what I see, if something that should have happened isn't happening or something that shouldn't happen is happening, I am going to say we got to take action.

What action do we take?

Well, let's reboot.

Now, because they did not want this watchdog timer to be controlled by people holding locks, it wasn't locking anything.

But it was rebooting the system if it didn't like what it saw.

One of the things that it said should happen is the bus management task should have run sometime within the past period of time and should have completed.

If it didn't, that's a problem because nobody's cleaning out the information bus. that could be seriously problematic.

We could have a bug in the bus management task, for example.

We better do something about that.

What can we do?

Reboot.

They rebooted the system.

So whenever you got into the situation that we saw before, the communication task typically would not complete because the watchdog timer would go off and it would say, ah, information bus cleanup task has not run.

Oh my God, something terrible has happened.

Reboot the system.

This was very difficult for NASA to figure out, but eventually they did.

So they knew ultimately after they worked out everything that was going on here, what's happening?

Well, we have a low priority task that's running because a lock is held by other tasks, not by the low priority task, but by other tasks.

And the low priority task is a long task.

Okay, and it's preventing an important task from running because that task needs a lock.

They figured it out.

What do they do?

They said, okay, well, which are the tasks The meteorological task holds the lock.

The bus management task needs it, can't get it.

The communication task is running for a long time, preventing the meteorological task from running and releasing the lock.

Therefore, the bus management task isn't able to get the lock.

The medium priority task is preventing the high priority task from running.

That's what's going on.

How do we solve it?

There's a very simple solution to this.

Essentially, what you would do is say, OK, lets temporarily increase the priority of the meteorological task.

High priority task is waiting for it to release its lock.

We'd like to run the high priority task as soon as possible, but in order to make sure we don't screw up synchronization, we need to complete this low priority task.

Why don't we bump the priority of the low priority task temporarily to the priority of the task that is waiting on its lock?

This is called priority inheritance.

Very well understood concept. well understood before the Mars Pathfinder rover and the effect here will be that in our example the meteorological task is going to have its priority bumped up to the level of the bus management task when the bus management task is blocked waiting for that lock.

Therefore the meteorological task which is not blocked and is high priority at the moment will will get to run to completion, release its lock.

That will allow the high priority bus management task to acquire the lock and run to completion.

This is a general solution to this kind of problem.

As I've said, priority inheritance problems are well-known, well-understood.

This was not a new discovery for this particular case.

So here's how it works in action.

We have the low priority meteorological task, the high priority bus management task, Lock held by the medium priority, the low priority meteorological task.

High priority bus management task wants the lock, can't get it.

We bump the priority of the meteorological task.

In comes the communication task.

Normally it outranks, has a higher priority than the meteorological task, but we've temporarily increased the priority of the meteorological task.

So he doesn't run.

Instead, the temporarily high priority meteorological task runs.

Sooner or later, it releases the lock.

Maybe it's still got some work to do after it releases the lock, but the moment it releases the lock, its priority gets decreased to its original level.

So B runs, gets the lock, gets to run.

Once it's done, we can run C.

Okay, as it turns out, this was a very simple fix once they figured out what the problem was.

VXworks, the people who designed that real-time operating system, said, oh, we know all about this particular problem.

So we'll just build into our operating system the ability to have priority inheritance just the way you need it.

How do you get it?

Change a bit in your configuration and reboot.

So all you had to do to make this work, to solve this problem for the Pathfinder Rover, was to change a bit in their configuration file, which they could do remotely-- not easy, but they could do it-- and then reboot.

And from that point onward, the problem never occurred again.

OK, now, priority inversion aside, we saw other problems with locking.

Performance problems.

There's the overhead problem.

There's a contention problem.

How do we deal with the overhead problem?

Well, what we don't do is we don't say, let's write more efficient code to do locking.

Why?

Because we've known that this is code that has an overhead problem for decades.

We know it's important code.

Therefore, smart programmers who understand these issues and understand how to code this stuff very, very well have been poking away at this code for decades, trying to make it as efficient as they possibly can.

There is not much chance that you can write more efficient, correct code to do locking than already exists in the standard mechanisms that are provided for you.

So there's no point really in trying.

Use what you got.

That's going to be the cost.

You're not going to do better.

The other choice is to say, well, as long as we don't have contention, then presumably, we're going to not run into convoys.

We're not going to run into these problems where I have to wait for a lock because I have a high probability of contention.

How can I reduce contention?

Well, there are some options here.

One, get rid of the critical section.

This is a wonderful solution.

You could try to say, well, if I have to have the critical section, one thing I really don't want to have happen is I don't want the code that holds the lock to be interrupted and put into a queue and not run for a long time.

So let's not preempt during the critical section.

Also, it would be nice if I don't hold that lock for very long.

How long do I hold the lock for?

Long enough to run all my code.

The less code I run, less time in the critical section.

Another option is to say, well, I only hold the lock when I am running the critical section.

At some point, I got around the critical section.

But maybe I don't need to run it all that often.

Maybe I can delay running of the critical section for quite a long time and only run it occasionally.

The less often I run it, the less contention I'm going to cause.

Another option is to say, well, I got this resource that is being controlled by the critical section of the code.

If I can say, I don't need exclusive use of that resource, or maybe I don't need use of the entire resource, just exclusive use of a part of the resource, maybe I can reduce contention that way.

So that says, don't use the resource exclusively, or divide the resource into pieces and have your requests spread out over different pieces, each one being controlled separately.

We'll look at this a little more carefully.

First of all, getting rid of the critical sections.

This is great if you can do it.

If you can give everyone their own copy of whatever it is, and it's theirs exclusively, they can do whatever they want.

There's no critical section.

If you can do your work without using the critical section, figure out a way to do that.

No critical section, no locking, no blocking, no contention.

If you are able to use atomic instructions, which means you're writing an assembly code at best, well, that's sometimes possible.

You won't need to obtain a lock if everything you do with critical section can be done in one instruction, but that's hard.

So generally speaking, this approach, getting rid of the critical section, is wonderful if you can do it, but you usually can't do it.

Now, you really do not want your critical section to be preempted.

If you're running on a single core processor and your code running the critical section is never preempted even by clock interrupts, then there are no synchronization problems, which you really can't do.

But one thing you can do is you can say, "I don't want to be preempted for other purposes."

So it would be a very bad idea to be holding a lock and say, "Oh, I think I'll yield."

Not a good idea.

It's also a bad idea to say, "Oh, gee, I'm in the middle of the critical section.

Now would be a good time to make a system call."

You'd prefer not to make other system calls while you hold a lock because they're going to take a long time and moreover, scheduling may say, "Oh, you just dropped out of your code and gee, maybe it's a good time to check to see if I should schedule somebody else instead.

How about this other guy?"

You may still be runnable, but you're not running.

Not a good idea either.

Another thing you can do is cut down the size of the critical section.

Now, in particular, you want to avoid doing the blocking operations I just discussed.

So if you hold a lock, you don't want to do a file read operation, for example.

So you don't want to allocate more memory before you take the lock.

You don't want to do other forms of I/O, like let's send the message while I hold the lock.

Don't do that if you can avoid it.

Generally, you want as little code in the critical section as possible.

So you have to figure out, what is it that I have to do related to this serializable resource, something where I need mutually exclusive access.

What are exactly the things I need to do?

And say, I will organize my code so that I do anything else I can do ahead of time.

Then I acquire the lock.

I do exactly what I need to do to avoid synchronization problems.

Then I release the lock and then I do other stuff.

Now, the difficulty with this is you may have code It doesn't really make a whole lot of sense if you weren't thinking about synchronization problems.

Why am I doing all this stuff first and all this stuff later and all this stuff in the middle?

Wouldn't it make more sense if I changed the order of those operations?

Well, the reason you didn't change the order of those operations is because that would have increased the amount of time that you held the lock, which would have had potentially bad effects.

So you may have more complicated code.

You may have to fiddle around a little bit with your code and do things that ordinarily you wouldn't do, you know, like creating temporary variables and and putting values in temporary variables and checking them again later and so on and so forth.

Another thing you can do is say, let's not call critical section very often.

If you don't call it very often, then the chances that when you do call it, there's going to be a conflict are reduced.

And if that's true for everybody who's using it, there can be a significant reduction in the probability that you ever fail to get your lock.

If you always get your lock, then you're going to get mutual exclusion at the cheapest possible cost.

So how can we do it?

Well, just don't use it very often, if that makes sense.

Another thing you can do is you can batch operations.

So a simple thing would be to say, OK, if I am going to, let's say, update a data structure that contains inventory, and I've got a whole lot of operations that say, here's changes to inventory, here's incoming changes, incoming changes, we sold this, we bought that, et cetera, et cetera, as I get each of those, I could just go into the file or the data structure containing the inventory, lock it, make the one change, unlock it, go back, get another change, go into the data structure, lock it, make the change, unlock it, et cetera.

Or I could say, let's do all of that set of operations that we've got of these are the changes to the data structure.

Let's make a temporary structure containing all those changes.

Once we have them all, or a large number of them, Then let's go to the actual data structure with the inventory.

Let's lock it.

Let's make all the changes in one operation and unlock it.

Chances are that'll be a lot cheaper in terms of how much time total you spend in the critical section.

Now, one example of doing this is what's called a sloppy counter.

What's a sloppy counter?

Let's say that you are trying to keep track of how often some particular operation has occurred.

And let's say you have multiple threads that all doing the operation.

Now, if every thread is potentially doing the operation, then the simple way of saying let's keep track of how many operations have occurred is to say somewhere I got a counter.

Multiple threads, they share memory, let's have a location in memory that is the counter of how many times things have occurred.

Let's update that counter.

Okay, so whenever a thread does one of the things, it goes and updates the counter.

Now, unless you have been able to do lose track of some of those situations.

You remember the counter equal counter plus one example?

So even if you wrote a piece of C code that said, this is the shared counter, counter equal counter plus one, you may lose track of some of your updates, some of the occurrences that you were supposed to keep track of.

So the way you would simply solve that problem is you would say, lock counter, counter equal counter plus one, unlock counter.

And every thread, every time that it wanted to update the counter would do those operations.

OK, if you're updating that counter frequently, you're going to do lots of locks, lots of unlocks.

There's going to be contention, maybe even a convoy formed.

It's going to be unfortunate.

What could you do instead?

You could use sloppy counters.

What you do there is say, every thread is going to have its own private count of how many times it has seen the event, the update or whatever it may be.

Whenever it sees an event, whenever this particular thread, thread seven, sees an update, it updates, it adds one to its own local count.

Its own local count is just what thread seven has seen.

It doesn't have any information about what the other threads have seen.

It certainly doesn't have information about the global state of how often this event has occurred totally across all threads.

Therefore, it's only going to have to update a piece of memory that nobody else is updating.

Only this thread is to have mutual exclusion applied because nobody else is touching that.

Now the problem with that is that you now no longer have one counter that keeps track of how many things, how many updates, how many events have occurred across all threads.

So what you could do is you could say, well, we will also have one of those, but if you update that every single time that something happens in one thread, you still have to lock that.

What you could do instead is say each thread has its own local counter.

Every time its local counter gets up to 10, it's seen the event 10 times, we lock the global counter, add 10 to the global counter, unlock the global counter, reset our local count to zero.

What this would mean is that you would reduce the number of locking operations by 90 percent, because nine times out of 10, whenever a thread saw an update, it was not going to lock the global counter.

So that's great.

It does have a little difficulty, which is if you're in the middle of running your code, you haven't reached the completion of your code, or if it's code that runs forever and isn't supposed to ever complete, you can't just go into the global counter and say, "This is the number of times this event has occurred," because you've got 30 different threads.

They each have their own local copy.

Some of them may have sent their combined set of updates, the 10 updates they were waiting on to the global counter.

Some of them have only three or two or seven updates, and they haven't yet gotten around to sending them to the global counter.

So you wanted the true value for how many updates have occurred.

You couldn't just look at the global counter.

If a thread crashes, thread got up to nine updates, and bang, that thread failed.

Doesn't necessarily bring down the program, but that thread isn't running anymore.

Well, then those nine updates are never going to be put into the global counter because you're going to lose track of them because the thread failed.

So what you can do is you can say, well, one of two things are the case.

Either I live with the fact that my global total is not correct.

If it turns out that you only care about the global counter, how many total updates you've seen when the entire process is over, and you're not worried about thread failures, well then you just wait until the entire process is over.

Make sure everybody flushes, every thread flushes their counter when they're finished to the global counter and then after everybody, all of the threads have said we've sent you your updates, the number of updates we've seen, then you update the global counter and you send that this is the final value.

Or alternately, you have the ability to say I'm going to synchronize now.

I'm going to send a message to all the threads saying you now need whatever your value is.

We don't care if it's three, we don't care if it's five, we don't care if it's zero.

At this moment, stop accepting updates, get hold of the lock on the global counter, put your updates into the global counter, then once everybody's taken care of that, all the threads have put their updates into the global counter, now it currently, at this moment, has the right value, then we output that value and we go back to normal operations.

Another thing we can do is to say, well, if you don't require truly exclusive access to this particular resource, maybe we can share the access.

Now, if we can share the access, if three threads all want shared access to the resource, well, that's okay.

Now, one obvious case where you can do this is if the three threads that want access to the resource are all just reading, none of them are writing, and give them all access to the resource because they're not changing anything.

What they read is not going to change.

None of the three threads are changing it.

If, of course, the fourth thread says I need to update this data structure, well then we have a problem.

We can't have a writer holding a lock simultaneously with multiple readers.

As many readers as we want can hold a read lock, but at most one writer can hold a lock on a resource.

Can't have two writers holding a lock on a resource.

Can't have a writer and a reader simultaneously holding the lock on a resource.

Now, this can turn out to be a valuable thing to do using these shared read locks because in many, many, many cases data is read far more often than it is written.

Experience with file systems, for example, has shown that reads are 50 times more likely than writes.

And if you're talking directories where you're looking at a directory full of file names, reads are 1,000 times more likely than writes.

So you can get big advantage by having shared read locks on resources that have these characteristics.

So this is a read/write lock.

So anytime somebody wants to access one of these particular kinds of resource just for read, they say, I'd like a read lock.

And if nobody has a lock, they get the read lock.

If other parties have read locks, they also get the read lock.

If anybody holds a write lock, they don't get the write lock.

For somebody who's doing a write, he tries to get the write lock.

If there's another write locker, somebody else holding a write lock on the resource, he can't get it, obviously.

But if there's also one or more readers holding read locks on the resource, he can't get that either.

Because if we gave him the write lock, when readers hold read locks, he could update the data structure and the read locker-- those holding the read locks could see inconsistent data.

So the problem you might have if you're doing this read-write locking is writers might get essentially shut out from obtaining locks.

If there are many, many people who are always obtaining read locks, whenever a writer wants to get his lock, he may not be able to get it because there are three parties holding read locks.

And if you're not careful about such things, a fourth guy comes in who wants a read lock.

Well, three guys have a read lock.

Give him the read lock as well.

Now there are four.

One of the original three drops out.

We're down to three.

But we still can't give the right lock to the right locker.

And maybe if more readers keep coming in and coming in and coming in, there may never be a moment when no readers hold that read lock, in which case there may never be a moment when the right lock guy actually gets his lock.

So you have to do something about that.

This is another example of a starvation problem.

Another option we have is to say, well, we got this resource.

And we have a lot of people who want to get exclusive access to the resource.

There are critical sections here.

But maybe what we can do is divide the resource up into pieces, separate pieces.

And maybe we can then say there's a separate lock for each piece.

And some people who want the resource will ask for piece 1.

Some people will ask for piece 2.

Some people will ask for piece 3.

If we set things up properly, the changes made to piece 1 will have no effect on pieces 2, 3, 4, and 5.

Therefore, we can have simultaneous writers on all five of the pieces, provided each piece is only locked by one party at a time.

Okay, now this is a matter of lock granularity.

How much gets locked when you get a lock?

If there is this big resource, when you get the lock, do you lock the entire big resource?

Do you lock a piece of the big resource?

If you lock the entire big resource, it's coarse-grained granularity.

You lock a lot.

You may not need a lot.

You may not need to have locked all of it, but you lock all of it.

What's good about this from a programmer point of view is you don't have to do a lot of thinking.

Need some of the resource?

Lock the whole damn thing.

Don't worry about what part you need, what part other people might be able to use.

Don't worry about it.

Just lock the whole damn thing.

If you make a mistake, well, you made a mistake where you locked the whole damn thing where perhaps you didn't need to lock all of it, but you did anyway.

And at least you're not going to get a synchronization problem.

But of course, you're going to get more contention because you're locking more than you need to lock.

Other people can't lock stuff that they could have locked.

There could have been parallelism.

Your locking strategy has prevented it.

The other option is to say, divide this resource up into small, tiny little pieces.

Have one separate lock for each tiny little piece.

This means, of course, that when I want to get, when I need to get exclusive access to one tiny little piece, I just need to lock the one tiny little piece.

All the other tiny little pieces have separate locks.

Other parties can get those without interfering with me.

Less contention.

And it also means that if I have to look for stuff, I don't have to worry as much about, you know, where I'm going to look because I've divided the resource into little tiny pieces and I only have to look in the little tiny piece I'm talking about.

It may be necessary in some cases to say, well, I do have little tiny locks for little tiny pieces but sometimes I need to lock the whole damn thing.

In which case I may need to have one big lock for the whole damn thing.

And if I have a lock for the whole damn thing then I'm not going to allow the other people to have locks for little tiny pieces.

Which of course is going to be limiting for them.

So there ain't no such thing as a free lunch.

That's always the case.

You know you get options.

Each option has its benefits.

Each option has its costs.

Here as well.

There are differences in what is the overhead because more locks is more space spent holding locks.

More locks for little tiny things probably means more lock operations get performed.

There's going to be more gets, more releases.

It's also the case that you may screw up on what you need.

So you thought you only needed this little tiny piece locked, but it turns out you need another little tiny piece locked.

Well, you know, either you don't lock it or you discover too late that you can't get it when you need it and you block because you can't get that lock, it can get complicated.

Now, here's an example.

Let's say we have a buffer pool.

We've talked about buffer pools in previous classes.

So we've got a whole pool of buffers, file system cache buffers, let's say.

So let's have a lock for each one of them.

So buffer A has its own lock, buffer B has its own lock, et cetera, et cetera, et cetera.

Anybody who wants to use one of these buffers has to first obtain the lock on the buffer they want to use.

But they don't have to obtain locks on other buffers not using.

That's the idea.

So most things you're going to do will lock one buffer.

So for example, my code is going to use some file system data that is in buffer B.

I want to make sure nobody changes the data in buffer B while I am using it.

I obtain the lock on buffer B.

I get to have exclusive access to buffer B as long as I hold the lock.

I don't do anything with buffers C, D, E, A, any of the other buffers.

Just with buffer B.

Somebody else wants to do the same thing with buffer D, they got the lock on buffer D.

I can hold my lock on buffer B, he can hold his lock on buffer D, we don't interfere with each other, we have less contention.

Right.

But it may be the case that some operations are going to require us to lock the entire pool.

So for example, let's say I want to add a buffer to the pool.

For whatever reason, I had a buffer, I'm releasing the buffer, and I want to release it to be put into the pool.

Not just lock, but it wasn't in the pool before, now it's going back to the pool, so we have more buffers in the pool.

Okay, well, somewhere along the line, I'm keeping track of what buffers I've got in the pool.

That's sort of a whole pool operation, not an individual buffer operation.

So, if two parties are both trying to put something into the pool at the same time, if I've got, let's say, a linked list here that is keeping track of my buffers, that could get a little tricky.

So what I do in that case is I lock the entire pool.

I have one lock for the entire pool.

All right.

The problem with that, of course, is if I use the lock for the entire pool, it might be a bottleneck because anytime that I lock the entire pool, nobody else can do anything with the pool.

So I want to minimize its use.

Perhaps if I only need some of those whole pool locks for read operations, I just need to scan through and see how many buffers I've got.

Maybe that would work out and that would help.

I could divide it into sub pools where I have five different file system cache buffer pools, different per pool lock on each.

Nothing happens between the sub pools.

There are things I can do.

They all have extra complexity.

OK, so these are things we can do to try to reduce the costs of locking, performance costs primarily, occasionally getting into situations where weird things happen, like priority inversion.

However, there is a problem remaining.

The locking that we're going to do will prevent improper concurrent operations if you use it carefully, if you use it to indeed lock critical sections.

And if you're careful, you can probably make it perform pretty well.

But that isn't enough.

Unfortunately, that will not always save you.

You have to be even more careful.

And this gets to be a very complicated level of care, Because if you do just what we've said so far, even with all of these additions for saying, here's how we're going to reduce the cost of locking, you can get into a situation where, because you're using locking, your system freezes forever.

It never, ever makes any progress, ever.

Deadlock.

Deadlock is bad.

We'll talk about why deadlock occurs, what's so bad about it, and what you do about it in the next class.