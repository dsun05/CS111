We had started talking about memory management for operating systems in our previous lecture. We discussed some very basic approaches to how we would manage memory, fixed partitions, dynamic partitions, and we discussed some of the drawbacks of those approaches, various forms of fragmentation, inability to perform simple relocation, inability to support large quantities of memory requirements, and so forth. Now we're going to start talking about what we could do to solve those problems. So with variable-sized partitions and with the use of segmentation registers, we were able to solve part of the problems that we had. The variable-sized partitions got rid of internal fragmentation. It did so at the cost of external fragmentation. But given that we can now using segmentation registers move partitions around, that will allow coalescing to be more effective. So it will mean that as we build up these little tiny useless areas of free memory, which is the effect of external fragmentation, we can compress those. And we can make them into larger, more useful areas of free memory. However, in the approach that we've been using up to this point, every one of those segments-- the code segment, the stack segment, the data segment-- for each of the processes that's running has to have a contiguous range of addresses. And while we can use segmentation registers to say we can move that contiguous range around, it still has to be contiguous wherever we move it to. That will lead to various kinds of problems. And it will mean that we can't possibly support more processes than we have RAM to meet their total needs. And that's gonna be difficult. We couldn't do what we do with modern computers, where we run hundreds of processes simultaneously, offering each of them large quantities of memory for their use. We couldn't do that if we hadn't solved this problem. So we're going to talk about a few things. We're going to talk about swapping, paging, and virtual memory. OK, now swapping was one of the early approaches that was used to solve this kind of problem, particularly the problem of saying we haven't got enough memory, not enough RAM. OK, well, we have a bunch of processes. a bunch of needs, we add up all of their needs, too much RAM, more than we physically have available. If you have a process that requires a vast amount of RAM, that's just what it's trying to do, then it's even worse because then you can't even support one process, which is really very depressing. So what could we do about that? Well, the obvious solution is RAM is not the only type the memory that we have around. We also have secondary storage. In the early days, we had hard disk drives. Nowadays, we more commonly rely on flash drives. But at any rate, we had secondary storage. And as I hope you remember from previous lectures, when we were saying, what is the state of a process, which means in turn, what is the state of the components of the process, which means in turn, what is the state of their code space? What is the state of their data space, what is the state of their stack space, how do we specify those? Well, they're a set of bits. We can copy bits. So, if we could take some of the pieces of a process, such as its data area, for example, and we could copy those bits off to the disk, the hard disk drive, the flash disk drive, then we could use the space that had been freed up by taking those pieces of memory out of RAM and putting them elsewhere, we could use that for other purposes. Now, of course, we couldn't run the process while its data area was sitting out on disk because we're going to need that data area. Even more, we'll need the code area and the stack area. But if we move that part of the process out to disk, we could make room for other processes. This suggests a way in which we could manage to support a set of processes whose total memory needs are greater than the amount of RAM available. Have some of the processes have some or perhaps all of their state, all of their stack, all of their data area, all of their code off on disk, not in RAM at all. Then the space they would have used can be used for the other processes. Of course, the one that's out on disk can't run. But maybe we can move it back onto disk sooner or later, replacing one of the other processes, and it can go to disk. Okay, so swapping to disk. This is an obvious strategy. This is a strategy that was created many, many years ago and was used initially to try to solve some of these problems. Generally speaking, the way it would work is if a process is running and it gets blocked, Maybe if our memory requirements are high, we could copy its memory, all of its areas, all of its segments, onto the disk. It's blocked anyway. It can't run. All right. Now, sooner or later, whatever caused it to block, will have happened, and now it need not be blocked anymore. We can't run it, though. We know it's out. We know the process still exists. We haven't killed the process. We just sort of put it to sleep by putting it out on disk. And now we could wake it up. To wake it up, we will have to bring back all of its segments from disk into RAM, because you can't run if you're not in RAM. Okay, so you copy it back. Now, that would be very, very tricky if you did not have the ability to relocate segments wherever you needed to relocate them. But fortunately, segment registers allow us to do that. So we don't have to put it back in the same stack and the heap and the code back in the same locations they were initially. We just have to find a suitably sized location, put in the suitably sized location, copying the bits back from the disk, change the segmentation register values for that segment for that process, and then it'll use the new location. Now this means that in principle a process could be given a memory space as big as all of the RAM available. Not quite, we have to have some room for the operating system, which typically means the operating system really has to be there. But all of the other processes, we could swap all of the other processes off to disk and then one process could use all the space that they were using. Much more space than it would have if it were sharing. Now there's some serious problems with this. Very, very serious problems. If we do this, then doing a context switch, changing from one process to another, can have an extremely high cost because we put a whole lot of data out there on the disk and we cannot run the process that has been swapped off the disk until we bring all that data back, which means we're going to have to schedule a lot of I/Os from disk and we can't do anything with this process until those I/Os have completed. Also, when we say, "Oh, we got to swap this guy out," we're going to have to pay those costs too. There's going to be a heavy load on the disk, we can't make use of the RAM until we have at least a full segment out there. And it's going to generally be a very, very heavyweight operation to swap somebody out and another heavyweight operation to swap them back in again. And we still haven't solved the problem of saying I need more RAM than is actually physically available at all, even if I get it all. Haven't solved that problem. So while that was a possible stopgap measure, and we'll talk later about a situation in which we still do potentially use swapping. It really didn't solve the problem in a way that made sense, that was going to allow us to perform the computations we needed in modern systems. Another approach was required. Another approach was created. That approach was called paging, which you may have heard of. So we'll talk about what is paging, the problem it specifically solve for us and how it achieves that solution. It's going to involve translation of addresses. We're going to go from virtual to physical as we did with segment registers, but in a different way. So we're going to talk about how we do that. We'll talk about the effects of using paging on fragmentation, and we'll talk then about how we use hardware to perform paging. So remember that that segmentation allowed us to relocate things, but only entire segments. You could relocate, for example, the stack segment to a different place. You move the whole stack segment to a new place. We use base registers to do that. And if we wanted to move something, we moved the bits and then updated the base register for the process in question. Didn't help external fragmentation really, because you still had to have contiguous segments. We need to get rid of the contiguity requirement. The paging approach is the following. Take all of the RAM that you have available, all of the actual physical RAM you have available, and divide it into units of one fixed size. Generally speaking, it'll be a fixed size of perhaps 4K or something on those lines. It could be 16K. It won't be a megabyte. It's going to be a much, much smaller size. OK, every single bit of RAM that you've got is divided into pages. A page size is probably gonna be a power of two because we always work in power of twos in computer science, particularly when we're talking about addressing and RAM. Now, having divided all of those pieces of RAM into these single fixed size, what do we call those? We call those things, the single fixed size chunk of RAM, that is called a page frame. So if you have four gigabytes of RAM in your computer, it will be divided into 4K page frames. All right. A page frame is capable of holding thus 4K of data. Now on the other side, of course, we're gonna talk about using virtual addresses again. That's how we're gonna do some kind of relocation here. And what we will do is say, let's divide the virtual address space into pages. Let's say each page is exactly as big as a page frame. 4K page frames, then you have 4K pages. 16k page frames, then you have 16k pages, etc. So, every segment of your program, you still have a code segment, you still have a data segment, you still have a stack segment, but each one of those segments will be divided into some number of pages. So, if your code segment, for example, was 128k and you have a page size of 4k, well that means you're going to have 32 pages. Okay, so What we're going to do now is say, well, let's try to arrange that any page in any virtual address space of any process can live in any page frame. Any one of these page frames that we've got that represents a fixed size piece of RAM may hold any page that any process that is currently running may need. And by currently running, I don't mean just the ones that are sitting on CPU core. I mean every process that's active that is still capable of running even if it's blocked. Okay so we can have the data of a particular page of a particular segment part of the virtual address space of some process and it could sit in any page frame at all and it doesn't have to sit in the same page frame all the time. We may have it in one page frame at one moment, sometime later it may be in a different page frame. Now remember, of course, what is the state of the page? It's a set of bits. So if we have the set of bits in one page frame and we copy the set of bits to the other page frame, that's fine as long as we make sure that the virtual to physical translation changes. That it no longer looks at the old page frame, now it looks at the new page frame. So we'll have to do that somehow. So how are we going to do that? Well, magic translation. There's going to be some magic way of translating the virtual addresses into the physical addresses. And in particular, we're going to have to translate virtual pages into physical page frames. Okay, so we'll have to do that. So this is, we'll talk about how we do that later, but here's basically what's going to happen. So we have a simple process. It's got a code segment, a data segment, a stack segment. Let's say its code segment is three pages, Its data segment is three pages. Its stack segment is three pages. We've got color coding here to show what they are. So it is possible that we would scatter the pages of this particular process, all three of its segments, into various different places in the physical memory, into different page frames. Note that they are not necessarily contiguous. They are not in this example. They are not even necessarily in order with the smallest page in the smallest numbered page frame. They're wherever it happens to fit. And we'll talk about how we decide where it fits later. But basically, now we're gonna say a processes memory is scattered through page frames all over the RAM. So let's talk a little bit about the effects we're gonna see of fragmentation if we do this. Now, what we are going to have still is segments. We still have a code segment. We still have a data segment. We still have a stack segment. Each one of those segments is going to be a set of pages. However big the segment is, we divide it into 4k chunks. That's how many pages are in that segment. So for example, here's one where we have 2, 4, 6, 8, 10, 11 pages. You note here that the blue indicates this is the actual data in this segment. And perhaps that's the length of the segment. Only the blue data is part of the segment. So as you can see, the last page here is not full. There's no data in there. It's not actually going to be used. In fact, because we have a length of the segment, it can't be used by this process because its segment length is only that long. It isn't longer. It doesn't go to the end of the page. However, the page frame that's going to hold the segment has been allocated to this process. We're not going to try to do anything fancy to say, somebody else gets to use the other part of that page frame. We're just going to say, "Meh, it's wasted." Which is fragmentation. This is internal fragmentation. As I hope you remember from the previous lecture, I told you that if we allocate pieces of memory in fixed sizes, you're going to get internal fragmentation. Pages and page frames are chunks of memory in fixed sizes. Therefore, There will be internal fragmentation. Now, when we were talking about the earlier approach of static segment size to memory management, we talked about how terrible the internal fragmentation could be there. How is it going to be here? Well, here's how it's going to be. You're going to have some number of pages in a particular segment, and then you are going to have a last page. Maybe sometimes the last page will be full, totally full, In which case, none of those pages experience any fragmentation. The early pages don't because they are full. The next page after the first page is full. The third page is full. The fourth page is full. The only page that might not be full is the last page. How much space is wasted in that last page? Well, certainly if there were no need for any data to sit in that last page, we wouldn't allocate the last page at all. So if the segment was not going to require more than the previous page, if it didn't require a little bit of extra memory that doesn't quite fit into a page fully, then we wouldn't have allocated that page. So the last page could have only a word of data in it. It could have exactly the right amount of data in it. It could fill up the entire page. Maybe it's half the page. On average, if you think that you are allocating segments of a more or less random length, it would be half a page. What's that mean? On average, under these assumptions, the half page that is left over is unusable. The process won't use it because it's not part of the segment. Other processes can't use it because it's not part of one of their pages. Therefore, it is wasted. Wasted on internal fragmentation. How much? Half a page. How big is a page? 4k perhaps. So on average perhaps 2k. All right, so overall, how much memory have we wasted in internal fragmentation if this is the approach we're going to use? Not a lot. We might have three or perhaps four segments in it per process. Maybe. Three would be common. So we have three segments. That means that on average, using the same assumptions, we're going to waste one and a half pages per process. 4k pages waste 6k. Now nobody likes to waste memory, but wasting 6k per process is not a big deal. 200 processes, yes it does add up, you know, it's 120k, but that's not all that big a deal. you know, you got 4GIG. How much internal fragmentation? 120k. Well, so what? Who cares? What about external fragmentation? None. There is no external fragmentation. External fragmentation can only occur when you are doing variable size allocations. Now, the segments are a variable size, but segments are made of pages. Every page in a segment is fully used except the last page, therefore we are always allocating a fixed number of pages. There's the internal fragmentation of that last page, but there's no external fragmentation. There are no unusable chunks of memory lying around because they're too small to meet any need. Every page frame is usable to hold any page, therefore every page frame can be used as part of somebody's segment. It's not a useless piece of memory sitting around too small for any need. It's exactly the right size for everybody's need, in fact. We never carve up pages. Since we don't carve up pages, we don't run into any external fragmentation. This is an immense reduction in fragmentation costs over any of the approaches that we've discussed so far. However, of course, there is a cost. We got to do something that we didn't have to do with any of those previous approaches. that magic translation mechanism that we were using for segmentation, where we said, okay, there's going to be three registers, well, for, you know, six registers, because you have to length as well, but six registers for every core on, they're going to hold the start and length of each segment, the process that's running on that core. That's not that much space. Here, how much are we going to need? Well, we are going to need to translate on a per page basis, page address into page frame address. And the translation is gonna be different for every page, and it's gonna change over the course of time. So we are going to need to have a translation mechanism capable of doing far more translations than those segment registers were capable of doing. And moreover, this will happen on every single memory reference. Every time we say, let's run an instruction, every time we say, let's fetch a word out of RAM, every time we say, let's store a word in RAM, we're gonna issue an address, and we are going to have to do the translation. We could have instructions that do two or three addresses per instruction, which is quite a lot. That would mean we'd have to do two or three translations within one instruction cycle. And we can't afford to have instructions be slow. They must be fast. So this implies, among other things, that the translation mechanism has to be hardware-based. The ordinary translation of a virtual address to a physical address of a page to a page frame has to be done in hardware. So we're gonna have to have some hardware, and we do. Hardware was created specifically for this purpose. This hardware is called the memory management unit. Modern computers, at least the modern general purpose computers, all have memory management units. You know, your laptop, your smartphone, everything that you're using, every server you use, they're all going to have memory management units, and that's what these do. And their purpose is exactly to do this, to translate page addresses into page frames addresses very, very, very, very quickly. So here's basically how it's going to work. Here's how the MMU is going to do that magic. So we have virtual addresses. These are addresses that are actually issued by the code of a running process. We have physical addresses. These are places in RAM that store the data associated with a virtual address. The virtual address is usually not the same as the physical address. If it is, it's a pure coincidence. So we're going to have to translate the virtual address to the physical address. the MMU is going to help us do that. So let's take a look at what we have in a virtual address. We will regard the high order bits of every virtual address as being a page number. And the low order bits are going to be an offset within that page. So if we have a 64 bit address and 4K pages, each page is going to be 12 bits. There are going to be 12 bits worth of addresses in there. So the offset within a page is going to be required. 12 bits to say where within this particular page are you talking about. The remainder of the address is going to be a page number. If you have a 64-bit address, that means you're going to have 56 bits devoted to what is your page number. If you think about this for a little bit, you will see some problems coming up. But we won't worry about that for the moment. So what are we going to do? Whenever an address gets issued, the hardware-- and this must be done in the hardware. We can't do this with instructions. We can't have code that does this. It's got to be done in the hardware for speed. The hardware is going to grab that address that we are trying to use, for example, to fetch an instruction out of RAM. And it's going to divide it into two parts, the page number and the offset number. It'll take the page number as a index into a table that maintained by the MMU. That table is going to say, okay, for this particular virtual page, here is the page frame containing it. The page frame, of course, is going to again have some set of addresses associated with it. Every page frame is the same size as the page. So every page frame is going to be 4K. So we look up in the table. Now, don't worry about what this green V thing is for the moment. We'll talk about that in just a second. But you look it page, that's the page number. Great. So the V is a valid bit, basically saying, "Is there such a page? Is that a page that makes sense? A page for which I do have a translation?" And there are a number of reasons you might not have a translation, which we'll talk about. But normally, if things are going well, yes, you have a translation. The MMU knows what page frame matches that page number. So that's going to tell you something about the physical location of the particular page holding the word of memory you're trying to get to. But of course that's only part of the address. The other part is the offset. Well, the offset you can just copy over because it doesn't matter which page frame it's in. It's always going to be, for example, the 75th word in the page frame in question. Whether it's page frame 12, page frame 50,000, page frame 232, whatever it is, It's still a 75th word in that page frame to get that offset. So what the MMU is going to do is divide up the virtual address into two parts, page number, offset, copy the offset over into a place where it's going to build up the physical address it needs to fetch, check to see for this virtual page number what is the physical address, copy the physical address in there, and then bang, it's got an address. It knows where in RAM, the particular word in RAM that it needs to access in order to, in our example, fetch the instruction that is to be performed. So, here are a few more examples. Here's a very simple page table maintained by the MMU. As you can see, we have a number of address translations there. So, let's say that our virtual address was 00041C08. What do we do? We look up 0004 in the MMU. The MMU does this in hardware. And it says, okay, what have I got? 041F. Great. Oh, and I just copy over the offset. Another example, 0000100. That's the address we want to translate, the virtual address. We look up the zeroth entry in the page table, and it says, "Okay, that's page frame 0C20." So we copy 0C20 there, and we just move over the offset, and now we go out to that location in RAM and fetch that content. If we have 0005, and we say, "Okay, fine, it's the fifth entry in the page table." Ah, here now we run into an issue. When we go to the fifth entry in this particular page table, There's not a valid bit and there's no address. So, why might that happen? And if it happens, what are we going to do? We'll talk about that. For the moment, let's talk about the actual hardware we're going to use here. Now, this is hardware, so this is not exactly the goal of this class. That's not really what we're talking about, though, of course, the operating system works closely with hardware, so there's usually a relationship, as there is here. But what is Well, when first created, when the idea was first built in the late 1960s, the MMU was a separate chip. It sat between the CPU and the bus. So the CPU would issue virtual addresses, send it off, and before it got to the bus, the MMU chip would pick it up and would do the translation, then would put on the actual physical address on the bus after the translation was made, bus would bring it back. Now over the course of time, it was determined that, you know, we got to do this really fast and things are slowing down because we're going off chip. One aspect of hardware is that doing things on a chip is always going to be cheaper than moving something off of the chip to another chip and doing it at that other chip. So for things that have to be very, very, very fast, we try to do them on the chip. As a result, MMUs nowadays are typically integrated into the CPU chip. OK, now what about those page tables? The MMU is supposed to maintain a page table. Where are we going to keep that? That's a bunch of data. Now, originally, this was just set up as a bunch of fast registers in whatever was doing the MMU. If the MMU was its very own chip, you had a lot of space on that very own chip. You could have a whole bunch of fast registers. However, now that we've moved the MMU into the CPU chip, we can't afford that much space. And in particular, we can't afford that much space because we've gone from 16-bit to 32-bit to 64-bit computers. And you remember, with a 64-bit computer, how many page frames have you got? 2 to the 56th? Big number. So, you probably don't have that much memory, actually, in your computer. But, you're going to have to do something about translating those addresses. So, we can't possibly have a 2 to the 56th entry page table sitting in your CPU just for the MMU's purpose. So if you had 4K pages, let's say, and you have a 64 gigabit memory, how many pages are there? 2 to the 24th. Even if you say, fine, I'm only going to worry about how many pages I've actually filled. I'll do something else about the other pages. I just have to have enough entries for all the pages I've actually filled. That'd be 16 megabytes of pages. You'd need to have 16 megabytes of translation entries, virtual to physical. You can't afford 16 megabytes of fast registers. There's just not space for them on the chip. If there were, you'd be burning up so much of the space of the chip that the chip wouldn't be able to have as many cores, for example. So that's not what you're gonna do. Can't use registers. What do we use? Well, the paid, and in particular, one thing you should be asking yourself is, oh yeah, so we're gonna do virtual to physical addresses. And you told me everybody had their own virtual address space. You told me that when we were talking about processes. Yeah, they do. So there isn't one virtual address space on a typical computer. 200 processes, 200 virtual address spaces. Gee, that's gonna be a whole lot of registers if you try to store all of that data in the MMU. So we don't. We store the page tables in RAM. They're kept in RAM somewhere. Now, if we just kept them in RAM and we said the MMU has to go out to RAM to get the page table entries so it can do the translation, That'd be really slow. That would mean in order to translate one address, every time we issued an instruction, we would translate at least one address, maybe two or three. But every time we did that, every time we needed to translate an address, if we only kept the page tables off in RAM, we would have to do two memory accesses for every memory access we actually wanted to do. One, to get the translation of the page table entry, two, to get whatever we actually wanted to get. We can't afford that. Memory is too slow. Memory is so much slower than the CPU that we really can't afford to do that. So that's not gonna work. We have to store them in RAM because we don't have room anywhere else, but we can't afford to do two bus cycles for every memory access. So what do we do? Caching one of our favorite solutions to these kinds of problems in computer science. We're going to have the MMU have a cache. The MMU will have a cache and what's going to be in the cache is recently used entries from people's page tables. So if you are going to, say, try to translate the address of one instruction in a piece of code, then the next instruction in the piece of code, the next instruction in the piece of code, as a rule, several of those instructions will be on the same page of virtual memory, which means on the same page frame of physical memory. So the first time we may need to grab hold of the entry out of RAM and put it in the cache and say, okay, now we have this entry cache. But then as we keep going through other instructions on that page, we've got the cached entry. We don't have to go back to the RAM to get the next entry. So we'll set up a set of fast MMU registers that we will use to hold a cache. And that's great. Now, if life is good, we are going to have wonderful speed because we're working with fast registers on the chip. However, we now have to worry about, okay, the cache is this big. How are we gonna maintain hit ratios? How are we gonna invalidate entries in the cache, for example, when a piece of memory is no longer in use? And what are we gonna do about the fact that we've got 300 address spaces and we don't have nearly that much space in the cache? And everyone has virtual addresses that are more or less the same. Everyone, for example, may have a virtual address 1 million, But for all 300 processes, that's a different location with different content. So what's the 1 million lookup in the MMU's cache going to point to? Which of the 300 processes is it going to have the right translation for? And how do we make sure that if we switch to another process, well, we end up with the right translation for the other process. These are issues that we're going to have to deal with because we chose to solve our memory problem in this fashion. The acronym TANSTOFFEL is one you may or may not have heard. It stands for there ain't no such thing as a free lunch. Every solution has its own costs. If you choose a solution, you're going to have to pay the costs associated with that solution. It's not like you choose the solution and then there is nothing more to worry about. Here, for example, we're going to have to worry about all these issues with sharing caches that aren't nearly big enough and making sure we have a high enough hit ratio so that we get good performance. Now, in particular, we've got hundreds of processes running. Every process has its own address space. Every process needs its own set of pages, not necessarily shared with any other process. Now, we can put any page anywhere we want to put. That's what the magic of this approach allows. But what happens if we need more RAM, more pages, when we consider all of the processes we're running, then we have page frames. What are we going to do about that? Well, clearly we cannot keep all the pages and page frames because we haven't got enough page frames to hold all the pages, which implies if we don't want to throw away pages and throwing away pages would be bad because then we wouldn't be able to use them again correctly. If we don't want to throw away pages, which we don't, we're going to have to store them somewhere other than RAM. Okay, where do we have other than RAM that could store pages, could store memory, could store bits? Well, we have the flash drive or the hard disk drive. Clearly, some of our pages are going to be out there. So as time goes on, processes run and use different parts of their code, different parts of their data area, different parts of their stack. And as things go along, a process finishes its time slice. And if we're doing some kind of, let's say, round-robin scheduling, then another process is going to run. It requires different pages, different page frame content. As time goes by, we're going to have to have different things sitting in the same page frames. So we're going to have to deal with that. Now, we keep the extra pages, of course, on a flash drive. Great. Now, on a normal system, the one you're using today, the one you're probably using to look at this lecture, you have a bunch of the pages that are being used to store the data for your processes sitting out there on the flash drive. They are just there. That's where they are. And they can't be used when they're on the flash drive. So what are we going to do about that? Sometimes a piece of code that is running on a core is going to issue an address. And that address is a virtual address in the space of that particular process. And we're going to say, let's do the translation. And we're going to say, oh, we don't have a page frame that particular page. What are we going to do then? Well, there are other issues that we're going to have to worry about that are related. So for example, let's say that you increase the size of your stack. What are you going to do then? Well, you're going to have to add pages, bigger pages. And when you add the pages, then you are going to have to update the page tables. And if you get rid of pages, because you say, "Oh, I don't need as big a data area anymore." Well, then any translations related to the pages that you have gotten rid of, you don't want those around anymore because they're no longer valid. So, you're going to have to get rid of those. There's got to be a way to get stuff out of that MMU cache when it shouldn't be there anymore. When you switch from one process to another, the virtual address space of one process is not the same as the virtual address space of of another process, same numerical virtual addresses, but they aren't supposed to point to the same data. Therefore, if we have a bunch of entries in the MMU for process A, because process A was running on the core, and then we switch to process B running on that core, we must make sure that the addresses issued by process B as it runs its code, as it looks at its stack, as it gets stuff out of its data area, we must make sure that those are page frames that belong to process B, not page frames that belong to process A. So we're going to have to do something about that. One thing we could do, of course, is when we do our context switch, we could say, let's flush the entire entry of the MMU cache because we've done a context switch. MMU cache contain entries for process A. Switching to process B. Now we don't want to use any of the translations that A had. We better do something to get rid of them. So that's one thing you can do. That has its costs, and we'll talk about some of that in a few minutes. And there are other approaches, but that's one thing you could do that mechanically would work. Sharing pages between multiple processes. This isn't a big deal. It's easy to do. You just say, OK, for example, if there is a shared library that is sitting somewhere in memory, all we have to do is say for every process, they can have whatever virtual address they want for that shared library. Just for every one of their page tables, we are going to have the translation to the same page frame. They're all going to point to the fifth piece of code that's in this library, the fifth page of code in the library, for example. And we can do read/write sharing if we need to, which we'll talk about when we talk about IPC. OK, now, it's clear if we're using this kind of approach, this paging approach, that some of our pages are in RAM and some of them are out on disk at any given moment. Can't use the ones that are on disk at the moment. Only once they are in RAM can they be used. So we have to choose out of all the pages that all of our processes want to have available, which ones do we send out to disk? And if it turns out we need to get one of the ones that's out there on disk, how do we bring it back? And there are various solutions that were developed for this, the one we use most commonly is called demand paging. This is how your computer works. What is demand paging? Basically, we say, we're not going to worry about what pages should be in RAM at the moment for any given process. The process will tell us what pages it needs to have in RAM. If we already have the page in RAM, it's already sitting in a page frame, then the MMU will say, here it is, and we'll just go get it. What if we don't? What if we get one of those entries that we saw on the example slide several slides back that says, oh, that page isn't around? Now, if that's a valid page, it's page that should be there, and if it isn't around in a page frame, it's on disk, what are we going to do? Well, go get it. Bring it into memory. The page frame is sitting bit for bit-- the page is sitting bit for bit exactly how it should be sitting somewhere on the disk. If we simply copy its bits from the disk into a spare page frame that's sitting available, and we update the page table of the process that issued that request to say, oh, that page you wanted, now it's in this page frame, used to be on disk, now it's in this page frame, then everything will be fine. On-demand processes will request particular pages. With luck, most of the time that page is in memory in a page frame. When we aren't lucky, we will go fetch it and put it into a page frame, and then we'll be able to use it. Now, another thing we could do here is we could say, OK, if we're doing this, maybe the right thing to do, if we assume, for example, that we're something like round-robin scheduling where processes take their turn to run. Maybe it would be nice if when a processes turn came up and it started to run, some of what it needed was still sitting around somewhere in RAM and page frames, because if it isn't, we're going to have to go out to the disk and get it, and that's going to be slow, because remember, these disk operations are quite slow. So we move pages onto and off of disk on demand, trying to maintain in memory a set of pages in page frames that are likely to be used at some point in the near future. So, mechanically speaking, the MMU has to be aware of this possibility. So it's going to have something that will indicate that a particular page that it is trying to do a translation for is not in memory. Now there's another case, of course, which is it's an illegal address, not part of any segment this process belongs to. So the MMU has to know that as well. But just because there is no page frame containing the page, that doesn't mean it's an illegal address. Often it simply means the page is on disk, not on a page frame. So we're going to have to have the ability for the MMU to recognize this. When it recognizes this, something must happen. Now, in principle, one could build into the hardware of the MMU that something, everything that you do to get the page. However, that is not what's done. That's actually a lot of work. There are many, many, many things you're going to have to do in order to bring a page in out of the disk and into a page frame. It's too complex to build into the hardware. So what do we do? We put that into code in the operating system. And what the MMU then is going to do is say, well, if I have one of these occurrences. Somebody issued a virtual address, valid virtual address, but not on the page frame. I'm going to tell the operating system, deal with it, handle it. I don't want to deal with it. You handle it. Tell me when it's done. So the OS will run some code to bring in the requested page, put in a page frame, and then it will tell the MMU, hey, try this address translation again. This time it should work because we just put the required page into a page frame. Now this means among other things that we don't have to have the entire process, all of its code, sitting in memory at the moment we start writing the process. We just have to have some of it. So we could have a very, very, very large process and we're going to have to, in terms of the amount of code in it, we're going to have to have a very, very, very large code segment if we have a very, very large piece of code to run, But we don't have to have every single page of that very, very large code segment actually in memory when we start running the process. We need at least the first few pages it's going to use. But from that point onward, maybe if it's a complex program like, let's say, Microsoft Word, which can do many different things, but on most runs of Word doesn't do most of them, we don't even have to bring in any of the pages for things Word isn't ever going to actually do in this particular run. And if it turns out we were wrong, and yes, we are going to do a 3D manipulation of a graphical object in this particular Word document, well, fine, we'll demand page in the pieces of code that do that, even though we don't normally have them in memory. But we can start up the process not knowing what of the many different options it's going to take by simply bringing in a few pages that we know are pages that are going to be used early in the process, regardless of what it takes. where its main routine starts, for example. Then from that point onward, we bring in pages as necessary. Now, from a mechanical point of view, is this mechanically correct? If you do things right, you know, if you build your hardware and you build your software in the proper way, mechanically this is not that complicated a thing to get right. You can do it, no question about it. This is possible. But there's a big challenge, and that challenge is performance, because going to disk is incredibly slow, incredibly slow compared to going to RAM, and going to RAM is incredibly slow in terms of doing something on the CPU chip itself. So we're taking many, many, many steps of overhead in terms of slowness if we have to do an operation to bring in a page from disk. So how are we going to do this and still receive good performance? Generally speaking, If most of our memory references don't require us to go to disk, we'll get great performance. If many of our memory references do require us to get something off of disk, we'll get terrible performance. Therefore, let's make sure, or let's do the best we can, to ensure that most of our memory references are already in page frames at the moment we make them. Okay, well, obviously what you'd like to do is say, okay, I want to make sure that after I perform the following memory reference, the next memory reference I perform, that one's going to be in memory. And then the next memory performance reference I make, that translation will also point to something in memory. And the next one, and the next one, and the next one, almost all the time. Every so often, if it turns out, page frame I need isn't in memory, okay, we can take a performance hit, but we must not take it very often. So almost always, the next address that we want must be in memory. How are we going to achieve that? Well, it turns out it isn't actually all that hard because of a property called locality of reference. Locality of reference is one of these things that makes it possible for us to perform modern computing operations at reasonable speeds. So we need to predict, you might say, how am I going to predict what the next reference is and what the next reference is and what the next reference is? I mean, the code could do anything. It could jump all over the place within the code. It could ask for a piece of data from this part of the data area and then from that part of the data area that's very, very far away. How are we going to make sure that doesn't happen? Well, we assume that it won't happen and why do we assume that? Because we assume that programs actually written by people behave in certain ways that achieve locality of reference. And all locality of reference means is if this time I ask for word 7000, pretty good chance that next time I'm going to ask for a word pretty close to 7000, maybe word 7001 or 7004 or whatever may be. High probability that'll happen. If I ask for word 7,000, low probability that the next word I ask for is going to be 12 million. That's simply the assumption we make. And is that a good assumption? Do we actually have good reason to believe that's what's going to happen? And the answer, fortunately, is yes, we do. Not because people have put a lot of effort into making sure it happens, but just because this is the way people write programs. So what happens in a program? What memory references do you get when you're running a program? Well, you have to get instructions, and you have to get data from the stack, and you have to get data from the data area. That's the memory references you're going to make. About instructions. Well, what happens in a typical program? Well, you run instruction one, then probably you run instruction two, then probably you run instruction three, then probably you run instruction four, and so on and so forth. Sooner or later, you're going to hit a loop. So you've run instruction 50, and you run instructions 51, 52, 53, 54, 55, and you go back to instruction 50. And then you do 51, 52, 53, 54, 55 again. And you may do that a whole lot of times, depending on how long the loop runs. So either you are going to do instruction one, then instruction two, then instruction three, and you're going to do them in order, or you're going to usually loop through a set of instructions. If we're careful about our caching, then every time that we loop through a set of instructions, we've already seen those instructions, we kept that translation in the MMU's cache, and things will go fast. Not only will it not require us to go to disk, but we won't even have to go to the RAM to look up the page table because, hey, we've cached the translation. Now, of course, every so often, You're going to say, I'm going to invoke a new routine. And the new routine probably has a piece of code that's far away in the actual executable code. Well, OK, fine. Maybe you're going to end up taking-- that's going to require you to go to disk to get a page in. But that's going to be relatively infrequent. Now, of course, that's just the code. How about the stack? Well, normally, when we're working in a stack-based system, we are working with a particular stack frame. that's the routine we're currently running. That stack frame will be a relatively small number of pages of data. It depends, you know, you could have one that's pretty big, but it's not gonna be usually that big. So you're going to be working in terms of what you do with a stack with that stack frame. And that's going to be a few pages of data, which probably you've accessed fairly recently, because at least you set up the stack frame when you call the routine. So you reference everything at that point. Therefore, probably your stack references are going to be in memory. Now, when you exit a routine, you're going to go to the previous routine, the one that called it. But hey, with a little bit of luck, maybe the references for that are still in memory as well, or at the very least, you haven't moved the pages out. OK, so stack will probably get pretty good locality of reference. How about the data area, the heap? Well, here it's a little bit more problematic. But the heap is essentially a data area that is organized in whatever way the program or the particular program you're running it organized. He's set up data structures of various kinds in this area. What has he set up? Well, he might have set up, for example, a table. A table is ordinarily going to be implemented as a contiguous range of addresses. Now, you can very easily see that if I use a piece of code that looks at table entry zero, there's going to be a page or a piece of a page or maybe if it's a big entry, a couple of pages that represent the content of item zero in the table. And what And what do I do when I'm done working with item 0? Probably go to item 1. And when I'm done working with item 1, probably go to item 2. If they're relatively small data structures, I may have multiple entries in the table all sitting on the same page. In which case, I'll get good locality of reference. Now this isn't guaranteed. If I set up a 50 million entry table and I randomly start poking at one location after another location after another location every time, a different place in that 50 million, then I'm probably not going to get good locality preference. What will happen? Well, my program won't fail. I won't have any errors, but it'll be slow because I will have to keep going and getting pages off of the disk and bringing into page frames. So at any rate, there are no guarantees. Some programs, particularly programs that have been written specifically to cause problems may not have locality of reference. They may cause a lot of problems where you have to get something off of a disk. But it's not what we normally do. That's not the way people normally write programs. Okay. Now, what happens when we do run into one of the rare, we hope, occasions where the translation we want to perform is represented by a page that's not on a page frame of RAM but is sitting off on the disk. That's called a page fault. No doubt a term you have heard referred to in computer science. Well now here you know what it is. That's a page fault. A page fault is not a mistake. It is not an error that has to be fixed. It is a performance related event. What's happening here is you've got a page table for for your process representing all the pages of memory that you are interested in. Some of those pages are stored in page frames in the RAM. Some of them are stored somewhere on disk. When you reference one that is not in a page frame of memory but is on the disk, you have generated a page fault. So what's going to happen then? Well, the MMU is going to signal to the operating system. This guy needs this page. It's not on disk. It's not in RAM. Let's go get it. So that is going to essentially be an invisible, except for performance, event. Your process is not going to know that a page fault was generated. It's going to simply stop in its tracks because it can't do anything until that memory reference is resolved. It can't resolve the memory reference until it can do the translation. The MMU can't do the translation until the data in question is sitting in a page frame. Data is in a page frame, we gotta get it into a page frame. We can't run this process again until the data it requested is sitting in a page frame. So we'll block the process, very simply. And then the operating system is going to get an exception. This is another of those forms of exceptions. We talked about traps several lectures back and other kinds of exceptions. Here's one of them. This is an exception that is not a fatal error. It is not a mistake made by anybody. It is simply a performance-related event that is going to have to be dealt with. So here's an example of what happens when there's a page fault. So we had that address, 0005 3E28. And as you remember, when we looked up 0005, we said, oh, it's-- we don't have a translation for that. No address. And that's going to happen because there's a page fault. In this case, the MMU would signal to the operating system page fault, and it would tell it the information about which particular page for which particular address space had a fault. The operating system is then going to have to do something. What's it going to do? Well, here's how you handle a page fault. This is an operating system activity. This is not built into people's code, individual applications. It's not primarily in hardware. This is code in the operating system. So we initialize all page table entries for a process to say, these pages aren't present. Perhaps we bring in a few at the very beginning, but all the others are indicated as not present. They're on disk. So the CPU faults if one of these not present pages is referenced. The MMU detects that, oh, you're trying to do a translation. No page frame for that page. That fault is an exception, which means we enter the kernel. We stop running user code on whatever core was performing the code that caused the page fault, and we go to the operating system. The operating system says, oh, this kind of exception is a page fault. I will gather the information what particular page this particular process from its address space was trying to access. In its page table, there will be some information that says here on the disk is where you can find that page. Then I will call, I, the operating system, will call up a page fault handler, piece of code in the operating system. This is going to say, okay, fine, this is the page we want. This is where it's stored on the disk. Fine, I'm gonna have to go to the disk and say, get me that page. You've got it here. Here's where I would like to get that page. We know exactly how big it'll be. It's going to be the page size, 4k, for example. You ask the disk, read the following 4k for me. All right. So that's an I/O. Now, of course, I/Os take a certain amount of time to perform. So we're going to block the process that had the page fault. And also, we're not going to continue sitting in a tight loop in the operating system waiting for that page fault to be resolved. That'd take far too long. So we're going to do something else. We might schedule a different process. We might run a different piece of operating system code to do some other work that is required, whatever. But at any rate, we're going to be waiting one way or another for the page fault to be handled by the disk. The disk will eventually go out. It'll find the block of data that is stored that represents the page in question. It will return it, and it'll get put somewhere. It'll get put into a page frame. Now the page frame it'll get put into has to have been specified by the operating system at the time it's scheduled the IO with the disk. So once the data has been successfully read by the disk, the disk will generate an interrupt. The operating system will say, okay, disk interrupt. It's a disk interrupt for this particular request I made. That's a request related to a page fault. It should be in the following page frame because I said that's where to put the page data. Now I can go to the process that performed the page fault, update its page table to say, that is no longer a not present page. That is a page that is present in the following page frame. Okay, so the page table entry is updated. Now, at this point, we can start up that process again. It hasn't been running for quite a while. It was blocked. Now it's unblocked. Sooner or later, based on scheduling and other dynamics, that process will get to run. When it gets to run, what are we going to do? Well, it was trying to run a particular instruction that caused the page fault. It hasn't finished that instruction. In fact, it hasn't really been able to do anything useful on that instruction because of the page fault. Therefore, we're going to try to run that instruction again. This time, we hope we will not get a page fault. We probably won't, because we just dealt with bringing the page frame in. And we will be able to successfully fetch the data that was requested from that address. We'll be able to go to the RAM, go to the page frame containing that particular page, get that data, and do the instruction for the process. Now, while all of this other stuff is happening, while the process is blocked waiting for the I/O to complete, we could just run other processes. And that's probably what we will do. It's important to understand that page faults do not cause errors. They don't impact the correctness of a process. A process can have zero page faults. the process can have a million page faults, and it's gonna have the same result. The same things are going to happen in that process. The same output, the same computations. Everything will be exactly as it was in both of those cases, except in one case it's gonna be fast, and the other case it's gonna be slow. It just affects speed. And once, of course, you have met a page fault, once you've dealt with a page fault and brought the page in question into the page frame, More requests to access that page are probably not going to generate more page faults because you've cached the entry. That page is sitting in a page frame. It's probably going to sit in the page frame for some while and moreover, you have a page table entry that says this page is in this page frame. So the MMU will, the next time that you ask for some address on that page, will already have the translation available and will not need to do a page fault. Programs should never ever crash because of page faults. They are not fatal events. They are not errors. However, lots of page faults, slow program. So, of course, page faults are going to block processes and processes can't run when they have been blocked, which means they are going to run slower because they've been blocked for a while. Further, there's going to be overhead associated with handling the page fault. You're going to run a bunch of code in the operating system. You know, you're going to do a bunch of things where you say, "I'm scheduling an I/O. I'm choosing a page frame to put this into. I am blocking the process. I'm then going to do something else. Sooner or later, an interrupt comes. I have to handle the interrupt. Once I've figured out what the interrupt is about and it's about this particular thing, I'm going to update a page table. Then I'm going to indicate to the MMU that it should do this again, but I may have to wait until the process has gotten its turn to run, but I will unblock the process and put it back in the scheduler queue in the appropriate place, and so on and so forth. So there will be heavy overhead associated with every page fault. The more page faults you have, the more overhead you have. So it's not just that the process has to wait for the disk in order to have its page faults handled. It's that overall, the computer on which the page fault occurs is paying a processing cost, slowing down everything that happens on that computer. More page faults, slower performance, not just for the process that is doing the page faulting, but for everything running on that computer. This means that there's a very high value in making sure you don't get a lot of page faults. If you have the right pages in memory, if they're pretty much, you always have the pages in and memory that you need to have in memory to manage the upcoming set of memory references that processes make, then you will run fast. If you don't have that data in page frames, if most of the time you get page faults, then you're going to have a lot of paging and it's going to be very slow overall on your computer. Now, the operating system has no control over what addresses processes requests. They ask for what they need, and who knows what they need? They know what they need. Nobody else knows what they need. You don't have in the operating system any predictive capability of saying, "This process is going to ask for this address next," except to the extent that locality of reference applies, which is a question of how the process was written, not what the operating system is doing. do not try to predict in the operating system the following pages are likely to be accessed in the future except to the extent of assuming locality of reference. So we don't have any control over which pages a process asks for, but there's something that the operating system does have control over that makes a difference. At any given moment, if we don't have the right page in memory, we're going to bring the right page into memory and probably that means we're going to to take some other page that we have in memory that's sitting in a particular page frame, we're going to kick it out of that page frame, meaning probably we're going to send it to the disk, and we're going to put the page that was just requested into its place. Now, if the page that we kicked out of memory, because we needed to make room for the one that caused the fault, is a page that doesn't get accessed in the near future, that's going to be relatively good. If, on the other hand, it gets accessed in the very next instruction, That's going to be relatively bad because that'll cause another page fault. In handling the first page fault, we cause the second page fault. We don't want that to happen. This means that we have to be clever about which pages we eject. When the time comes to remove a page from a page frame, because we need an empty page frame to put another page into, we have to be clever about choosing which of the very, very many page frames we are going to use and which page as a result is going to be ejected. OK, now, when we put all of these things together, including the demand paging and figuring out how we are going to eject pages, we have a generalization of how memory is going to be managed in modern computers that is called virtual memory. Mac OS, Windows, Linux all use virtual memory. Pretty much any computer you're going work with except possibly some very, very primitive embedded machines. They're using virtual memory and they're doing the kind of stuff we've just been talking about. Okay. So we are going to, within the paradigm of virtual memory, be able to do certain things that are valuable from the point of view of providing assistance to programmers and to users. First, we are going to say, okay, hey programmer, you can have a very, very, very large amount of memory. And we don't care if there are other processes running or not. You still can write a program that uses a very, very, very large amount of memory. In fact, if you want, if you need, you can write a program that uses more memory than I've got on my computer. We can make it work. How do we make it work? Well, we have some pages in memory in page frames. Some of your pages are on disk. They'll be page faults, what is necessary. We can do that for every single process. And we don't care how many processes there are. we can give every single process the same promise, as much memory as you need. And how are you going to get your memory? Well, you don't have to do anything fancy where you figure out, oh, this is on disk, or this isn't on disk, or, you know, I have to predict what's going to be required in the near future and tell the operating system. We don't require you to do any of that. Just address whatever you need to address. We'll make it work. And we're going to try, if we do it right, and if things work out properly, to make it look like you're running at RAM speeds, which is good. RAM is relatively fast. So, this is the state of the art in modern memory abstractions used in operating systems. This is what they like to do. So, we give every process an address space of immense size, perhaps as big as the word size allows to a 64-bit word. Why don't you have an address space of 2 to the 64th? Now we may not give them quite that much, like in 2 to 63rd and save half of the address space for the operating systems use, but you know that's still a lot of space. That's much, much more space than almost any program could possibly require, which means, among other things, that when you're writing your programs you don't need to worry about that. You don't need to worry about, "Gee, will I have enough address space?" Yeah, you will. Now, how are you going to actually use this immense address space? Well, we're not going to provide you with actual pages in that address space. We're going to have every process effectively request particular segments. They're going to have a code segment. They're going to have a data segment. They're going to have a stack segment. Now, the chances are you've never written a program in which you say, I request a segment of this size. The code segment, you don't have to request at all. How big does this code segment have to be? big is the code after it's been compiled and linked together. That's how big it has to be. And you don't need to know that until you're trying to run the program. So, until you have the load module, it doesn't matter really how big it is. Once you have the load module, the operating system can say, oh, this is how big the code segment is, and it can set up a segment of that size. For the stack segment, we don't really know very well how big the stack segment is going to need to be. Now, if you were intentionally writing highly recursive programs, we're going calling routines, calling routines, et cetera. You might know a little bit saying, this is going to be a big stack. But generally speaking, we can probably make a guess and say that most programs are going to need about this much space. And if we don't use it, well, it's not really that big a deal. Unused stack space is not expensive. We're not going to store a bunch of empty pages of stack space that we never use anywhere. So we can have a pretty big stack and not worry about it. The heap, we have kind of an intermediate situation. You know a little bit about the size of the heap because you've declared all these global variables, all the stuff in your.h files, for example. And that is going to be in the heap segment. That's going to be in your data area. But also, of course, you're going to do things where you allocate. Now, you may say, oh, I malloc data. Have I just increased the size of my data segment? Well, no, you haven't actually. What we typically do is we say, all right, at the time at which you're creating the load module, Let's make a guess about how big your data segment is. We know it's got to hold all the.h stuff. We know how big that is. You're probably going to ask for other stuff. We're not going to try to figure out how much you're going to ask for. We're just going to say, you need a big segment of about this size. And we are going to give you that segment. Now, it may turn out that that's right. Usually it does. If it doesn't turn out that it's right, and you need more space than that, what then? Well, in that case, you can increase the size of your data segment. There's a call in Linux, for example, called sbreak. Not malloc, but sbreak. Sbreak increases or decreases the size of your data area. And you can make that call if you want to. Your data area increases in size. OK, so once we have set up the various segments for a process that's going to start running, we're going to use dynamic paging and, under certain circumstances, swapping to support this abstraction of virtual memory. And of course, the key issue is we don't have that much memory. So we're going to have to hide from the user the fact that sometimes some of the stuff he thinks is in RAM isn't actually in RAM. OK, so the key technology to make virtual memory work, the thing that's hard-- you already have the MMU. The MMU's hardware. It does what it does. We don't worry about that from the operating system perspective. It's not terribly complicated to say, how do I deal with a page fault? You know what you have to do to deal with the page fault. You have to go out to the disk, get the data from the disk, schedule that, block the process, et cetera, et cetera, et cetera. That's very simple, predictable stuff. You don't need sophisticated programming to do that. The place where sophistication may come in, or at least where you have to do it right to achieve the performance goals you need to achieve, is in the replacement algorithm. The replacement algorithm is the algorithm that says, I have a certain set of pages sitting in page frames. I need to empty out one or more page frames, meaning I need to take something that's in a page frame and put it out onto the disk. And now I have to choose which one. There are millions of page frames sitting here. I could choose one of those millions of page frames and because of demand paging, I know that anyone I choose will not cause an error. Now I just copy the data from it out to the disk, remember where I put it, put it in the right table, saying this is where your data is now, there it will be no error. But if I choose badly, there will be very soon a page fault. If I choose well, there won't be. OK. And we have no predictive ability to say beyond hoping that locality of reference holds, which is just a hope. We have no ability to predict which pages are going to be required by our processes in the near future. All right. If we choose wisely, if our replacement algorithms chooses wisely, then typically whenever a process is running and it requests a piece of memory, that piece of memory is sitting in a page frame. There is no page fault for that. So we're going to keep some set of all of the pages in memory, a subset of the total number of pages. Typically, we're probably going to try to fill up all or more likely almost all of our page frames with pages. Now, on a typical computer, as I keep saying, we have many, many, many different processes running, perhaps hundreds. So what are we going to do? Are we going to say at any given moment, there's one process running for a single core system, four for a four core system, whatever. Are we only going to keep pages and page frames for the processes that are running? Probably not. This will turn out as we can see, if you think about it a little bit, and we'll go through this, this would turn out to be a poor choice. So what are we going to do? Well, we're going to say the page frames that we've got are divided up among many different processes. Each page frame holds some data for a particular process, normally, except perhaps on code sharing, probably not data that belongs to multiple processes. All right, every so often, our best guesses about what we should have in memory turnout to be completely correct. Somebody issues an address that requires us to handle a page form. At this point, we have to find a page frame that either is empty or that we can take the data from that page frame and put it onto the disk and use the empty page frame to deal with page fault. That's what we're going to have to do. Whenever we have this requirement to find a page to eject, a page that we're not going to keep in a page frame anymore-- it's in a page frame now, we need to empty that page frame-- we're going to run an algorithm to say, out of the pages that are currently sitting in page frames, which one are we getting rid of? Now, it turns out there's an optimal algorithm for doing this, provably optimal. That is, predict which address of all, which of the pages that you've got in memory will get a reference the furthest in the future, longest time before it ever gets referenced again. Kick that one out. This is the right page because it delays every, and if you do it every time, it delays every page fault as long as you possibly can. And this is provably, theoretically, the optimal. Fewer page faults, lower overhead, faster performance. One little problem with this optimal algorithm, it requires an oracle. What is an oracle? An oracle is a system that perfectly predicts the future. In this case, the prediction it's making is Which page of those sitting in a page frame will be accessed furthest in the future? We have no Oracle that will do that. So we can't run the optimal algorithm. Do we have to? Well, not really. What is the result of running a suboptimal algorithm, an algorithm that isn't quite as good as the optimal? The result of that is that we have more page faults than we could have had if we'd run the optimal algorithm. What's the result of that? A hit on performance. we will not be providing the optimal performance of our system. We will be providing something less than the optimal performance. In other words, it's slower. So this is often an acceptable trade-off, given that we don't have the ability to have an oracle anyway. If we are right, though, most of the time, if we come fairly close to what the oracle would have chosen, few page faults, high performance. If we're wrong, if we often make a mistake about which one we get rid of, and we have more page faults as a result, Poor performance. Now, if we have traces, if what you do is you make a trace of all of the accesses that a particular set of processes have performed while they're running, and then you run them again in exactly the same way, then we know exactly what they're going to do, because they're going to be deterministic. In which case, yes, we do have an oracle. However, you almost never have that option. That's not what you're doing. You don't run the same programs over and over and over again with the same results, because you already got the results of those programs. So we're going to approximate the or optimal, and the approximation is going to be based primarily on locality of reference. We're going to assume that what we got is probably close to what we want. But how are we going to deal with the fact that it isn't really quite right? We know that in the future at some point we're going to need something we don't have. We're going to have to go out to the disk. We're going to have to make room for a new page. Well, if we knew which one was going to be used furthest in the future, we could choose that one, but we don't know that. How could we guess which one is going to be used furthest in the future? One way of doing that is to say, well, if locality of reference applies, a page that has not been used in the recent past, and a long time since we used that page, locality of reference would suggest we're not going to use that page in the near future either. Choose that one. Okay, so if we kept track of when every page that we keep stored in the page frame was most recently used, when was the last time we used that page? We could then say, all right, we need to eject a page. Let's go to that set of pages that we've got in memory, look for all of the ones that have been not used very recently and choose the one that it was used furthest in the past. Let's predict that's the one that will be not used again furthest in the future. Okay, great. Sounds like a good idea. Unfortunately, it's got some serious problems. One problem is how do we manage to maintain information about when a page was accessed? Well, that would mean that every time you accessed a page you updated something to say, "Hey, I just accessed this page. It used to be accessed two seconds ago. Oh, I just updated it. Now it's been accessed this very moment." That's going to be very, very difficult. Okay, but if we could do that, We could do that. So how would we do that? What algorithm might we use that might achieve this result? You could say, well, I'm not going to try to achieve that result directly, but I will choose a page at random. Another you could use is first in, first out. You could say, okay, this page was brought in a long time ago, therefore, because it was brought in a long time ago, these other pages were brought in more recently, get rid of the one a long time ago. Both of those algorithms, random and FIFO, are terrible algorithms for page replacement. Analysis and history have shown that they are very, very poor choices. We aren't gonna use those. You might say, well, how about least frequently used? Keep a counter of how often you use a page. Get rid of the one that was least frequently used. Well, this isn't really better. I mean, consider the following. Let's say that you bring in a page and you've had one access to that page. Okay, it's been used once. Now, something else, another page gets accessed. You need to bring in another page. What's the least recently used page? one you just brought in. Probably you're going to access that page again because of the locality of reference, but it's only been used once so it gets kicked out. That would be a poor choice. Least recently used. Here we're getting closer to what we might want. You assume the future is going to be like the past, which is often a good assumption, and you say, "Okay, the one that hasn't been used at all recently, he's a good one to get rid of." All right, fine. So let's say we choose that. That's the algorithm we're going to use. How? How do we actually implement that? Well, let's consider. Every time a page is accessed, record the time. When you need to eject a page, look at the records of the time each page that's sitting in a page frame was accessed. Choose the one accessed furthest in the past. Okay, fine. We're gonna have to store timestamps somewhere to do that. How many timestamps? One per page. You got a lot of pages. 16 megabytes of pages, perhaps. That's going to be a lot of timestamps. Moreover, whenever we choose to eject a page, we're going to have to search through 16 megabytes of timestamps. That's going to be a lot of work. We have to look at every single one to find out which one is least recently used. Alternately, of course, we could try to keep them in order, which means every time we touch a page, we're going to have to resort the list. That would be very expensive, too. So, we can't really afford to do that. This is going to be very, very, very expensive. But if we were going to do it, how would it work? Well, here is a simplified example of just a few pages, where we only have a few page frames, and we are going to have to use true LRU to replace pages. So the reference stream is reference to pages. The bottom table here with these four rows is sets of page frames. We have four total page frames in this system, only four. All right, so let's say we start off with nothing in any page frame. So we have reference A come in. We're going to have to put that somewhere. Let's say we put it there and we make a record somewhere saying it was put in here at time zero. Then B comes in place. We have empty frames, so we'll put it into one of the empty frames, let's say frame one. And B was accessed at time one. C comes in. We have a reference to page C. We put C in one of the empty frames we have available. It was a reference at time two. comes in, we still have empty frames, so let's put in an empty frame rather than replace a full one. D goes in frame 3, accessed at time 3. Great. A is accessed again. Locality of reference. Isn't that great? Now we could say, okay, fine, A is sitting in frame 0, it stays in frame 0, but we update its timestamp to say 4. It was referenced at time 4. B, more locality of reference. B is in frame 1. We'll update its timestamp to say it was accessed at time 5. D. D is in frame 3. More locality of reference. Very good. We will update D's timestamp to say it was accessed at time 6. Now we get a reference to page E and E is not in any page frame. So we're going to have to choose according to LRU, if we're using LRU, which of the four frames is going to be used, it's going to have its page replaced. So we take a look and we say, okay, well, let's see, frame zero is referenced at time four, frame one was referenced at time five, frame two was referenced at time two, frame three was referenced at time six. Great, we'll replace the page that is sitting in frame two. So the red here indicates this is a page fault, a page replacement. Then we get a reference to page F. F is not in memory either. So we have to play the same game. Again, we look for the least recently used, and as you can see here, that would be the page in frame 0. So we're going to put F there. Another page fault. Then, unfortunately, we reference A again. That's the very next thing that happens. Well, unfortunately, we just kicked A out. So we are going to have to bring A right back again, and we're going to use LRU to say where to put it. Okay, let's put it in frame 1. So we put A there. Now his timestamp is nine. How unlucky. B is the next page reference and B was the one we just kicked out. Well, we'll have to kick somebody out to bring B back. Who? Let's say D. So we're going to put B in frame three and that was at time 10. Then we get C. C is no longer in memory because we replaced it some time ago. Use LRU to choose where to put it. Goes in frame two. So we put C and frame 2. Now add five page faults. Then we want D, and D as you can see is no longer in memory, so we're going to have to replace something. Which one are we going to replace? Well, frame 0 is the one with the oldest reference, so we put in frame 0. And then we're going to get a reference to A. Well, more locality of reference. Isn't that nice? A is in frame 1 at the moment, so we just update the stamp in frame one. Then we have a reference to E. E isn't in memory, so we're going to have to put it in the least recently used one. That would be frame three. We just used frame one, so even though A was put in frame one longer ago than B was put in frame three, we still replace B because A was reference. And then we get a D reference, and that's a reference that we win with locality of reference. So that's what would happen in this simple case. We would have four loads, those are the orange ones, where you're putting a page frame into a page into a page frame that's empty, there's nothing there, and seven replacements, those are the red events where we got a page fault. Okay, so if we're going to be keeping these time stamps for every page frame, where? Where are we going to keep them? Well, maybe we could build it into the MMU. The MMU would, every time a page was referenced, update the reference for that particular page frame. Great. Now this has a problem because the people who designed the MMU, the hardware engineers who designed the MMU, worked on the assumption that said we must have the highest possible speed we can manage for every translation. Now if it turns out there's a page fault, you know, there'll be stuff that isn't our problem and we can afford a little more cost there. But if it's going to be a correct translation, if it's going to be something where yes, that is indeed in the page frame, and we have it in the cache, we want that to be very, very fast. We want it to have the minimum possible time. Updating a counter is not as fast as not having to update a counter. If we have to update a counter on every memory reference, we are going to run the MMU slower, significantly slower. We won't do it. So they would say we're not going to do that. You go to the MMU designers, they say no, we won't do that. And you ask them, oh, can I get some help? And they say, all right, I'll give you a bit, maybe two bits, a read bit and a write bit. So for every page frame entry, for every entry they've got in their cache, they might have two bits of read bit and write bit. When you ask to read a page, they set the read bit. When you ask to write a page they set the right bit. That's all they do. That's not that expensive. They're willing to do that. Okay, so you can't keep these counts in the MMU. Where else could we keep them? Well, we have RAM. We have quite a bit of RAM. You know, we're essentially using, we're, this whole problem is not having enough RAM, but relatively speaking we have a lot more RAM than we have MMU. So what could we do? Well, we could do it in software. If we're doing it in software that means we're running instructions on every memory reference we are running instructions that cannot be good that would be multiple overhead instructions for every real instruction we wanted to perform that can't work we can't do that that would be really slow so we need something that is going to approximate LRU at a much lower cost so what requirements do we have from this surrogate, this cheap surrogate for LRU. Well, we can't have extra instructions performed on every memory reference. That can't happen. We can't cause page faults because of whatever we're doing with this particular surrogate. And when we are going to choose to replace something, we're going to say, we've got to get rid of one of the things in a page frame, we cannot look at every single page frame. Because if we did, that would take forever. That would be too long. What do we do? Typically, we use something that's called a clock algorithm. This is an approximation to LRU. So what do we do? We organize all the page frames into a circular list, page frame zero, one, two, three, four, all the way up to whatever the biggest page frame is, and after that, we go back to zero. Every time that we have a memory reference, read/write for a particular virtual address that maps into a page, that maps into a translation to a page frame, If the MMU has that page available, it will set the appropriate bit, the read bit, the write bit. Okay, whenever we need to find room for another page, got to eject something, we're going to say, all right, we need to find a page frame that we can empty out. Something in the page frame currently, we want to put the new page into the page frame. Which page frame shall we choose? Well, you start at some point in the list and you ask the MMU, "Is this frame, is its reference bit set?" If the answer is yes, clear the reference bit, go to the next one, and keep doing that until you find somebody whose reference bit isn't set. Okay? And the next, and then what you're going to do in addition to that is you said, "Okay, fine. That's the guy I choose. That's the page frame that we're going to eject. However, the next time that it comes time for me to eject a page frame, I won't start at zero. I will start at whatever came after the one I just chose. So I chose 1000. Next time I'm looking, I'll start at 1001. Now, this should sound somewhat familiar. This is a guess pointer. You remember next fit algorithms that we talked about in the previous class. Here's another guess pointer. We say we don't always start the search at the same place so that we can spread out the search across the entire range of things that we might be looking at. Now, this is not going to cause extra page faults and if we do it right, typically we're going to end up only having to look at a few pages before we find one that we can choose. Now we probably are not going to choose the true LRU, the least recently used. We're going to choose something else. But it's going to be, based on how this works, something that wasn't all that recently used. So here's how it might work in our same example. This is the same example we saw before but with a different algorithm, the clock algorithm. So this is just like what we saw before, except now we're maintaining an extra piece of information called the clock. The clock is shown here at the lower part of this, below the page frame. Okay, so down here at the bottom I've got the copy of what we did when we were using true LRU. That's how true LRU worked. That's what we saw on the previous slide. What happens if we use the clock algorithm? Well, we get same stream of references. So we're going to put A there. We're going to move the clock position to 1. Then we're going to put B there, move the clock position to 2. Put C there, move the clock position to 3. Put D in the remaining empty frame, move the clock position to zero. Okay, so far we've just been filling up empty stuff. We haven't had to replace anything. We get the reference to A. We set the reference bit. We don't do anything where we say here's a time stamp. We just set the reference bit. And because we managed to have a hit here, we don't update the clock position. It stays at zero. Then B is referenced, and B is a hit as well. So we set the reference bit for B, and we update the clock position to zero. Then we say we've got a reference to D. D is also in memory, so we set its reference bit, the single bit, and we remain at zero with the clock position. Now we get E. E is not in memory, so this is going to require us to eject a page. Something's got to go out of one of the page frames. What do we do? Well, the clock position starts at zero, so we look at zero and we say, "Oh gee, there was a reference bit there." So clear that reference bit, change the clock to one. Now we go to one and we say, "How about that one?" Reference bit was set. Clear the reference bit, update the clock to two. Now we look at two. No reference bit there. Okay, let's put E there. So you put E there. And the clock position goes to three. Next we get F. Okay, F isn't in any of the page frames. We're going to have to eject something so that we can make room for F. We look at three first because that's where the clock position was. That bit was set. So, therefore, we're going to update the clock position to zero after we clear the reference bit in frame three. Then we look at zero and we say, "Aha, that's not got its reference bit set. Let's put it there." And then we update the clock position to one. So, we start looking at one the next time we have a fault. A, of course, was a poor choice. So, we are going to have to do something to replace one of the existing page frame content for A. We'll have to eject something. Where do we start looking? We start looking at 1. Okay, fine, his bit wasn't set. We put A there. We update the clock position to 2. We get a reference to B. B's not there. We're going to have to start looking at 2, because that's where the clock position says to look. Is that bit set? No, the bit isn't set. Therefore, we're going to put B there. Now, note this is not the same place that LRU would have put the bit, as we can see from looking at the bottom part. but we update the clock position and we then say what do we get next? Oh, we get C. Well, C unfortunately was just replaced, so where are we going to put it? We're going to put it there because the clock position was at three. Three's frame did not have the bit set, therefore we put C in that frame. Again, this isn't the same place that we would have put it if we had used LRU, and we update the clock position, start our next search at a different place. Next we want D, D is not there, so we're going to have to put it somewhere. Clock position says take a look at frame 0. Bit set? No, bit isn't set. Put D there. Okay, then we update the clock position to 1. We now need to put A somewhere. Oh, look, A is there, so we're going to update the bit for A, and we're not going to move the clock. Then we want E. Well, E is not there. So what do we do? We say, okay, the clock says start at 1. What do we got at 1? The bit's set. Clear the bit. Set the clock to 2. Check 2. Okay, two. Oh, okay. E will go there because the bit wasn't set. Note again that this is not exactly what LRU would have done. So, then we get D. D is there, so we set the bit. How many loads? How many replacements? Four loads, seven replacements. So, how does this approximation to LRU compare to true LRU? Same number of loads and replacements. Now that is not always going to be the case, but it turns out to be the case here. But even though there are the same number of loads and replacements, it didn't always replace the same pages. What's that mean? Not much. Remember, the reason we were using LRU was we wanted to approximate the optimal algorithm. Couldn't do the optimal, LRU would be an approximation. The clock algorithm is an approximation to an approximation. So maybe it's not gonna make as good a decision as LRU, perhaps not. But maybe it's close. Maybe it's almost as good. And if it's almost as good, and it's a lot cheaper, which it is, it's a win. So we're going to do that instead of trying to do anything really like true LRU. Clock algorithms are the kind of thing we're going to use in modern virtual memory systems. Okay, now, as I've said several times, we have many, many different processes running at the same time, perhaps two, three hundred processes on a computer running simultaneously. What should we do? Well, every time that we are going to do a scheduling choice, we're going to do a context switch. Somebody was running on core three, somebody else will be running on core three a moment later. What that means is the references that we are getting to pages on core three before we do the context switch are going to be to different pages than after we do the context switch because a different process is going to run. It's going to want to work with different data. So what we expect to happen if we're doing anything that is sort of kind of like round robin, such as, for example, an MLFQ algorithm, is processes are going to run for a while, they're going to not run for a while, then they're going to run again. Processes even if they blocked when they finish-- when they are unblocked, they're going to start up from where they were before. And in either case, what this means is that when the process runs again, it is going to need the same data, probably, based on locality of reference that it needed when it was running on the previous iteration, before the clock expired, before it blocked, whatever. OK. Now, if we have, in the meantime, kicked out all of his page frames, all of his pages from page frames, because they weren't being reused. And why weren't they being used? Because other processes were running, not this one. So he wasn't using his pages. Therefore, he was not updating any of those bits, saying it was recently used. Therefore, probably if we needed to do some page replacement, his were the pages that got replaced. Then when he starts running again, he hasn't got his pages. What's he gonna do? Page faults. So we're gonna get a bunch of page faults every time we restart a process that stopped running in the past. Not necessarily because that process did anything wrong, not even necessarily because it blocked. It was just stopped in its tracks because of a clock. So how are we going to go about sharing the full set of page frames that we have available among the many different processes that want to use page frames? We could say we have a single global pool and we approximate LRU within the single global pool. We could instead say, let's divide up the overall set of page frames we have available and give 1/n. If you have n processes, 1/n of the page frames will be devoted to each process. And it will do LRU within its 1/n if it has page faults of its own. But other processes will not kick out its pages so that when it runs again, it probably has what it needs in page frames. There's another option called the working set page frame allocation, and we'll talk about what that means. First, though, how about that global pool? The global pool essentially says, let's approximate LRU for every page frame in the system. Generally speaking, if we have anything like round robin, we've got a lot of processes running. This means that every time a process gets its turn to run, probably it's not going to have the page frames it needs. This is going to be very, very, very problematic. This is going to cause a lot of page faults. So we don't want to do that. We could say you get 1/n of the total page frames for every process. Maybe 1/n, maybe some processes get more, some processes get less. But at any rate, you have a fixed number of page frames devoted to each process. And then when you have a page fault, you're going to do LRU approximations within the pool of pages that belong to the particular process. Now, of course, you have to ask, how many page frames per process? Well, it turns out that if you just say it's 1 nth, or any other fixed number, everybody gets 100, whatever it may be, this turns out to be bad because different processes have different degrees of locality. Different processes, depending on what they are doing, need different number of page frames to avoid page faults. Further, even within a single process, that changes over the course of time, depending on exactly what the process is doing. So this is a little bit like what is a natural scheduling interval. We had to do something to adjust dynamically using MLFQ, for example, for scheduling intervals. We have the same sort of thing going on here. Different processes have different needs, just as they had different needs for scheduling. So we need something that's dynamic and customizable, that's going to match whatever a particular process needs and change if the process behaviors change. This is what working set algorithms are all about. So the goal of a working set algorithm is to say we're going to give a customized number of page frames to every process that's currently running. As the process continues to operate, if its behavior doesn't change, then we will get to the right number of page frames, and it'll pretty much stay there. But if it turns out that it changes in its behavior, it needs more page frames, it needs fewer page frames, we're going to tend to adjust its working set size to the number of page frames it actually needs. The working set specifies how many page frames do we give to each process. Ideally, the working set size should match perfectly for every process with how many pages it needs to avoid page faults, preferably ever, but that's overly optimistic. We want a low number of page faults for every process. We want to have enough page frames in every processes working set to have a minimal, a small number of page faults per unit time for that process. OK. So then when a process is running, with lock it will not have any page faults. But if it eventually does, and some processes will, then we are going to do some kind of approximation to LRU within its own working set. Now, of course, that is going to have change over the course of time, and we'll talk about that in a moment. But how big should the working set be? Well, the bigger the working set size is, the more page frames you give to a process, the less likely it is to have a page fault. The fewer page frames you give to a process, the more likely it is to have a page fault. And it turns out you get a curve something of this shape. So the number of page faults increases as the working set allocated to that page, that process decreases. If you have too small a page frame allocation in the working set, then you get a large number of page faults for that process. If you get a very, very large number of page frames for that processes working set, if you devote a lot of page frames to a particular process, it probably will have very few page faults. But if you keep adding more page frames to its allocation, you're going to reach a point where it doesn't really matter very much. not going to tend to get much benefit from having a few more pages, page frames in its working set. Therefore, what you want to do is find a good balance between enough page frames so you don't have a lot of page faults, not so many page frames that you are wasting them. Because remember, there's a fixed number of page frames. If you have a lot, somebody else has a little. And if they have a little, they're going to have bad behavior. So in order to make sure that nobody has bad behavior, we can't give too many page frames to anybody. This is where we want to be operating, somewhere around the knee of this curve. So what is the optimal working set size for a process? Obviously, if we could, we'd like to hit the optimal. Well, the optimal turns out to be how many pages are required to be in page frames to avoid any page fault in the next time the process runs. It's going to run for some time slice. If it runs for that time slice without having any page faults, for that time slice, that was the optimal size. OK. If indeed you could reduce it and still not get any page faults, well, then the reduced number is the optimal size. Now, if you are running in smaller numbers, if you don't have as many page frames in the working set of a process as its optimal size, it's going to have a lot of page faults. and that process will run slowly. If, on the other hand, you have a whole lot of page frames in there, it won't have very many page frames. But on the other hand, perhaps you won't have enough page frames to avoid page faults in other processes. So how do you know what the working set size should be for this process? Watch. Observe what the process does. Generally speaking, if the process gets a lot of page faults frequently, you haven't given it enough page frames. If it almost never gets a page fault, Maybe you've given it more page frames than it needs. If it doesn't have enough, give it more. If it has more than it needs, take some away. And then you might say, well, OK, fine. I know how many I need. I've done that based on that. I've made my guess at it at least. Which one should it be? Don't worry about it. Use demand paging. So how do we implement it? Basically, we have a desirable working set size for every process at every given moment. the operating system will maintain this information. And what it will do is say, okay, if you have a process that's running, I'm going to assign page frames to that process. That process will be able to use this set of page frames to hold its pages. And if it gets a page fault, ordinarily what will happen is it will have to choose, well, we, the operating system will choose for it, one of the pages in its working set and we'll eject the page from that page frame and bring in another page that it needs because of demand paging. And we'll keep track of the fact that it had a page fault. Every so often, we will look at processes and we will say for every process, how many page faults did it have over the course of the last period? If it's too many, we're gonna need to get it some number of page frames. If it's very, very few and we need more page frames for somebody else, maybe that's a good candidate to take a page frame away from. Maybe his working set is bigger than it needs to be. So this results in a kind of algorithm that's called a page stealing algorithm. We have given working set sizes, set different numbers of page frames to every process. If one process needs another page frame, chances are pretty good we don't have any empty page frames just lying around. If we do, of course, we just give it one of those, but chances are we don't. In which case, we're going to only be able to get a page frame for this process by stealing a page frame from another process. How do we do that? We use a clock algorithm. It's a more complex algorithm because we're doing more complicated things than the one we saw. So I'm not going to be going through it in detail. But basically, what we're going to say is that every owning process will track the time used. And whenever it gets a page fault, it will simply do a page fault within its own working set using a clock algorithm. And if it turns out that we hit one of these periods where the operating system says, this guy's too many page faults, we're going to start looking at other processes, looking at their working sets, and see if we can take away a page from one of them. OK, now let's say that we are doing this. We're using working sets, and we're stealing pages when we need to steal pages to adjust working set sizes. What happens if the actual size we need for an appropriate working set for every single process, When we add up all those sizes, it comes to more page frames than we've got. We add up the requirements of every process to be able to run typically without a page fault, and it's more than you have available. What's going to happen then? Well, the working set size of a process says you can run for some time period, like your time slice, without getting-- with a low probability of getting a page fault. If we give you fewer pages than that in your working set, the probability you'll get a page fault will increase. And eventually, if it's small enough, it will be very, very, very likely that within your time slice, you are going to get a page fault. So, if you do not have enough memory to match the actual required working set sizes of all your processes, somebody is going to have a smaller allocation of page frames than they need. They're probably going to have a page fault when they run. What's going to happen then? Well, they're going to need another page. And of course, if this keeps happening, we can't just say, well, he is a slow process too bad for him. We're going to have to try to take away a page from some page frame from somebody else. Okay. Now, if it turned out that guy was running at just the edge of what he needed, when we take away his page frame, when he runs again, he's going to get a page fault. And he's going to need eventually to have some page stolen from somebody else. If it is the case that we just don't have enough page frames to manage the processes that we have, what's probably gonna end up happening is every time that we run a process, it's not gonna have enough page frames, which means it's going to have a page fault. And the result of it having a page fault is probably going to mean it's going to steal a page from somebody else, which is probably going to mean that whoever it stole the page from when they run, they're not gonna have enough page frames. They're gonna get a page fault. going to steal a page from somebody else. The person they steal a page from, the process they steal a page from, that process won't have enough pages in memory. The next time it runs, it's going to get a page fault. And we get page fault after page fault after page fault. What happens when we get so many page faults? Well, we keep going to the disk saying, "Give me this, give me this, give me this, give me this, give me this." And of course, we may also be writing pages because when you kick somebody out of memory, you may need to write that page out to disk so you don't lose bits. The disk is going to be overloaded. We are going to be requiring more work from the disk than it can do in a unit time. Overloaded systems, such as a disk drive, perform very slowly. And we're running at the speed of that system because processes that are waiting for a page can't do anything until the page has been delivered. If they're sitting in a big long queue waiting for their turn, then they're going to sit in that big long queue for a long time. If the queue keeps getting longer and longer and longer and it never goes away, it's going to be very, very slow. This particular behavior is something that has been observed in real computers and it is called thrashing. Thrashing is very, very bad from a performance perspective. Again, not a problem from correctness perspective, but your system's running so slowly that it hardly matters whether it's correct or not if you're thrashing. If systems thrash, everybody runs slow. And generally, thrashing doesn't go away. It just continues until you do something to make it stop. How do you make it stop? Well, you can't add more memory. You can't say get more page frames, because that's a hardware thing. You have to shut the computer down to do that at best. You can't squeeze working set sizes. You can't say, let's reduce this guy's working set size, because that'll just cause him that page faults, which in turn will cause thrashing. What you could do is say, "I have 300 processes. I don't have enough memory for 300 processes. Maybe I have enough memory for 299 processes, but not 300. If I only have one less process to deal with, maybe I wouldn't thrash." Well, you could do that. How would you do that? What you could do is you could say, "I will choose a process, some poor victim process. I will take every single bit of data that he's got and I will copy it to the disk. Everything. He loses all his page frames. Then I take the page frames that he used to have and I divide them up among the other processes. Maybe now they have enough page frames and we stop thrashing. So then everybody else runs fast. Of course, that poor victim who sent out to the disk is not going to run. He's not going to run as long as he's out on the disk. Well, if you're worried about thrashing continuing, if you bring him back, you could kick somebody else out and then bring him back. And you could round robin among proxies and gradually kick somebody out. That's going to be very, very slow, but it will prevent thrashing and it doesn't mean you have to kill any processes. Thrashing is not good. It's better to try to avoid getting in the situation at all, but that depends a lot on your available resources and your workload, and you may not have much control over those. Now, here's another issue that makes a difference in how we manage memory, especially when we're talking about moving pages back and forth between disk and RAM. Clean versus dirty pages. Okay, so let's say they've got a page fault and we brought in a copy of all of the bits that that page had and we put them into a page frame somewhere. Now we actually have two copies of that page. One of them is sitting in the new page frame. The other is still out there on the disk. Just because we copied the bits in from the disk doesn't mean that we threw away what was on the disk. We have the page, identical, bit for bit, in two places, in memory, on disk. Okay, now, if the memory copy is not modified, if we read the data that we have brought into that page frame, but we do not write any of it, not a single bit on that page, then the bits that we have in memory and the bits that we have on the disk remain the the same. They have not been changed. They're still identical. Okay. Such a page sitting in a page frame is called a clean page. What's good about a clean page is that if we choose to replace a clean page in memory, if we say we've got to take some page frame away from somebody and we take the page frame of somebody who is a clean page, then we don't have to We don't have to write that content out to disk because it's already out on disk. We had identical copies on disk. We had identical copy in the page frame. We're gonna throw away the one that's in the page frame to make room for something else, but we do not have to first write it to disk. Consider on the other hand that we brought in a page of data from disk. The bits are identical, but now the page that is sitting in the page frame in RAM is written. We changed the bits. Now we have two copies of the page, But the one on disk has old data that isn't correct anymore. The one on the page frame has new data that is correct. If we choose to eject that page, if that's the one that is chosen when we need to eject a page, make room for something else, then before we can eject that page, we must write its values to disk. Because otherwise, we would lose the updates that were made. What's out there on the disk isn't completely correct. We need to get a completely correct copy before we can make use of this page frame. This kind of page is said to be dirty. Dirty pages are more expensive to eject because we have to do a write operation before we can use the page frame. So at any given moment, we can replace any clean frame we want to replace. Of course, it's no longer in memory. If it turns out that's going to be accessed very soon, it's unfortunate that it was chosen. But we didn't have to do a write operation to disk. We had to do a read operation to bring in the new page that is replacing it, but we didn't have to do a write operation to save the one we discarded. Dirty pages, we can't do that. If we choose to put a new page that a page fault requires us to get into a page frame held by a dirty page, first we must write the dirty page before we can bring in the new page that the page fault requires. This means it's going to take twice as long to get that page because first we have to write, then we have to read. A clean page, we just have to read. Alright, so what could we do? One thing we could do is we could say, alright, when you're doing a choice of pages to replace, only replace clean pages. Okay, but that limits your flexibility. If you have a clean page, but you're reading and reading and reading and reading and reading from that page, you don't really want to replace that one because you're probably going to keep reading that page and you'll need it again. On the other hand, if you brought a page in, you wrote it and you have not touched it for the past five seconds, you may never touch it again. So that was a pretty good page to get rid of from the point of view of approximating optimality. So having a lot of dirty pages in memory limits you on what you can do for page replacement. You don't have good choices for page replacement. So you don't want to have a lot of dirty pages in memory. What do you do about that? Preemptive page laundry. So this basically says I would like to have lots of clean pages in memory. If almost all the pages in my memory are clean, I have a lot of flexibility in page replacement. If very few of my pages in memory are clean, I have much less flexibility or the page replacement is much more expensive. So how do we get a lot of clean pages in memory? Well, some pages in memory going to be clean. Code, for example, will not be written. Other pages in memory, though, will tend to get dirty. However, a page that is dirty may not be dirtied again. You've updated a word on the page. You're now going to keep reading that page, possibly, so you are going to access it, but you're not writing to that page anymore. In that case, if only that page existed in the same bits on the disk as it currently is in memory, then you you could say, oh, that's a clean page. Well, you could just write the page out to the disk, replacing the old version of the page on the disk with a version that's in memory, but not actually ejecting that page from its page frame at all. Now, it sounds like that's an expensive thing to do, but one thing you have to be aware of when we're talking about computer systems is the average computer isn't really all that busy. It has periods of time when it is very, very busy and things must perform well at those periods of time. There are other periods of time when effectively nothing is happening. Nothing is going on at all. Now, if nothing is going on at all, you don't really even have processes that need to run. They're all sitting there waiting for stuff or they're just inactive. In which case, why don't you do something that's going to make life easier when it gets busy again? This is one of the things you can do. There are other things that operating systems tend to do during these slack periods. So if the operating system observes that, gee, none of my user processes have anything to do, it may choose to say, why don't we go and do some preemptive page laundering? Even if the processes are fairly busy, if they aren't generating I/O activity on the disk, the disk can do a certain amount of I/O activity per unit time. If you aren't generating any I/O activity to the disk, you're sort of wasting that ability of the disk to do some work. So you might say, OK, if I trapped into the operating system to do something for some process. Maybe if there's nothing going on with the disk and there hasn't been for a while, there's no queue of stuff that this needs to do, maybe I'll throw in a few requests to write some data. A little overhead on this trap, but on the other hand, maybe it'll make life better later on. That's what preemptive page laundering is about. When it is sensible to do so, you look through all the dirty pages you've got and choose some of them to write to disk. You don't throw them out of the page frame that they are currently in, you just make the page clean by ensuring that this copy matches the current version of the in-memory copy. OK, so that's all I intend to say really about memory management. Pretty much finished with that topic. In conclusion for today's lecture, we managed to solve many of our problems in memory management by using paging. We are able to use the RAM much more efficiently, which means we can run more processes, get more done on our computer. Now, in order to make this an easy way for programmers to interact with the memory requirements of the system, we provide virtual memory. Virtual memory gives each process the illusion that it has complete access to all the memory in the computer, or at least whatever memory it has requested at whatever location it's requested. We achieve virtual memory by using paging. We use demand paging in particular so that we don't have to predict in any way ahead of time, here are the pages that we need to store in the limited page frames we have in memory. The key technology to make virtual memory work properly and efficiently, fast, is page replacement. We have to have good page replacement. And when we are talking about modern computing environments where we have many, many processes running at the same time, we have to consider in our memory management and in our implementation of virtual memory that there are multiple requirements on what's going to be kept in page frames. We cannot optimize for the current running process or set a few processes on the few cores. We've got to consider what other processes that aren't currently running are going to need when it is their turn to run. And that is typically done with something on the order of a working set approach.