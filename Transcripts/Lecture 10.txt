In the previous class, we had been talking about synchronization problems that can arise, things involving the overheads associated with synchronization, creation of convoys, things of that nature.
 But there is another, even more serious problem with using locking for synchronization purposes.
 Now we discussed in several classes back that we didn't have other good alternatives to to locking in many cases for ensuring that we get proper mutual exclusion.
 So we're going to have to use locking in order to achieve the synchronization that our systems require for correctness.
 But if we do, we have to worry about this other problem, the problem of deadlock.
 We'll talk about what deadlock is, and we'll talk about how we deal with it.
 And we'll talk about a few related issues of synchronization in this class.
 So first we'll talk about what is deadlock, what's the problem.
 Then we'll talk about approaches to handling the problem.
 Now, some of the approaches that people have developed for handling the problem are not all that attractive.
 So we'll talk about a more general approach that deals with many, many different synchronization problems with its own shortcomings, but it's also its own advantages.
 And we'll talk a little bit about how we simplify synchronization by making use of certain aspects of programming.
 But first, deadlock.
 So what's deadlock?
 Deadlock occurs when we have two entities, like two processes, each of which has locked a resource, and they are waiting before they can proceed to do their next piece of work for the locked resource the other holds.
 Now, if they are not going to release the lock they currently hold until they get the lock they need, A will not release A's lock until it gets the lock B holds.
 B will not release B's lock until it gets the lock A's hold, since A holds.
 Since neither of them is going to release the lock, and neither of them is going to make progress until it gets the other lock that they can't currently get, neither of them makes progress.
 They essentially sit there forever, doing nothing, waiting for the other to release a lock.
 Now, this is a problem that we have known about in computer science for a long, long time.
 This is not a newly discovered problem by any means.
 Many years ago, when people were first thinking about the theory behind synchronization, they recognized the possibility of this problem.
 And in order to have a better understanding of how the problem worked, they created a totally artificial problem that represented the situation in which synchronization problems of deadlock could arise.
 This problem was called the dining philosopher's problem.
 What is that?
 Well, here's the problem.
 Now remember, this is totally artificial.
 This has nothing to do with the real world.
 Never really occurs.
 It is simply a way to understand deadlock, to think about deadlock.
 So in the dining philosopher's problem, we have five philosophers.
 They are sitting around a table.
 Now they're going to sit there essentially forever, so they have to eat every so often.
 They are provided with plates of pasta that sit in front of them.
 Every philosopher has his very own plate of pasta sitting in front of him.
 They also have forks.
 So there's five forks spread around the table in the fashion that you see.
 Now as it turns out, according to the way these philosophers do things, the philosophers can't eat the pasta, the spaghetti, let us say, until they have two forks to help them eat the pasta, one in each hand.
 Okay, now, they can only pick up one fork at a time, though.
 They can't pick up two forks simultaneously.
 Obviously they first have to pick up one, then they can pick up the other.
 The philosophers are all different schools of philosophy, and they will not negotiate with each other.
 They won't talk to one another and say, "Shall I pick up the fork?
 Shall you pick up the fork?
 Please release this fork."  They won't do any of that.
 They will simply do what they feel is best.
 They may eat at any time.
 We don't know when a philosopher will want to eat.
 The philosopher will try to eat at some point, but we don't know when.
 There's no schedule.
 Now, given these constraints, we have to have some method by which we have the philosophers behave that will ensure that they do not deadlock while they are trying to eat.
 Deadlock being a situation where two philosophers do not have two forks, they both want to eat, they can't get the other fork.
 Now, in order for us to have a true solution to the dining philosophers problem, it has to deal with every possible circumstance that could arise within the limitations of the rules that we have set.
 That's what we mean by an absolute solution, something that always works.
 So dining philosophers is the classic problem for illustrating deadlock.
 And in fact, it was created to illustrate deadlock.
 That's what it's for.
 It is clearly extremely artificial.
 The rules don't make any sense in the real world.
 They never happened in the real world.
 They wouldn't happen in the real world, but because we want to understand deadlock, that's the way they're set up.
 It will cause deadlocks if we aren't careful, if we don't have some good solution.
 Now, generally speaking, in order to come up with a good solution, we're going to have to change some of these rules.
 And that's sort of the point that we have here.
 The problem is used to help us think about in what way can we relax the rules, change the rules, alter the behavior of the philosophers in limited ways such that by making those slight alterations to the rules, deadlock becomes impossible.
 That's the point.
 If we change the problem a little bit, can we solve the deadlock problem?
 So here's one deadlock that could occur if we don't do anything, if we just follow the rules I've already outlined.
 All five philosophers at the very same instant decide they want to eat.
 Okay.
 Each grabs one fork.
 So let's say they each grab the fork that is to their right.
 Now, they each have one fork.
 No philosopher has two forks.
 Philosophers won't eat without two forks.
 So none of the philosophers can currently eat.
 And no philosopher will ever release a fork that he holds until he eats, until he's already eaten.
 Well, he can't eat without a second fork.
 He won't release the fork he's got until he has the second fork and has eaten.
 None of the other philosophers are going to release their forks.
 They're all in the same situation.
 So, the effect that we're going to see here is deadlock.
 Nobody is ever going to eat again.
 That's not what we want.
 We don't want deadlock.
 So this is a situation that would lead lead to deadlock if we followed all the rules as outlined.
 Now, as I said, the dining philosopher's problem is extremely artificial, nothing to do with the real world at all in its details.
 So why do we care about the dining philosopher's problem?
 Well, we don't care about the dining philosopher's problem per se.
 We care about deadlocks.
 Dining philosopher's problem is meant to illustrate deadlocks.
 That's what we care about in the real world.
 And it turns out deadlocks are real.
 They are not something that is purely an outcome of trying to perform the dining philosopher's problem, to think about that.
 And if you think about this a little bit in the realm of parallel interpreters, such as different processes, you can very easily see that there are going to be circumstances in which the processes actually will act like philosophers.
 Unlike people, processes tend not to negotiate.
 Unlike people, processes can get into a situation where they don't do anything.
 If they're blocked, they don't do anything.
 And unlike people, philosophers can't usually, excuse me, processes can't usually come up with alternate solutions for when they run into something that seems to be a problem.
 It turns out that this kind of situation does indeed arise in real world parallel processes or multiple threads working within a single system when you have locking.
 And when they occur, when these problems occur, when you get deadlock in a real system, it's pretty catastrophic for the system.
 The system is not going to run successfully in most cases when deadlock occurs until the deadlock is broken.
 And unless you have a special solution for breaking deadlock, your only choice is typically go in and shut down the system and start it all over again and hope we don't get back to the deadlock condition.
 It's hard to figure out why the deadlocks occurred.
 In special cases, if you knew exactly why this deadlock occurred, you could perhaps alter the code, alter the behavior of the processes or threads, so that they wouldn't deadlock in the future.
 But unless you know which processes, which threads are going to deadlock, and what are the circumstances that cause them to deadlock, it can be very difficult to fix your problem.
 You know, you're getting locks because you have critical sections.
 Presumably, you have multiple critical sections because deadlock requires multiple locks.
 Which locks are important?
 If you have 77 different critical sections scattered throughout your code, which of the 77 critical sections may lead to deadlock?
 Are there some of them that won't?
 Are there some of them that will?
 It's very hard to tell.
 It's hard to debug when it occurs, because it may not occur for a long, long time.
 It's not one of these things where you can say, well, once we have our code up and working in the beta form, We're going to do a lot of testing.
 We'll find all the deadlocks and solve them there.
 They may not occur then.
 They may occur only under various specialized circumstances, particular timings, which may not be what you tested for.
 So typically, the advice that you get from designers is, well, let's design it so we don't get deadlocks, or at least we're very unlikely to get deadlocks.
 If you understand how deadlock works, it is more likely when you try to build this kind of code, which by the way, is the kind of code we use nowadays.
 Nowadays we have very, very complex programs or different components, different things that are working independently but cooperate with each other.
 And it is thus very common to have parallel interpreters running often on different machines that are trying to achieve one common goal, one common outcome.
 Deadlocks can occur in such things.
 And unless you've thought through ahead of time, they're more likely to occur.
 So if you don't understand deadlocks, and you don't understand even the possibility of deadlocks, there's a fairly good chance you will build a system that might deadlock in the future.
 It may not be obvious when you're going to deadlock.
 Now in particular, one of the things that happens with modern systems, of course, is it's not the case that you build a system, you've written the code in a particular way, that's the way the code remains.
 If you are successful, if you build a good system, a system that actually gets used, then presumably you are going to evolve that system.
 It's going to change.
 Bugs are going to be fixed.
 Performance will be improved.
 New features will be added.
 It will be made more powerful in different ways, all of which implies that you're changing the code.
 When you change the code, what used to work yesterday doesn't work quite the same way tomorrow.
 That's the intention, after all.
 You intended things to work differently.
 So what seemed OK yesterday may not be OK tomorrow. the critical section that was only shared by two processes become shared by seven processes.
 You add several more critical sections.
 You forget that something is a critical section and leads to other kinds of problems, which in turn leads to deadlock.
 So it may also be the case that even if you don't change the code, the data you're working with may change.
 And if the data changes, it may lead to different code paths through different components that are performing the parallel computation, which could lead to different patterns of locking, And that, in turn, could lead to deadlock.
 When you have errors in your code, that can lead to situations where, for example, you don't release a lock.
 You probably should have released, at least not as quickly as you should have released it.
 Maybe deadlock will occur because of that.
 So there's all kinds of dynamics that can lead to deadlocks suddenly popping up where they didn't appear to be before.
 Modern software is built from components.
 We have lots and lots of different components, some of which can be quite complex, some of which components of their own.
 So they are often developed without knowing that the other component even exists.
 Somebody has a component that does good graphical manipulations.
 Somebody else has a component that is able to analyze a large data set and make good predictions about what's going to happen.
 Somebody else has a component that displays data in a very interesting and useful way, all made completely separate from each other.
 But you smash them all together because you're going to get a great program out of having all three of those capabilities in one program.
 Those three things were not built with each other in mind.
 Therefore, there probably was not any possibility even before you put them together that you would have thought about, well, will one of them deadlock with the other?
 So there's a lot of complexity in these services in many cases.
 These are big code components, millions of lines of code in the combined set of services that we are building our overall system out of.
 And we don't know in all cases exactly what's going on inside those components.
 Sometimes they're black boxes.
 The black boxes may be doing things that we don't understand in terms of locking.
 So also, of course, deadlocks aren't the only synchronization problem.
 There are many, many other synchronization problems.
 And as we will see, what we do to deal with deadlocks may only help us with deadlocks.
 It may not help us with the other synchronization problems.
 All right, now, in terms of the locking that we see with deadlocks, there are essentially two kinds of resources that we work with.
 One of them is a commodity resource.
 The other are general resources.
 Commodity resources are a special type of resource.
 They're essentially something where you got a pool.
 You got a bunch of stuff.
 And this stuff can be shared between multiple different parties.
 Each party will want some piece of the overall pool.
 So you may have 23 different parties, different processes, different programs, all sharing the same pool of resources.
 And they each get their own separate part of it, which they don't share with each other.
 It's their own.
 So for example, you're sharing RAM.
 You're sharing the amount of bandwidth that you can send out over a particular network interface.
 You're sharing a storage device, such as a flash drive.
 And you have devoted some portion of that to everyone.
 Now it's frequently going to be the case that what you're going to do is say, effectively, I am giving you a lock.
 You, one of the parties who wants part of the pool of the commodity resource, I'm giving you a lock on part of that pool and other people get locks on their part of the pool.
 And if you're not careful, these locks can effectively lead to deadlock.
 Now, typically this is because you have part of the pool and you need more or somebody else needs more and the pool has already been allocated completely and they're going to block until they get the more that they need.
 So the way that we handle that kind of deadlock most commonly is try to avoid it by saying, we have some party, somebody who is in charge of the entire pool.
 And the way they're going to respond to requests for a process or a thread to use part of this pool of resources is they're going to, among other things, think about, if I give them this or if I promise them this, will that potentially lead to a deadlock?
 or can I be sure it won't?
 Now, that's a special kind of resource.
 More commonly, when you think of locking, you think of general resources, such as a data structure that requires serialized access.
 You want to make sure that there's mutual exclusion on the use of that data structure, or a piece of code that should only be run by one process at a time, or something of that nature.
 Now, in those cases, of course, the way that we deal with a mutual exclusion problem for these resources is we put a lock on the resource in question.
 Whoever needs to use that resource must acquire the lock before they use it.
 They release the lock when they're finished with the resource.
 There can be a lot of these.
 In complex systems you may have hundreds or even thousands of different resources that are being controlled by locks.
 So if you are going to try to prevent deadlock among that kind of resource, typically at design time you're going to try to do something.
 You're going to try to say I'm going to design my system in such a way that I'm not either possibly going to get a deadlock, or at least I'm not likely to get a deadlock.
 Okay, now, you might say, "Well, how am I, as a designer of a system, going to ensure that my system does not experience deadlock?"  The way that you can do that is by understanding deadlock.
 And as it turns out, the people who did the theoretical work on this many, many, many years ago, decades ago, they thought very deeply about deadlocks using, among other things, the dining philosopher's problem to think about it.
 And they said, "You know, there are four conditions required, four, that must happen before you can possibly get a deadlock.
 If one of these four conditions is not true, any one of the four, no deadlock is possible."  Which in turn implies that if you can design a system where you can be sure that under all circumstances at least one of those four conditions isn't true, you won't have deadlocks.
 You may have other problems, but you won't have deadlocks.
 So what are the four conditions?
 We'll talk about these in detail, but briefly, one, mutual exclusion, two, incremental allocation, three, no preemption, four, circular weighting.
 We'll talk, as I said, about each of these in more detail in turn starting with mutual exclusion.
 We know what mutual exclusion is.
 We talked about mutual exclusion in previous classes.
 What is it?
 There's a resource.
 The resource can only be used, should only be used by one entity, one process, one thread, one machine, whatever it may be, at a time.
 So the way that we're going to ensure that is by saying we will use some kind of locking mechanism that will say one party is permitted to use this resource.
 As long as this party holds this lock, parties can't use the resource.
 When the party holding the lock is finished, he releases the lock, then somebody else can use the resource.
 Okay, now, if you do not have any situation in your system that ever requires mutual exclusion on anything, you're home free.
 No deadlocks.
 And we talked about other advantages, performance advantages, to being in such a situation.
 So that's one situation, one thing that is required for deadlock.
 What else is required?
 Incremental allocation.
 Now you've got processes, you've got threads, things of that nature, interpreters of some type, and if you allow them at any given moment in their run to say, "I need a lock on something.
 I didn't have the lock.
 I need the lock."  If at any point in their run, they are permitted to ask for more locks, they've got some, they ask for more, then you're in a situation where you've allowed incremental allocation, which means you may get deadlock.
 Now, if you don't allow incremental allocation, if you say you process have this set of locks, you may not ask for any more locks while you, as long as you hold those locks, then you don't have incremental allocation.
 So, the way you deal with that in actually writing program is to say my interpreter, my thread, my process, will at one point, before it has any locks, figure out all the locks that it needs to get.
 It'll ask for all those locks.
 It'll get all those locks.
 It'll use everything it's locked.
 Then it will release all the locks.
 Now, if you do that, you're never in a situation where you are holding a lock and waiting for another lock because you either got them all, you asked for a set of locks, you got them all, and you're not going to ask it for any more locks.
 So you're not going a deadlock.
 If everybody is in the same situation, nobody ever asks for more locks, then you're going to be okay.
 So that's going to work out just fine.
 Of course, if you think about what we said in previous classes, you can see there are going to be some issues here.
 So let's say I've got a process that's going to run for hours and hours and hours, and at various points in the process, it's going to require holding certain locks.
 And maybe it's supposed to hold those locks, as we said in a previous class, for a short period of time.
 We hope it's going to hold them for a few nanoseconds or microseconds.
 Certainly, we don't want it to be holding locks for hours and hours and hours because then nobody else can use those resources for hours and hours and hours.
 So if we say that for the entire 33-hour period this process is going to run, the very first thing it does is figure out all the locks it ever will need.
 It acquires all those locks and then it holds them until it is finished running.
 Well, it's gonna be holding locks for hours and hours and hours, preventing other people from using them.
 It's not quite that bad because once you're finished with the resource and you're not gonna use it any again, you could release the lock.
 You can incrementally deallocate, can incrementally allocate.
 But still, that means you may hold some locks for a very, very long time, including for a very, very long time before you actually need them.
 Okay, so that's a potential problem, but if you don't allow incremental allocation, there won't be any deadlock.
 Third condition, no preemption.
 By preemption here, we're not talking about preemptive scheduling.
 We're talking about preemption of locks.
 Thread 12 holds a particular lock.
 If we do not allow preemption of locks, thread 12 holds that lock Thread 12 says release that lock.
 If we allow lock preemption, at some point, some entity in the system can say, "Hey, Thread 12, I know you hold this lock.
 We're taking it away from you.
 This very next moment, at the very next instruction that you run, you don't hold that lock anymore.
 We've taken that lock away from you."  Now, the reason that this can mean you avoid deadlock is is because deadlock is caused when there are two or more processes which hold locks and wait for locks.
 If you say, under the right circumstances, somebody holds a lock, even though he does not want to release that lock, is forced to release the lock, then somebody else who is waiting for that lock can acquire the lock.
 Therefore, they can make progress.
 Now, of course, there are difficulties with this, as you can see.
 But if you can't take away locks from people, if parties, processes, threads, the whole locks get to hold them as long as they want and can never, never have them taken away, then you might run into deadlock.
 But there is a fourth condition still required, circular waiting.
 As we said, the problem that we have in deadlock is A is waiting on a lock held by B, B is waiting on a lock held by A.
 It can of course be a much more complicated thing.
 A is waiting on B, B is waiting on C, C is waiting on D, D is waiting on A.
 But at any rate, there is essentially a circular wait here.
 Parties are waiting on things that other parties hold and it essentially amounts to a graph.
 If you plot this out in a graph, then you would be able to look at the graph and say, well, gee, here is a list of parties, a list of people in this graph who are all waiting on each other.
 Now, assuming all those other conditions that we talked about, If this occurs, then you got a deadlock.
 On the other hand, if you prevent it from occurring, somehow or other, you don't have a deadlock.
 So, what could you do?
 Well, if this is not what happens, if A is waiting on B, B is waiting on C, C is waiting on D, but then you prevent D from waiting on A, sooner or later, presumably, people will start releasing locks and things will work themselves out without there being a deadlock.
 Maybe not very quickly, but sooner or later.
 Now, in order to work on this particular condition, what you could do is say, "Well, why don't I maintain a graph, one of those graphs that you talked about that's going to keep track of what locks are held by what parties to see if we have this circularity."  This kind of graph is called a wait-for graph.
 Here's what it might look like in a simple case.
 Let's say we have two threads, thread one, thread two.
 And let's say we have two critical sections, critical section A, critical section B.
 Being critical sections, they each have a lock associated with them.
 So there's two locks in the system, one for A, one for B.
 At the start of this little example that we're going to go through, nobody, no thread holds any lock.
 These are the only two threads.
 Those are the only two locks.
 So let's say thread one is running along and he says, "I need a lock for critical section A."  So we give him a lock for critical section A.
 Now, if we are using a wait for graph, then we have actually got the threads and the critical sections represented as nodes in the graph.
 So there are four nodes in the graph.
 And now, because we have given thread one a lock on one of those critical sections, we're gonna put in an edge in the graph.
 Before there were no edges.
 Now there will be an edge.
 The edge will go from the thing that was locked, critical section A, to the party that holds the lock, thread one.
 Okay, we look at the graph and we say, is there a circuit in this graph?
 No, there isn't.
 Okay, so now things continue.
 Thread two runs and thread two says, I need a lock on critical section B.
 Nobody holds a lock on critical section B at the moment, so we give thread two the lock on critical section B.
 Our wait for graph is then updated to have a second edge, an edge leading from what is locked, critical section B to the party holding the lock, thread two.
 Is there a circuit in the graph?
 No, there is not.
 These are directed arcs, as you can see, based on the fact that we have an arrow.
 So they point in a particular direction.
 Things continue.
 So let's say thread one requests a lock for critical section B.
 Now, critical section B is still locked by thread two.
 So thread one cannot get this lock.
 That means in ordinary circumstances, such as these, the thread one is going because he's trying to get the lock, he can't get the lock.
 So we can't give him the lock, but there's still no problem from the point of view of deadlock.
 Because what we can do is in our graph, we can say, all right, there is now a lock relationship between critical section B and thread 1.
 Not a lock relationship based on thread 1 holding the lock, but a lock relationship based on thread 1 needing the lock and being unable to get it at this moment.
 So we do set up an edge between critical section B and thread 1.
 But unlike the other edges we've set up prior to this, this edge is going to be directed from the requesting party, thread 1, to the critical section he wishes to lock, B.
 So as you can see, the arrow now points in the other direction.
 Is there a circuit in this graph?
 If you look at this and you understand anything about graph theory, you can say, no, there's no circuit in this graph.
 We continue to run.
 Thread 2 isn't blocked, so thread 2 continues to run.
 And thread 2 says, gee, I need a second lock.
 I need a lock on critical section A.
 It attempts to create a lock on critical section A, but of course it can't because critical section A is currently locked by thread 1.
 So given that thread 2 is now blocked and is waiting for a lock on critical section 2, in our wait for graph, we would add an edge and that edge would be directed from thread 2, the party wanting the lock, to the resource it wishes to lock, critical section A.
 Okay, so now we have that edge added to our graph.
 Do we have a circuit in this graph?
 Well, we go from 1 to B, from B to 2, from 2 to A, from A to 1.
 We've gotten back to where we started.
 There's a circuit in the graph, therefore this represents a deadlock.
 This is a genuine deadlock.
 Okay, so those are the four conditions under which we could determine that deadlock occurs.
 Without all four conditions, remember, has to be all four conditions possible before deadlocks can occur.
 So let's go back to those commodity resources.
 Now in a commodity resource you can say, fine, You know, you've got 100 gigabytes of memory, let us say, and processes ask for memory.
 Now you could just say, I give them the memory they ask for at the moment they ask for.
 But we could do instead is say, well, people often ask for more than they need of any commodity.
 You know, they ask for a large amount, they're only gonna use a little bit of it.
 So why don't we make a reservation instead?
 Why don't we guarantee them?
 Yeah, you want 50 gigabytes, fine.
 We'll give you 50 gigabytes, but right now you only need two.
 That's all you're really using.
 Later on, maybe you'll need more.
 Maybe you'll need up to 50 gigabytes.
 We've honored your reservation.
 We guarantee you that when you need it, we'll give it to you.
 So we can have a resource manager that works that way.
 This is going to be beneficial because it means that we can guarantee resources for a large number of parties, more resources guaranteed than we actually have available.
 That means that we don't tell people, oh, you can't run because maybe we aren't gonna have that 50 gigabytes you need available.
 We say, go ahead, we give you the reservation.
 Now, if it turns out that when you say, I really do need my 50 gigabytes, we have it available, then clearly there's no problem.
 We just give you the 50 gigabytes.
 If it turns out that they're not available, maybe that's going to be a problem, but maybe we can just say, well, you're gonna have to wait a while.
 Yes, you have a reservation, but we've currently used up all of our memory.
 Wait a little while, Other parties will release their memory, then we'll be able to give you your 50 gigabytes.
 That's fine, provided those other parties actually are going to release their memory.
 Why might they not release their memory?
 Well, what if there was a deadlock?
 What if there are two resources and you're waiting for 50 gigabytes of memory and the other guy is waiting for 10 megabytes of message send per second, and you hold all the message byte per second locks holds all the gigabyte locks, you're never going to release any of those locks.
 There would be a deadlock.
 So we don't want that to happen when we are doing commodity resources.
 So what do we do?
 Well, we still want to try to do something that doesn't require us to allocate permanently.
 A bunch of resources don't actually get used.
 We want to allow reservations.
 But we want to allow reservations to be given in such a way that they cannot possibly cause a deadlock.
 So we do this a resource manager that says, fine, I will accept reservations.
 When I get in reservations, I'm going to analyze what's happening here.
 And I'm going to say, well, maybe I can actually give him what he asked for right now, maybe I can't, but what I want to be sure of is if I honor, if I say, yes, you get your reservation, I want to be sure that there will always be at least one party who is going to finish and release their resource.
 That means that there'll be more resources for you later if there weren't enough when you actually said, "I need my reserved resources."  May have gotten blocked while waiting for them to come free, but there'll be more.
 Now, it's not enough to say, "At this moment, there's one guy who's definitely gonna finish and release his resources."  You also need to be able to say, "Well, if he finishes and releases his resources, then there'll be a second guy who we can be sure will finish, and he'll release his resources," and so on and so on and so on.
 So that is possible.
 There are algorithms that do that when you're dealing with these kinds of commodity resources.
 We won't get into the details of them.
 However, what this does mean is that under some circumstances, when a process asks for a particular amount of resources, it may be granted those.
 But when it runs tomorrow again and says, I want the same resources I had yesterday, please reserve those for me, it may be told, no, I can't give you those resources.
 That being the case, of course, we now must have our processes or threads able to deal with rejection.
 They have to be able to say, OK, I asked for 50 gigabytes and the guy who handles the memory told me he can't have 50 gigabytes, can't give you that resource.
 The reason he told you that was not necessarily that he didn't have 50 gigabytes right now, but that if things don't go well, by granting you that resource, he may get in a situation that leads to a deadlock.
 So he didn't grant you the resource.
 He said no.
 Your process, which asked for the 50 gigabytes and didn't get it, has to be prepared for not getting it.
 It's got to do something based on the fact that it didn't get what it wanted.
 Now, it could go into a spin loop and keep asking, but it may never get those.
 It could say, "I will ask for a smaller reservation and see if I can get by with that."  Maybe that'll be granted.
 It could say, "I'm going to go and do something else that doesn't require me to have the guarantee of the memory.
 I'll do something that doesn't need that memory and as long as I'm doing that then it doesn't matter that I can't get a reservation on these.
 But you have to do something.
 Now generally speaking the problem that we're seeing here is with these commodity resources you kind of have a choice.
 You can either have overbooking or you can have underutilization.
 What do we mean by that?
 Well the problem is with these resources is that you can't perfectly predict when you were writing a process and running a process how much of the resource you're going to need.
 But you do know, perhaps, that there's a maximum amount you need.
 And on some days, with certain types of data, certain conditions that occur in the program, whatever it may be, sometimes you need that full amount.
 Often, perhaps, you don't.
 You can get by with less.
 Well, in that case, instead of asking permanently for the full amount as long as you're running, Wouldn't it be nice if you could just reserve the full amount and say, "Okay, I need 50 gigabytes, but at the moment all I actually require is three.
 Give me three gigabytes right now.
 I want a reservation for the other 47."  Now, if you can be guaranteed that the 47 will be available if you should happen to need them, then you're doing fine.
 And the 47 gigabytes aren't actually being used by you.
 They're just going to be available if you need them.
 You don't need them, they won't be available.
 They'll be used by somebody else, then that's fine.
 So you have to, though, if you're going to do that, say, all right, what's going to happen if I promised you the 50 gigabytes and you come back and say, where's my 50 gigabytes and I can't give it to you?
 Something's going to have to happen.
 So you have two choices when you're doing this kind of thing as a resource manager.
 Either you can grant requests up until a point that everything has been reserved, but you will not grant any more than that.
 So you have 500 gigabytes, you'll grant up to 500 gigabytes of reservations, but not a gigabyte more than that, not a byte more than that, because you might not be able to meet the reservation.
 Well, in that case, what's probably going to happen in realistic circumstances is much of your memory just isn't going to be used by anybody because they only need it under extraordinary circumstances.
 Normally circumstances aren't extraordinary.
 That's what we mean by extraordinary after all.
 So in that case, you're going to say, yeah, I've got 500 gigabytes, but at the moment I'm only using 250 gigabytes.
 Well, that's too bad.
 I spent a bunch of money for 250 gigabytes of memory I'm not using.
 The other choice is to say, okay, I got 500 gigabytes of memory.
 All these processes keep asking me for reservations for more and more and more.
 I will give reservations up to 700 gigabytes.
 Now I don't have 700 gigabytes.
 So if everybody comes back and says, give me my memory, I won't be able to meet all of those demands.
 But under most circumstances, they're not going to do that.
 So I'll probably be fine.
 I will be using 400 or 500 gigabytes.
 I've got reservations for 700.
 But nobody ever gets into trouble because they don't get their reservations because not everybody asks for everything.
 So you're going to either have to somehow or other make this decision.
 Am I going to underutilize the resource?
 I got a lot of memory, but I've promised people they have it available so I can't use that memory.
 Or alternately, am I going to refuse to give people what they need?
 Now, if we looked at this from a realistic point of view, in most circumstances, processes and threads don't need as much as they sometimes need.
 Their normal behavior requires less of the resource than their abnormal behavior, abnormal but correct behavior.
 So can you overbook resources?
 Now, this isn't just a problem in computer science.
 Some of you who travel may have run into this problem yourselves.
 You have reserved a seat on an airplane, and when you get to the airport, you're told, I'm sorry, but there's no seat available for you.
 What happened there?
 Well, it is very common for airlines to overbook on a flight where they have 100 seats on the airplane.
 They may have booked seats for 105 passengers.
 Now, they only have 100 seats, so they can't fly with more than 100 passengers, but they sell the seats for 105.
 Now what they expect to have happen is somewhere along the line some number of passengers, at least five, will cancel their reservations, not show up in time for the flight or whatever it may be.
 In which case there's a hundred people on the plane, they've made a bunch of money, they haven't made anyone unhappy because everybody who showed up for that flight got a seat.
 They're doing fine.
 Now the The other choice they have is to say, "Well, I've got 100 seats on the plane.
 I'm never going to book more than 100 reservations on this flight."  If they do that and five people cancel, then they only have 95 seats filled on the plane.
 Perhaps the five people who cancel don't end up paying either, in which case they've lost money.
 This is something that airlines have worked on and have had a lot of experience doing, and they typically do some degree of overbooking on flights.
 So what we want to do is say, not in just the context of airlines, but in the context of computer systems, is to say, if we're doing these kind of commodity reservations, we want to give reservations out to the point at which we are safe.
 What's the safe point?
 The safe point for the purpose of this lecture, at least, is to say no deadlocks.
 No deadlocks because you've overbooked.
 If you can finish everybody, everybody unwinds, everybody eventually completes, everybody eventually gets what they need in terms of the reserved resource.
 Maybe they have to wait some, but they eventually get what they need.
 There is no deadlock where everybody freezes.
 We call that a safe allocation.
 So that may result in some parties being blocked for a while because their reservation cannot currently be honored.
 It will be some point in the future.
 They'll be unblocked when it can be honored.
 They will get whatever they reserved at that moment.
 But they're blocked for a bit.
 But as long as they're not blocked permanently, and other people in the system also aren't blocked permanently, there's no deadlock.
 And we are happy.
 So we typically do this.
 And this is actually used quite commonly in certain places.
 Now, I've spoken a lot about memory reservations.
 But one place, more modern example, where this is very widely used is in cloud computing.
 If you've done any work with cloud computing facilities, things like Amazon Web Services or Azure, the Microsoft system, you will have made a contract.
 You will set up a contract with the web service and the contract will say, I'm going to pay you this much money and in return, you're going to guarantee me this amount of cycles, this amount of RAM, this amount of disk space, this amount of capacity to communicate to the internet, this amount of capacity, network capacity to communicate to other nodes that I have rented. so they can communicate among each other.
 And that essentially is a reservation.
 So cloud computing facilities make very, very extensive use of these reservations.
 When we talk about cloud computing, we will discuss that it's very important for the cloud computing provider to make the most efficient use of their hardware resources that they can.
 They don't want to have a bunch of stuff reserved that they cannot use for anybody else that is making them no extra money when if they could just allow those resources to be used by somebody else, when the guy who they reserve them for isn't using them, they'd make more money.
 Cloud computing is all about making more money.
 So at any rate, that's a situation where you have these quality of service contracts, where you have to think about, how much am I allowing to be reserved of each of my commodities?
 Will I get into a deadlock situation as a result?
 So, this does mean that a reservation must eventually be honored.
 Sometimes in cloud computing, it isn't honored.
 Sometimes you are given in your contract something that states, if we cannot give you, for example, the number of cycles that we've guaranteed because we over-reserved, then we will lose some money.
 You will not have to pay us as much.
 We will give you back some money for your unmet reservation.
 That's also bad from the point of view of the cloud provider.
 They don't like to give back money.
 So generally speaking, you try to ensure that reservations can be met if you're doing this sort of thing.
 From the client point of view, though, what this means, among other things, is that when you go to the cloud provider, for example, and say, I want a new contract.
 I want this.
 They may say, we can't give you that.
 So you ask for it.
 You can't get it.
 What are you going to do then?
 Well, you know that if you just go away, you're not going to get any services at all.
 You know, if you ask for less, a smaller reservation, and they say, I can give you that, you're free from deadlock.
 But you are going to have to deal with the fact that you wanted x amount of something, and you couldn't get x amount.
 You could only get x over 2, or whatever it may be.
 So if you're doing this from the point of view of a program, a program is asking for one of these commodity resources, you have to be prepared for what happens when the system says no.
 What do you do?
 All right.
 Now it isn't good of course to say no when somebody asks for something and the system would like to meet that but can't.
 But it's better than saying yes and not actually giving him the guy what he needs when he needs it or at least never being able to give the guy because there's deadlock.
 That would be very bad.
 So generally speaking you hope that as the provider of these commodity services that But if you were rejecting a request, that the party, the process, or the system that is asking for that amount of the resource is going to be able to adjust what it needs to do.
 It can try later.
 It can try to work with a smaller amount of the resource.
 It can do something that doesn't require it to work on that particular problem with that particular amount of the resource.
 Okay, now that was all about the commodity resources. there are general resources.
 These are things where you have critical sections, typically.
 You require mutual exclusion.
 You have locks.
 You don't have an explicit lock with a commodity resource.
 You have a promise.
 Here, though, we're now talking about explicit locks.
 I request a lock.
 I get the lock.
 So what we're going to try to do here is say, well, we can't ensure that there is no possibility that any request for any lock causes a deadlock.
 But let's see if we can try to say that when somebody requests this lock, that requesting that lock will not cause a deadlock.
 How are we gonna do that?
 Well, we can try to attack for every single request that comes in for a lock.
 We can try to make sure that if we grant that request, one of those four conditions is not going to be true.
 Remember the four conditions for deadlock.
 Because if one of the four isn't true, there's no deadlock.
 Remembering these are the four conditions, Mutual exclusion, incremental allocation, no preemption, circular weighting.
 Let's talk about how we could, in practice, practically speaking, attack these conditions.
 OK, so deadlock requires mutual exclusion.
 P1 has the resource and the lock on the resource.
 P2 can't get the lock on the resource.
 That's required for any possibility of deadlock.
 So if you have a shareable resource, something that you can only access using atomic instructions, then perhaps that'll work.
 Maybe depending on how you do things, having something that has reader/writer locks can be made to work, maybe.
 But you're going to have to do something about that.
 And if you have a private resource, a resource that belongs to you that you don't require a lock for, well, no deadlock there.
 When you give every party, every process, every thread, every machine, its own copy of the resource that it can use internally without communicating to anybody else.
 If you can, no deadlock.
 Usually that's not going to work out too well.
 You're going to have to have mutual exclusion.
 Okay, so we got mutual exclusion, but fortunately there are three other conditions that we can work with.
 We need all four conditions for deadlock, or we don't want deadlock, so we'd like to prevent all four conditions. prevent mutual exclusion in many cases.
 What about one of the other three?
 Let's look at them.
 Incremental allocation.
 You can't have a deadlock unless you block waiting for a lock while you're already holding other locks.
 Because if you're not holding any locks and you block, well, nobody is waiting for you to release a lock.
 So with luck, if that's true for everybody, somebody is going to release the lock they hold, you hope, and eventually you will not be blocked anymore.
 Okay, so that's great.
 So how could we do that?
 Well, one thing you could do is you could say, "If I ask for all of the locks I'm ever going to need at one moment in a single operation, then I could say, 'All right, either one of two things happens.
 I get every single lock I need.
 I needed 30 locks.
 I asked for 30 locks.
 I got all 30 locks, or I needed 30 locks, I asked for 30 locks, I wasn't able to get all 30 locks, I have no locks, then you're not going to be holding any locks that will prevent anyone else from getting a lock.
 And if you do things in this way, you request an all or nothing set of locks.
 Now, you don't have to do this just once at the beginning of your program.
 What you can do is say, I'm going to request a bunch of locks.
 I'm going to get all my locks, Fine, I've got all my locks.
 I'm going to do the work with the critical sections associated with all of these locks.
 I'm going to release all my locks.
 At that point, having released all your locks, holding no further locks sometime in the middle of your program, you can repeat, and you can say, "Well, now I need another set of locks, and I will ask for all of those locks, and I will, I hope, get all of those locks.
 I'll do all my work with all of those locked critical sections, then I'll release all of those locks."  As long as you never request a lock while you're already holding a lock, which usually implies you ask for a bunch of locks at once, can't be a party in deadlock.
 Another thing you can do is you can say, "Well, the problem with deadlock is I hold a lock, I request another lock, I can't get the other lock, I'm blocked.
 What if I hold a lock, I request another lock, I can't get the lock, but I'm not blocked?"  Well, if that's the case, then depending on exactly what I do, after I've failed to get that second lock, maybe eventually I'll release the first lock.
 And if I eventually release the first lock, well then, fine, nobody else will be waiting forever for that lock to be released.
 It's not gonna work so well if I just do a bunch of spinning, you know.
 There you can get into situations where effectively you do deadlock.
 But that would mean that As long as I sooner or later release the lock I hold, even though I can't get other locks I'd like to get, then you'll be okay.
 Another thing you can do is you can say, "Fine, I would like to hold a bunch of locks, but I cannot do anything that blocks me while I'm holding those locks."  Now, this is important because we said, "Oh, you're blocked because you're holding a lock."  Well, that could be true, where you're trying to get a lock that you can't get.
 But there might be other reasons you're blocked.
 If you were blocked for some other reason, not because you're trying to get another lock, but you're holding a bunch of locks, if that other thing doesn't happen, then you're not ever going to get back to release the locks you're holding.
 And under some circumstances, this could lead to deadlock.
 So we don't want to do that either.
 We don't want to block while you're holding locks.
 So, it could be that what you do is you say, "Okay, if I'm going to have to block for some reason or if I think I'm going to block for some reason, first thing I'm going to do is release all my locks because I don't want to hold locks while I'm blocking.
 That might cause other people to enter into a condition that it ultimately will lead to us all getting deadlocked.
 So, I release the locks before I block."  Well, probably the reason I was holding those locks, So the reason I didn't say, "Oh, I'm finished.
 Now I can free my locks," is I was planning to do something with them.
 It may be that I've done part of it.
 It may be that I haven't done any of it yet, but I'm going to in the near future.
 Well, in either case, once I am unblocked from whatever other thing it was that caused me to block, then I'm going to need that set of locks again.
 So I'm going to have to ask for them again.
 But if I ask for all of those locks that I need again in one of these all or trans requests.
 So I asked for all 17 locks that I need.
 I had the 17 locks, I released them, I blocked for another reason, I unblocked from that reason.
 I get my 17 locks again in one operation.
 That won't cause deadlock.
 Now, this doesn't sound so good.
 This sounds like it's gonna be a big pain in the ass.
 This sounds like it might lead to a lot of problems of its own, like I never again, after having released the 17 locks can ever get those 17 locks again, because other people keep using one or two or three of them.
 Well, yeah, that's true.
 But it's not a deadlock.
 All we're talking about today in this class, for this moment at least, is how do you deal with deadlocks.
 Fine, we've prevented deadlocks.
 You had another problem?
 Well, that's too bad.
 But we prevented deadlocks.
 OK.
 But let's say, for the moment, that you have to have mutual exclusion, And you don't like this idea of incremental allocation at all.
 What could you do?
 Two more conditions.
 No preemption.
 That's the third condition.
 So in this case, what essentially we're going to do is say, under some circumstances, some party who holds a lock is going to have that lock taken away from them, even though they do not intend to release it.
 We are going to confiscate that resource.
 We're going to take away the lock on that resource.
 Why?
 So we can give the lock to somebody else.
 How can we do this?
 Well, if you understand at the point at which you're setting up your code that what you're getting is not a permanent lock, but what we call a lease instead, it's maybe acceptable.
 What's a lease?
 A lease is a lock, but it's only for a fixed period of time.
 I have exclusive use of this resource for the next three seconds.
 After three seconds are up, whether I release or not, I no longer have exclusive use of resource and I know it because that's the way this works.
 I know I asked for a three-second lease.
 Now if that's the case, then we won't get deadlock.
 There are other issues which we'll get to when we talk about leases in more detail.
 Now one thing that's absolutely necessary if we're going to do any kind of preemption is whoever does the preemption is going to have to be able to enforce its decision.
 It's going to have to be able to say, "Hey, Process A, you had this lock.
 I'm taking the lock away from you so I can give it to process B.
 You have to enforce the process A really understands that it no longer has the lock and cannot behave as if it did.
 Because if you do, then you're going to run into other problems like you're not providing actual mutual exclusion.
 So, generally speaking, if we're going to talk about preemption, it's going to have to be in circumstances where there's some party, the operating system or some other privileged piece of software, that controls the locked resource.
 And if we have taken away the lock from somebody for this locked resource, that privileged code, the operating system or whatever other piece of code it might be, can prevent the guy who thought he had the lock from using the resource.
 We've taken the lock away from him.
 We prevent him from using the resource.
 Not a question of does he want to, is he behaving right, is he being a good guy.
 If we take the lock, he can't use that resource.
 There are circumstances where we can make that happen.
 There are other circumstances where we can't.
 There's also the case that frequently, the reason that you're holding a lock on a resource is you're going to do a multi-step operation that requires several things to be done, possibly all to one data structure, possibly to multiple related data structures.
 Multiple things have to be done before you release the lock.
 You want an atomic set of operations.
 OK, we saw a bunch of those examples a few lectures back.
 All right, so fine.
 You want a bunch of operations.
 What happens if we're going to do preemption of locks?
 If I've acquired a lock for one of these resources, I do the first thing I'm supposed to do.
 I do the second thing I'm supposed to do.
 Somebody takes away my lock.
 And I didn't get to do the third and fourth things I was supposed to do before they took away my lock.
 Now, if I've already done the first two, I may have made changes to data reflected by those first two things that I did. and I didn't do the other things I should have done, this may leave data structures in an inconsistent state, which would be unfortunate.
 Now, if the party that controls this resource can say, "Okay, you held the lock, "you started doing things to my resource, "I made some changes to my resource, to my data structures, "then I took away the lock from you."  If that party can say, "Gee, it's not enough for me "to just take away the lock from him.
 "I'm going to have to restore my resource situation it was in before I gave him the lock.
 So when he did operations one and two, it didn't do three and four.
 I will undo operations one and two.
 So it's as if he never held the lock.
 If I can do that, then I won't run into inconsistencies.
 If I can't do that, well, who knows what's going to happen?
 This particularly means that probably I'm going to have to have undo abilities, things where I can say, yes, you know, this field got changed, that field got changed, but I'm going to have to change them back whatever they were before, which means I have to know what they were before.
 So, when can we actually do this?
 Well, generally speaking, hold on for a moment, please.
 So, generally speaking, sorry for the delay, in order to use this kind of situation where we can say we can preempt the use of a resource, take away the lock for the resource, thus avoiding deadlock, we have to be able to control access to the resource.
 Typically, if you're talking about, let's say it's a thread and the resource in question is a data structure or a piece of code in the application, we're going to have trouble doing that because we won't have the ability to control access to the data structure in the shared memory of multiple threads or to the code and the shared code of the multiple threads.
 If on the other hand, what you're doing is you're sending a message to a remote machine saying I want to do this to your file, then there's some piece of code on that machine that is going to accept that message and either do things to the file or not do things to the file or take some other action like undo everything you've done to the file up to this point, then you would have the ability to break that lock associated with that file.
 So if you're going to take this approach to ensuring you don't run into deadlock, you have to make sure that this is the kind of resource, the kind of situation where you can take away access to the resource despite somebody having a lock to it and can ensure that they no longer are able to access that resource having lost the lock.
 Okay, now, while it sounded great to say, well, there are four conditions and any one of them is enough to prevent deadlock, when we start talking about the implications of those first three conditions, you know, some of those implications of avoiding those first three conditions aren't so hot.
 What about the fourth condition?
 There is still that fourth condition.
 The fourth condition says no circular dependency.
 All right, now you could do that, of course, by building a wait for graph, as we discussed when we talked about circular dependencies.
 And you build a graph, and when a request comes in and you look at it and you say, "Oh, gee, it looks like that would cause deadlock."  Well, then you reject that request.
 But of course, then you have to say, "What do I do when I reject the request?"  Is there anything else you can do that will ensure you never have a circular dependency?
 Yes, under some circumstances.
 Total resource ordering.
 You figure out ahead of time in the system that I'm working on, the complete system, everything that's part of the system, here are all the things that could be locked.
 Here's the complete set of locks.
 This is the set of locks.
 There are no other locks in this system.
 They're the only ones.
 OK.
 Then you assign each lock a number, an integer, from 1 to n if there are n different locks.
 OK.
 Then you tell the people who are going to work with your system and are going to request locks.
 You can request locks anytime you want.
 You can hold locks, you can request more locks.
 We're never going to take your locks away from you.
 However, there is a condition.
 If you hold lock X, you can ask for lock X+1, you can ask for lock X+2, you can ask for any lock that has a bigger number than X.
 No problem.
 You can't ask for x minus 1's lock, or x minus 2's lock, or one of the smaller locks than the highest numbered lock you hold.
 You've got to ask for locks in order.
 Okay, now, if you think about this, and if you say, if everybody, all the different processes, all the different threads, all follow these rules, they all agree that the locks are numbered 1 to n, they never ask for a lock smaller, with a number smaller than the one they already hold, You can see that we're not going to have any deadlock.
 Well, what might happen?
 So let's say I hold lock 50 and I want to get lock 51.
 This rule says I can ask for lock 51.
 Might somebody else hold lock 51?
 Yes.
 Somebody else might hold lock 51.
 Will they?
 And since they hold lock 51, I will block.
 I can't get lock 51 while they hold it.
 But will they ask for lock 50?
 They can't ask for lock 50.
 They already hold 51.
 And they can't have asked for lock 50 in the past, because if they asked for lock 50 in the past, they couldn't have gotten lock 51, because I hold lock 50.
 Therefore, if we follow this rule uniformly, always, without exception, then we will never have a deadlock situation.
 This is wonderful if you can do it.
 The problem that you run into, the reason this might not be easy to do, is we have to in the first place know what all of the lockable resources are.
 We have to have the complete list.
 If there is one lockable resource that we don't know about, then suddenly it's not following this rule that you can only get them in order because it isn't part of the order.
 That then runs into a situation where a deadlock sort of becomes possible.
 It also means that we have to be able to order them in a reasonable way. has to be some possible ordering of locks that makes sense.
 Further, it may require us, under some circumstances, to do some rather peculiar things in our applications that are performing the locking.
 So let's say that I hold lock 50, and I just have to get lock 49.
 I can't do anything without lock 49.
 Well, I can't get lock 49 while I hold lock 50.
 So what do I do?
 I do what's called a lock dance.
 I release lock 50.
 I then request lock 49, assuming I don't have any lock that's greater than 50.
 I can do that.
 I get lock 49.
 Now I can get lock 50, because 50 is bigger than 49.
 So I can ask for that.
 This is called a lock dance.
 Here's how it might work in a somewhat realistic circumstance.
 We got a buffer pool.
 The buffers are organized by a linked list.
 So we have a list header, and we have a bunch of buffers in the buffer pool.
 So let's say that because we are going to, in addition to using the individual buffers and getting locks on the individual buffers, we'll have locks on the individual buffers, but we'll also have a lock on the buffer pool itself.
 And if you're going to search or you're going to put a buffer in that isn't already there, or you're going to take a buffer out of the list, remove it from the linked list, then you're going to have to lock the whole list because that will, any of those operations would require you to deal with pointers in more than one buffer.
 Okay, so how do you do that?
 Well, you have a lock that represents the entire list.
 If you hold that lock, you can't do anything to anything in the list.
 But the individual buffers are usually what you're going to work with.
 You're going to put things in the buffer, you're going to change items in the buffer, you're going to read what's in the buffer, that kind of thing.
 So we'll have locks on the individual buffers.
 So what we're going to have to do is say, okay, to avoid deadlock, if we're going to do anything that involves working with the list as a whole, we have to make sure that the list head is going to have a smaller lock number than the locks on each of the buffers.
 So list head has a small number, each of the buffers has a bigger number.
 So how do we find a buffer if we're looking for a particular buffer in the buffer pool?
 We re-lock the list head, then we search because now that we have locked that list head we know nobody's going to be fiddling with our linked list because we have the lock.
 So we find it, we then say let's get a read write lock on the desire buffer because now we're going to use that buffer.
 And then we unlock the list head.
 Now we can do this because the list head has a smaller lock number than the buffer.
 And we return this is our buffer.
 How about getting rid of that buffer?
 We want to pull that buffer out because we're going to permanently use that memory for ourselves.
 So the buffer should leave the list.
 Don't want that to be part of the list.
 How do we delete it?
 Well we've locked it.
 We've locked it because we to the buffer.
 Okay, and we need now to pull it out of the linked list.
 To pull it out of the linked list, we're going to have to change some pointers in the linked list.
 In particular, a pointer that is not in this buffer itself, it's in the previous buffer.
 Well, we can't ask for the lock on the previous buffer because probably it's a smaller number.
 And moreover, in general, we are going to have to lock the entire list to make sure that bad things don't happen while we are fiddling with our our pointers.
 So what do we do?
 We lock the list.
 But we can't get the lock on the list.
 Because we hold the lock on this buffer, its number is greater than the lock for the entire list.
 So in order to achieve our goal, we unlock the buffer.
 So now we don't hold the lock on that buffer anymore.
 Then we right lock the list head, search for the desired buffer.
 Now we can lock that buffer again, because now we have no lock that is greater than the lock number of that one.
 OK, now there was a period there where we had made changes that were potentially important to the buffer.
 But we didn't hold the lock on the buffer, so somebody else may have fiddled with that buffer in the meantime.
 We can't guarantee that.
 That's going to make things a little bit difficult for us in actually making sure everything happens right.
 Okay, so that gets complicated.
 At the very least, you're doing all kinds of lock, unlock, lock, unlock stuff, which you otherwise wouldn't do, but you get to avoid deadlock.
 Okay, so you're going to build a big complicated system.
 Which approach should you use to deal with the deadlock issue?
 Well, you don't have to choose one.
 What you have to do is say for for every situation that could possibly arise, one of those four conditions isn't true.
 So you can do that separately for different resources, for different situations and so on, provided you've covered all your bases.
 No situation can arise where requesting any lock leads to all four conditions being true.
 So you make resources shareable whenever you can, use reservations for commodity resources, you have ordered locking or you never hold and block, you as a last resource say, you can't have an actual permanent lock on this, you get a lease and we're going to deal with lock breaking and make sure that we can do that effectively.
 Now, internally, of course, within the operating system, we are going to have locking internally within the operating system.
 It's not an application explicitly saying, I want to lock on this.
 It's the operating system saying there could be multiple things going on simultaneously or pseudo simultaneously in the operating system's own resources. and we got to make sure that we don't screw up.
 That's very, very important.
 It's very important you don't screw up, so you have to have the mutual exclusion on these operations.
 It's very important that you don't have deadlock because a deadlocked operating system is really, really bad.
 So if you're building the operating system, you're going to be exceptionally careful about avoiding deadlock.
 But the operating system is not necessarily going to say, "Hey, I'm going to make sure applications don't deadlock."  Not its job.
 The operating system offers some tools.
 You, as the designer of the application, have to make sure that your application, running in multiple threads, multiple processes, multiple machines, your application doesn't deadlock.
 Now, there's a one other deadlock solution, which is a very popular deadlock solution in the real world, which is, "Ignore the problem.
 Don't worry about it."  In many cases, you're not going to run into deadlocks.
 Clearly, if you're not doing any locking, you won't run into deadlocks.
 But even if you're doing locking, in many cases, maybe it's not that complicated.
 Maybe it's not too likely that you could possibly get into this situation.
 And it may well be that when you think about it a little bit, you say, "Well, okay, I want to be on the safe side.
 I don't want deadlocks, but what will I have to do?
 My gosh, how terrible it would be if I had to do that."  Think of all the extra code.
 Think of Think of all the extra complexities.
 Think of the overhead I would add.
 Think of the delays I would sometimes run into.
 Think of rejecting requests that I probably could safely perform.
 Maybe I don't want to do that.
 So frequently what people do, either because they don't know what they're doing or they do know what they're doing, but they say, "Better to take our chances."  They simply do nothing.
 Okay.
 Now, that's great because in many circumstances, indeed, nothing bad happens. don't run into the deadlock.
 What if you're wrong?
 Well, if you're wrong, a deadlock has occurred.
 Now what?
 Well, you can say I'll detect that a deadlock has occurred.
 How can I detect that a deadlock has occurred?
 Well, if I build a wait for graph, when my system isn't running well, I could go to my wait for graph and see, gee, have I got a deadlock by looking for cycles in the that the deadlock is related to, I'll do something about that.
 So is this a good idea?
 Well, it's not usually a very good idea.
 Something that has a great deal of control, that is able to run code despite the fact that a bunch of code is deadlocked, will have to be able to examine this graph.
 Further, it will have to have a lot of privilege.
 It will have to be able to break things that ordinarily you don't want broken.
 Like you're going to get rid of a lot of locks People may have done partial operations on whatever you've locked and so on and so forth.
 So, if it is the case as well that the operating, you could do this, might say, well, shouldn't the operating system allow this possibility?
 Well, then the operating system has to know about what the locks are.
 It has to know that locks are possible in this multi-threaded application.
 And as a rule, in a multi-threaded application, how are you building the locks?
 You're probably building them in the memory of the application. probably not actually when you're doing locking operations invoking the operating system at all.
 In which case, gee, how do I know as the operating system that there are locks there?
 On the other hand, you could say, well, I'll build a lock detection mechanism into the thread library.
 Well, that's fine provided you have one multi-threaded program.
 What if there are several separate processes?
 Each library is going to be for one of the processes.
 It's not going to to figure out where all your locks are.
 If you're talking about using somebody's code that is not your code, it's a running system that's going to be a component of what you do, you may have no clue about what locking they're doing at all.
 They may not have made that particularly visible.
 So, it's probably not feasible, but even if it is feasible, you might say, "Well, if I'm building this graph anyway, why did I allow a deadlock to occur?
 Why, given that I was going to build the graph, take all that cost of building the graph, when somebody asked for a lock that was going to cause a deadlock, why didn't I just say no?
 Okay, now in many cases, of course, as we've said, deadlocks are not the operating system's responsibility because it's not the operating system's locks that are being used.
 Database systems do a lot of locking internally, for example, and you can do this in a multi-threaded program as well.
 Or you can say, well, I've got 17 different machines that are communicating with each other and sometimes they do locks and sometimes they even lock things on other people's machines.
 You know, who is keeping track of all these locks?
 There isn't one operating system for the 17 machines.
 So it may be that this is not something that is feasible.
 Now in certain cases, like in a database, within a database all of the locking is internally within the database and they have a great deal of understanding, because that's how database code works, of exactly how to deal with locks.
 So you might do deadlock detection in that kind of system, but in many, it's not really feasible.
 Okay.
 It's also the case, of course, that not everything that goes wrong related to synchronization is a deadlock.
 Some things are other kinds of problems.
 So what are you going to do if your system hangs?
 Are you going to say, "Oh, it's a deadlock"?
 Well, it might be a deadlock, might be something else.
 Live lock is one example.
 One example of live lock, there are different kinds of live locks, is where you say, okay, I am going to be getting a message and what am I gonna do?
 Once I get the message, I'm going to act on the message and then I'm gonna do something.
 But sometimes I get a lot of messages, so then I don't really have time to act on each message as it comes in because the next message is coming before I've acted on the first one.
 What am I gonna do?
 I'm gonna queue up the next message.
 If this continues at a high enough rate, You're spending all of your time queuing up messages.
 You're never going to get around to actually dealing with any of the messages that you've queued.
 And that's going to look a lot like a deadlock, but it isn't really a deadlock at all.
 It could be that you've built your own locks, probably not a good idea.
 And you have a flaw in your code, not a deadlock.
 It could be a bug in your own code, not related to a deadlock at all.
 You could just have entered an infinite loop that had nothing to do with any kind of locking.
 So there are many possibilities of bad things that could happen.
 Further, even if you had a magic oracle that said deadlock, that system is deadlocked.
 If that's all it tells you, that doesn't help that much.
 You could say, OK, fine, I'll reboot everything.
 But you didn't need to know it was a deadlock in order to say reboot everything.
 You probably rebooted everything a few times in your career without knowing why it was that your system was hanging.
 So what we would really like, rather than saying, let's have some elaborate thing that's going to detect deadlocks and help us when a deadlock occurs, is something that's going to say, if there's problems with the way my system is running that are perhaps related to synchronization, I'm going to deal with-- I'm going to detect those problems.
 I'm going to get some advice on what I should do.
 So how do we deal with general synchronization bugs?
 As a rule, we do not build deadlock detection systems.
 They're complex to implement.
 They only detect deadlocks.
 They don't deal with any of your other problems.
 And it's not clear based on them.
 They say, OK, here's your deadlock.
 Here are the locks you're holding.
 These are the processes holding the locks.
 I can tell you that.
 But now what should you do about that?
 Not clear.
 So what you're probably better off doing is implementing something called health monitoring.
 What's that mean?
 Health monitoring means that you build your applications in such a way that you can keep track of whether things are going well in your application or not going well.
 And if they're not going well, you will get clues because of what you built into it that tell you what kind of problem is happening.
 Where is your difficulty?
 What could you do about it?
 OK.
 So how do we do that?
 Well, typically what this means is we either build code into our applications or we build other pieces of code that observe how our applications are behaving.
 And they provide us with information.
 So based on that information, we can then say, here, we were expecting that we were going to get output that would have the following characteristic, telling us that everything's fine.
 We're not getting that output.
 Either we're not seeing everything is fine, or we are getting output saying it looks like there's a problem.
 Now we can do something based on that.
 Now what's good about this is that unlike somewhat complex systems for deadlock detection, this doesn't have to be all that complicated.
 And it detects many, many, many different problems.
 It detects crash components.
 It detects being in an infinite loop.
 It detects various kinds of live locks, in addition to deadlocks.
 So for example, live locks.
 Now, there are many forms of live lock, as I've said.
 Here's another form of live lock.
 There's a process running.
 It holds a lock R1.
 And in order to make progress, in addition to holding lock R1, it's got to get a message.
 A message must be delivered.
 There is another process that's going send that message, but it will never send that message until it gets lock R1.
 Now there's no lock associated with receiving that message.
 There's no lock associated with sending that message.
 The only lock in this particular example is R1, but your system's hung.
 Well, that's not going to be detected by any kind of deadlock mechanism, and it's not going to be prevented by any kind of deadlock mechanism necessarily either.
 Another problem is that you can, we've seen an example of this problem in a previous class, is the sleeping beauty problem.
 You got a process or a thread that's sleeping and somebody's supposed to wake it up.
 Prince Charming is supposed to wake it up, but Prince Charming ain't coming.
 There is no Prince Charming in this example.
 The sleep wake up race we talked about before is an example of this.
 Somebody went to sleep.
 The expectation was that another thread or process was going to wake it from its sleep, but because of timing issues, the other process isn't going to do that.
 That first process that went to sleep will never be woken.
 Priority inversion hangs.
 They aren't deadlocks at all.
 They could be very, very unpleasant as they were in the Morris Pathfinder Rover case we talked about, but they aren't deadlock problems.
 That problem was not detected by deadlock detection.
 That was detected effectively by a health monitoring mechanism, the thing that was watching to see if the data bus that they were using was being cleared up, it said, "This isn't happening properly."  And it not only reported that I have to reboot, but it said, "Here's why I rebooted."  That helped them figure out what their problem was.
 None of these problems and there are others would be helped by a deadlock detection mechanism at all because none of them are deadlocks.
 The deadlock detection mechanism would not detect them.
 On the other hand, a good health monitoring system would detect all of them and detect the deadlock.
 So how do we do health monitoring?
 What are we actually doing if we're gonna do health monitoring?
 Well, first thing we're gonna do is say we've got several different processes running in this system and we know we have several different processes running in the system.
 It's designed that way.
 If one of them crashes, we know we have a problem.
 There are supposed to be 12 processes running, only 11 of them are running.
 That 12th process, we can figure out who it is and we can say, okay, it's exited at this point. and then we can take action based on that.
 Or if it's not actually crashed, it's just responsive, we can say, oh, look, this is a process which we do a status on this process on the system.
 Look, it is blocked, it's not running, it hasn't run for the past hour.
 Well, you know there's something bad there.
 Even if it is running, but it is not doing the work it should do, like it's not sending any messages and it's supposed to send messages, we can detect that.
 We can have more detailed external health monitoring.
 We can say this particular process is supposed to respond to things sent by other processes.
 Therefore, we will build into this process some testing code.
 Things that say, for example, here's a message we sent to this process.
 If the process is doing everything as it should be doing, it will instantly respond and say, "Yes, I got your message."  May not do anything else, it'll just say, "Yes, I got your message."  That kind of message is called a ping.
 And pinging a process and getting a response back can be very helpful in figuring out, is this process stuck in a tight infinite loop somewhere and it's not even looking at incoming messages, or is something else going on here?
 If we want a little more detail, we could build in the ability to say, okay, this is a process that receives database requests.
 Let's send a database request that says, I don't want any data.
 I just want an answer saying we have no data.
 Does it send back the answer saying we have no data?
 which means, of course, it's gone beyond the basic message receipt code into the code that says, I am able to consult the database.
 Or you can even say, I have standard test requests.
 There's a test record that I have sitting in my database.
 If I'm not happy with what's going on on this particular database process that's running there, I can send the test request saying, show me what's in the test record.
 And I know what's in the test record, because it's been set up to have that value.
 And I can see, does it come back with the correct value in the test record or does it not?
 And that will tell me something.
 I can also have more internal instrumentation.
 I can, for example, say I can send a command to this remote process.
 And this command will cause it to do all kinds of things, to figure out, for example, how big is this table that it was supposed to build?
 Are all of the values of these particular data items within the range that they should be within?
 There's an algorithm that this thing runs.
 I'm going to send it a standardized request to run the algorithm.
 I know what the result should be because it's a standardized request.
 Did it produce the correct result from running that algorithm?
 That would be called an exerciser.
 I'm going to watch, perhaps externally.
 This thing sends messages.
 It makes requests to various files and so on.
 I'm going to be able to watch what it does.
 I've set it up so I can watch what it does.
 I've built the code that way.
 Now I'm going to watch it and see if it's doing what it should be doing, monitoring.
 So if you have this kind of thing built into your system, you will, in many cases where you've run into problems, be able to detect this is a process that is not behaving well.
 OK, it's unhealthy.
 Did health monitoring.
 The health monitoring says he's sick.
 What do you do?
 Generally speaking, in most cases, we're going to have to restart that process, which means if it's still running, we're going to have to kill it, and then we're going to have to restart it.
 Now, of course, if your overall response health monitoring is the system is sick.
 There are 73 different processes and all it tells you is the system is sick.
 Well that's not as helpful as saying process 12 is behaving badly.
 Then you probably know you have to do something about process 12.
 If it's just the system is sick and you have no further details, then maybe you're going to have to restart the entire system.
 It would be nice if you could have a bit more detail about what's going on.
 Even if it doesn't point you at this is process 12 and it's the one that's a problem. it can tell you something like, this problem seems to be related to the graphical interface components of our system.
 OK, well, those are these processes.
 They're the ones that probably need to be fixed.
 What you want to do is be able to identify which processes are causing our problems when we have problems.
 Then you want to be able to stop those processes and restart them and have them interact properly with all of the other processes that you think we're doing just fine.
 Now, you do have to think about that in terms of, in the first place, those other processes.
 But of course, in many cases, the system that you're building is not just, you know, 73 processes talking to each other.
 It's 73 processes that are providing service to an arbitrary number of external clients.
 There are people out there who are saying, "Here is a big model that I've built.
 I would like you to do the following 3D manipulation, and I would like you to make predictions on what happens when I do that manipulation and send me back the results.
 Now in this case of course, you may have a lot of clients, you may have very few clients.
 Clients may be doing very different types of things.
 Some of the problems that you're seeing may affect some of the clients and not affect other clients.
 In these cases you want to know and you want your health monitoring system to help you figure out who is being harmed by the problems with the system, which of our clients are in trouble.
 Then you have to say, "Well, okay, fine.
 Clients one, two, and three are the ones who are in trouble.
 Clients four, five, and six, they're fine."  If possible, you want to do things that are going to fix stuff for one, two, and three without affecting four, five, and six.
 Ideally, what you'd like to say is, "Well, everything was going fine for clients one, two, and three up to this moment.
 So all the stuff they got before that, that was all fine, but something went wrong at this moment.
 So what I'd really like to be able to do is restart things such that they get into a clean where everything they've done up to that moment has already been done.
 So we don't have to redo all of that work.
 But we can take the output of the work that was done up to that moment and then feed it into the restarted components that caused a problem in the first place.
 OK.
 Now, when we have these complex apps that have many, many different processes, many, many different machines, many threads within the processes, we will need to figure out what relates to what else.
 If I touch this component, if I cause this guy to stop and restart.
 What other components are going to be instantly affected by that?
 What's gonna break downstream?
 So you need to understand that.
 You also need to be able to say, well, can I restart science one, two and three from the point at which everything was fine?
 Do I have enough data to say, as a result of everything being fine, here's what they had as a partial computation.
 Things went bad after that. but up to this point, their partial computation was fine.
 Can I restart from the partial computation?
 That would be a warm restart, where you are able to take some data that you've saved about what was happening before things went to hell and start from the point at which everything looked okay.
 A cold restart would be to say, I don't know what the hell happened or I don't have any confidence that there was ever a good moment in the system or I don't have any data about what was happening in that good moment. going to restart from the beginning.
 That would be a cold restart.
 A partial restart would be where you said I've got 75 machines that are running here.
 I'm going to have to restart these eight because they are, for example, the database machines.
 That's where the problem was.
 But I don't have to restart the other...
 Okay, sorry for the delay.
 The Zoom crashed on me.
 We're starting again with a discussion of how we do health monitoring.
 All right.
 Now generally speaking, if we have a very complex system, very complex problems can arise.
 And what are we going to do?
 Well, in these very complex systems, there are frequently many machines running many different processes.
 The more of them that we cause to reboot, or the more processes we start and restop, the more serious the implications are going to be for our users.
 They may lose a great deal of work.
 They may get a lot of delay.
 So what we'd like to have happen is let's make that delay as little as possible.
 Let's make the chances of lost work as limited as possible.
 So what do we do?
 Well, if we can do something that doesn't require an actual reboot, maybe we have the ability, for example, to send a process a signal saying reset to the following condition.
 We built the process that way.
 Maybe we can do that and we can try a few times.
 That doesn't work.
 Well, then we're gonna have to go to more complex issues.
 We can of course say, perhaps if this is just one client whose system call is not working, they're giving us a piece of work that isn't working.
 Maybe we can try to reset to before we started doing their work and tell them try it again.
 And sometimes that works.
 Sometimes we're going to say, OK, well, several of our machines simply aren't working.
 We're going to shut those machines off.
 That means perhaps we can continue operations, but at a reduced capacity.
 Or perhaps we can continue operations, but we can only do certain types of things.
 We can't do all of the functionality we ordinarily provide, because some of our systems aren't currently working.
 We can, of course, have automatic restarts.
 We can have things where we don't have to have a human being in the loop saying, change this, reset that, shut this down, restart that.
 Depending on how much work we put into this, we may be able to do warm restarts this way.
 And we may have partial restarts.
 We have a set of strategies.
 Generally speaking, having a set of strategies, a set of escalation mechanisms is a good idea.
 Try this first with minimal impact.
 If that doesn't solve your problem, try something else that has somewhat greater impact.
 If that doesn't solve the problem, keep trying.
 Restart more groups of related machines and related processes.
 Go to maybe necessary restart everything.
 But you don't want to go to that final step if you don't have to.
 OK, so moving on to another synchronization-related topic, pretty much the last synchronization topic we're going to be talking about until we distributed systems, how can we go about making synchronization easier?
 One thing you may have gathered from everything we've discussed about synchronization up to this point, this is kind of complicated.
 That means that for the programmers in particular, and probably most of you guys are going to end up being programmers, so this is directly relates to you.
 It can be quite complicated.
 How do I figure out where my critical sections are?
 How do I figure out where I have data structures or code that must have proper synchronization.
 How do I make sure that whenever I need the synchronization, it's provided, but on the other hand, I don't do too much synchronization, which is likely to have poor performance, as we've discussed in a couple of lectures.
 So generally speaking, if you're using mutexes or locks or semaphores or any of those things, it can be kind of hard to do it right.
 If you are very, very conservative, your performance may be bad.
 But if you aren't sufficiently careful, you may run into synchronization errors.
 So how could we make things easier?
 How could we make life easier for programmers with regard to synchronization?
 So generally speaking, here's an approach that people have taken.
 Identify what is a shared resource.
 In many cases, especially if we're talking about object-oriented programs, but not necessarily just object-oriented programs, but things where there are objects, like a file is an object, You figure out, gee, are the things we do with this object likely to cause synchronization problems?
 Do we have to serialize them?
 Then we write code that operates to serialize them.
 And then we assume that all critical sections related to that code will be serialized.
 If we do this properly, maybe the compiler can actually generate the serialization for us, particularly if we're working in a language specifically designed to do that kind of thing.
 So then, we will simply do something in the language that says, I'd like to lock this object, I'd like to lock this method, I'd like to lock this piece of code, and all the locking, all the actual code that does the locking, could be built in by the compiler.
 The mere fact that we've asked for it to be done means it gets done correctly.
 That'd be great.
 Not just the acquisition of the lock, but proper release of the lock at the moment when it should be released.
 So this would be very, very nice because, of course, the code then that we can use for that purpose can be something that is built into the compiler.
 It can be pretty much guaranteed to be correct, to be of pretty high performance.
 So how do we do that?
 Well, there are various ways of doing that.
 One way is called using a monitor.
 Monitors are involved with object-oriented programming.
 So in object-oriented programming, of course, we have classes of objects, different types of objects.
 You can have instances of a particular class, So you can have a class that represents some type of thing, and you can have 12 instances of that class.
 So what we can do if we have protected classes is we can say, we will simply have some way of saying, this is a monitor class.
 And associated with this monitor class, there's going to be a mutex.
 And every time that an object is created, it will have its very own mutex of the appropriate type.
 And any time that you perform a method on that object belonging to that class, it will acquire automatically, simply by asking to do the method, it will automatically acquire the mutex for that particular object of that particular class.
 And when you exit the method, it will release it automatically without your doing anything else.
 You'll just have said, I want this protected.
 Now, this approach of using monitors in an object-oriented language has a very good encapsulation.
 You can say, here is what is going to be protected.
 And it starts here and it goes to there.
 That's what defines this object.
 And you don't have to be concerned at the client level with objects.
 You don't even really have to know that it is happening.
 You're just the user of a particular object of a particular class.
 And the mere fact that you're using that object without even your doing anything to request it ensures that you will get proper synchronization.
 The protection's automatic.
 So here, for example, is how you might do it.
 So let's say we have a class checkbook.
 We're going to say it's a monitor.
 And maybe in your object-oriented language, putting the keyword monitor on a class indicates that I'm going to be protected by a monitor type of synchronization.
 What this means is that for all of the objects, all of the methods associated with this type of object, for things of this object class, every single one of them going to be locked.
 And there's going to be one lock for all the methods.
 So even if it's one object of this class and there are multiple different threads of control calling different methods for that object, there will be one mutex for that particular object.
 So for example, here we have balance as a method and we have debit as a method.
 So if you call balance, then you will get the mutex.
 And you will hold the mutex until the balance method is completed.
 If you call debit, then you will acquire the mutex and you will hold the mutex until the debit method is completed.
 If somebody is running the debit method and you try to run the balance, you are going to block waiting for the mutex to be released by whoever is running the debit method.
 You don't have to do anything to make that happen.
 Merely the fact that you call balance on this monitor type of object is going to cause that to happen.
 So what's nice about monitors from the point of view of programmer use of monitors is they're very simple.
 There's no real complexity involved here at the programmer level.
 On the other hand, they're very conservative.
 They basically say for any object that is of a type monitor, any method that gets performed is going to have synchronization applied.
 Does it need to have synchronization applied?
 Well, maybe it didn't.
 But because it is a monitor object, it will be applied.
 And moreover, it will be applied for every single line of every single method.
 It may be that there are only a few lines of one method that actually require this degree of synchronization.
 But because you use the monitor construction in this object-oriented language, every line of every method of that particular class of object for every one of the objects is going to get properly synchronized.
 And of course, that can cause performance problems, because now a lot of stuff gets locked, and other parties can't use it while you have it locked.
 You're getting rid of some parallelism.
 And if you have a widely used particular object that many, many, many threads of control are using, and every single method that is used on that object has the lock acquired and held for as long as the method runs, you could very easily get a convoy on that object, which is bad.
 As always, there ain't no such thing as a free lunch.
 If you wanted to do fine grain locking instead of this, you would have to do a lot more work.
 There'd be a lot more involved in the program and you'd have to think a lot more about, must I have this particular method or these particular lines of code in this particular method, acquire the lock, or do I not need them for that particular code or that particular line?
 And that's a lot more effort.
 But on the other hand, if you do it right, you're going to be holding locks for a shorter period of time, which is going to have better performance implications.
 Holding for a long period of time with coarse-grained locking could very easily create bottlenecks and even convoys.
 Okay.
 Now, another choice that you have in Java, not necessarily in all object-oriented languages, is called the synchronized method.
 As the name implies, we are no longer going to synchronize entire objects, but we're going to synchronize particular methods of particular objects.
 So there's only one mutex associated with each object when you're doing Java synchronized methods.
 But not all of the methods of that object need to acquire the mutex in order to be properly run.
 It's also the case that if you have nested calls in a complex object, you may have a method calling another method, then you're not going to have to reacquire the mutex every time you call another method.
 If you have it, you have it.
 And when you get back to the final return, the return from the method that originally acquired the mutex, then it will automatically get released.
 You don't have to think about when you're using the various different methods, have I got to the point where I need to release this mutex?
 It happens automatically.
 You don't even have to say I'm acquiring the mutex.
 Just as with monitors, the mere fact that you are calling the method acquires the mutex for you.
 The mere fact that you're finished with the methods that the mutex releases the mutex for you.
 You don't have to write code to do that.
 Don't have to call special code that does that.
 And unlike monitors, some of the methods in a particular object that require synchronization, if some of those methods do not require synchronization, they don't have to be synchronized.
 They don't have to acquire the mutex.
 OK, now you actually can also have static synchronized mechanisms.
 This will lock the entire class for those methods, which is fairly restrictive.
 If you have 75 different objects of a particular class, and you're going to have a static synchronized method, if any one of those 75 objects calls that method, it's going to get the mutexes for all of them.
 OK, now leaving that particular issue aside, the static synchronized methods, the advantages you get of having synchronized methods over having a monitor is a finer granularity of locking.
 You don't lock as much, you don't lock for as long.
 This reduces the chance of deadlocks, it reduces the chance that you're going to have long blocking periods when you're waiting for a lock.
 Generally, you'll get better performance.
 The cost, of course, is that somebody wrote this object and they had to decide these methods need to be synchronized and these other methods don't need to be synchronized.
 If they get it right, and if they are not too conservative about what they synchronize, then you have these performance benefits.
 If they get it wrong, then you appear to have performance benefits, but you may have improper synchronization, which is very bad.
 OK, so here's how it might look.
 So in this case, we can say, fine, we have the checkbook.
 This time the object is not going to be a monitor.
 You notice there's no monitor on this class.
 But it is otherwise a similar type of thing.
 It's got a balance method, and it's got a debit method.
 The balance method is just a method.
 Nothing special about that one.
 Running the balance method does not require you to obtain the mutex.
 Running the debit method, however, is synchronized.
 As we can see on debit, it says public synchronized int debit.
 The synchronized keyword on that particular method indicates it's a synchronized method.
 This means that if you are running the debit method, then you are going to have to hold the mutex to run that method.
 Now, of course, you don't have to have the mutex to run the balance method, which could mean, among other things, that somebody else is running debit and you're running balance at the same time.
 That could mean that you're going to get an unlikely, unexpected result from the balance method.
 However, depending on how code is written, what may happen is you may simply get the balance from before the debit, the balance after the debit.
 It's one of the two, in which case maybe it's OK.
 OK, now, this entire lecture has been about dealing with a particular problem, deadlock of synchronization, and the general problems of how do we practically build code that does not run into trouble with synchronization, doesn't run into trouble with improper synchronization, doesn't run into trouble with poor performance due to synchronization.
 The reason this all comes to light is, as we said several lectures ago, we cannot get high speed out of modern computers without exploiting parallelism.
 We've reached the limit of what we can do with the speed of a single CPU core running as fast as it can.
 If we want to go faster, got to have things running in parallel.
 Parallel highly increases the chances of serious errors, non-deterministic errors which are very unpleasant.
 Deadlock is one such example of an error that can occur if we're gonna have parallel activities.
 And this means that if you were gonna work with parallel code, which if you're doing important computations, the interesting kind of programming that probably most of you would like to do in your future careers, you better understand synchronization reasonably well.
 If you don't, you're going to either not synchronize properly and see weird results from your programs, unpredictable, non-deterministic results, or you are going to synchronize sufficiently, but perhaps too conservatively, and you're going to lose a lot of performance, in which case you're going to have non-competitive code.
 Other people will write code that runs faster than yours, and people want to run that instead of your code.
 So you have to, if you want to be a successful programmer in this environment in the future, You have to understand synchronization.
 Where are the problems?
 What are the solutions?
 What should I be thinking about?