In our first lecture, we discussed the importance of operating systems and the kinds of things one expects the operating system to do for one, to provide services that are going to make it easier to work with a computer, to have your programs operate properly on the computer, to have things that are happening on the computer happen in the right order. Now we're going to start talking about how the operating system can go about doing such things. talking today about the general concept of abstractions in operating systems and services offered by operating systems and how it can offer them. So what we're going to talk about in a little bit more detail in this lecture is, first of all, what do we mean by an abstraction at the operating system level? We'll talk about some important types of operating system abstractions that we see in pretty much all operating systems that are critical to being able to make proper use of computers. Then we'll talk about services, which are a bit different than abstractions. We'll talk about what we mean by an operating system service. And then we'll talk about how can an operating system go about providing these services. We'll begin with abstractions. Now one thing that we've discussed already a little bit is that we have these very, very simple, powerful, but not very complicated resources at the hardware level. And we want to offer more powerful, more complicated resources to our applications, our application developers, our application users. So the way we do that typically is we provide these parties-- the developers, the users, the testers, et cetera-- with abstractions, things that aren't actually natively built into the hardware, but that can be derived from the hardware by adding software support typically. So what you work with when you're working with an operating system, when you are, for example, developing a program and you make system calls, what you're doing is you are accessing the abstract services that the operating system provides. You are typically not working directly with physical resources. The abstractions make use of the physical resources. But the operating system mediates what you're trying to do at this higher level with what's happening way down at the physical level. And of course, nothing is magic here. Everything is being done with bits, bits stored in RAM, bits that are used in instructions in the CPU, bits that move across various buses to get to different devices. Everything works with bits in computers. So there's nothing magic here. So in order to provide any kind of abstraction, you're going to have to use real things. The operating system will have to make use of real hardware in order to provide the abstractions. And ultimately, at the bottom, it's going to have to be actual hardware. There's going to have to be an actual piece of hardware, RAM locations, a CPU core, a bus, a storage device, a network card, or whatever it may be that you're trying to provide a service for. Ultimately, the hardware will have to be used to do that. For example, one very important operating system abstraction is a process. There's no such thing as a process in hardware. The hardware does not do processes. It just does instructions. So how do you get processes? Well, you build on top of the instructions that are offered by the CPU and on top of the RAM that is also available to the operating system and perhaps other resources as well, depending on how you want to implement it. And you produce an abstraction, the process abstraction, that uses these underlying physical resources. So files are another example. These storage devices, as I already mentioned in the first lecture, things like flash drives, they do not store files directly. They don't know anything about files. You go to a flash drive and you look at what can this flash drive do. And the answer is it can store a block of data, some amount, perhaps 4K or 16K. It can retrieve that for you. Maybe it can erase that block of data. That's about all it can do. That's not a file. A block of data, a 4K block of data is not a file. If we want to have a file, we're going to have to do something to produce a file based on the fact that really all we can do is read or write these blocks of data on the storage device. That will be the operating system's job. The file is an abstraction. The actual physical storage device, the hard disk drive, the flash drive, whatever it may be, is the physical reality. There is a gap between the physical reality and what you want applications and users to be able to see. That gap is filled by the operating system. Now, the reason we're doing this is because the abstractions are a lot easier to work with, a lot more understandable, simpler to work with, easier for a programmer to understand what to do. So you're writing a program, and you say, I've got a record here that I just created, and I want to permanently store that record. The record is 100 bytes long. I don't want to say, OK, I'm going to put these 100 bytes somewhere and put it into the following 4K block on the disk. What you want to do is say, oh, I want to have a new file that will hold these records. I've got the first of them. It's 100 bytes. Write 100 bytes into that file. And not worry about the fact that what ultimately really has to happen down there at the low level is sooner or later you're going to have to write a 4K block, because that's all you can do. That's the only thing you can write on that disk. You cannot write 100 bytes. You can write a 4K block. Now, 4K block could contain your 100 bytes, but you're going to have to be careful about how you do this kind of thing in order to make efficient, fast use of your storage device. And you certainly don't want the users, or even the programmers, to be aware of the complexities of what's going on here. So one thing we're going to do with these abstractions is they're just going to be easier to use. You're not going to have to worry about things like, well, today I got a device that's 4K blocks. Tomorrow I have a device that's 8K blocks. The next day I have a device that's 2K blocks all on different computers. You don't want to have to reprogram in order to work on that. So you don't want to worry also about the fact that, as we will see when we go through many, many issues in operating systems, there are varying speeds of hardware. CPU is very fast. RAM is pretty fast. Hard disk drives and flash drives are not very fast. And other things are even slower. So you don't want to run at slow speeds. We're always interested in performance, high performance in our computers. To get high performance when you have a device that's not very fast, you've got to make sure you're not running at the speed of the slow device. You want to run at the speed of the fast device, the CPU for example. So you're going to have to play some tricks in order to avoid dealing with the slowness. One of those tricks for disk drives for example is to say, when I say I'm going to write that 4K block, for whatever reason I've chosen to write a 4K block, it can't happen instantly. There can be a lot of time, there will be many instruction cycles before the 4K block can be written. I can tell the disc gets started on this and sooner or later it'll be done. I don't want to sit around just running in a tight loop saying, "Are you done? Are you done? Are you done? Are you done?" until actually the disc is done with that. You could be doing a lot of other stuff during that time. So what we do instead is we say, "Well, all right. remember that we wanted this written and we'll expect the disk to tell us when it's done. It'll generate what's called an interrupt. Well, you don't want users and you don't even want programmers worrying about interrupts. Those are very complicated. They're asynchronous. You don't know when they're going to happen. It can all get quite complicated and you don't want that complication to be clear to the user. So let's hide the interrupts. Let's put those away in an abstraction. Also there's going to be complexities in our computers. One thing that you may or may not be aware of is if you work on a laptop computer like the one I'm recording on right now, there's not one process running, there aren't two processes running, there aren't ten processes running. Probably on most computers there are a couple hundred, three hundred processes running on your computer simultaneously. Now on modern computers we have multi-core processors maybe eight cores, which means eight things can happen at once. But there are 300 things that want to happen at once. So this is going to get real complex. Somebody is going to have to decide out of the 300 things that we could be running, we can run eight of them. Which eight shall we run? And when shall we switch from one to another? How long will we let something run? When do we decide they shouldn't run anymore? There are all kinds of issues of complexity there. We don't want users is worrying about that. Certainly, we don't want programmers worrying about that. We also want to hide behavior that's irrelevant to the user. Now I mentioned flash drives. Flash drives are probably the most common form of persistent storage, by which we mean when you put something into the storage, you turn off the power, you bring the power back on, hey, it's still there. The bits are still where you left them. That's persistent storage. Flash drives are one form of persistent storage. There are others. But flash drives, while they have many good characteristics, also have some inconvenient characteristics. Flash drives, the hardware, at the hardware level, a particular block on a flash drive is right once. You can write it, but then you can't change it. You can read it as often as you like, but you can't change it. The only way you can change it is to erase it. Okay, so you can erase it, and then you can write something new in there. But erasing doesn't deal with a block. It deals with a sector, a large collection of blocks. So you're going to erase a whole sector. And moreover, it's slow. It's much, much slower than reading or writing a block. So if you do something on the flash drive that's going to require you to erase a sector, it's going to take a lot longer. Well, we don't want our programmers, and certainly not our users, worried about that. Let's hide that. Let's try to hide that behind an abstraction. And of course, there are other issues like making everything look convenient. So of course, most of our computers nowadays have one or more network interfaces, ways we can communicate with devices off of our computer. We could be using wireless, we could be using wired, we could be using Bluetooth. There are other possibilities as well. We've got 300 processes running on our computer. Now not all of them want to do any communication off the machine, but probably some of them do. You may be running a web browser. You may be running an email program. You may be running a network game. You may be updating software from a remote site. There's all kinds of things that could require you to communicate off the network. And that's going to require you to use the network interface. So you're going to have dozens of programs that are running simultaneously on your computer and they all want to use the network interface and there's one. And the way network interfaces work is at any given moment, they can be sending one message or receiving one message. Maybe sending and receiving depending on the architecture, but certainly it's one at a time. And you got dozens of people who want to do it. So you don't want people worried when they're writing their programs or even worse when a user is running this program to say, "Oh gee, I can't send this message now because somebody else is using the network card." You want the program that wants to use the network, your game for example, to have the illusion that it has exclusive use of the network. It wants to send a message, a message gets sent. A message should be received, the message comes in and it gets delivered. And you never worry about the fact that there are other processes that are simultaneously doing the same thing and thinking the same thing, that they have exclusive access to the network card. Nobody has exclusive access to the network card. How are we going to make it look like they do? How about an abstraction? Why don't we provide an abstraction that says, this is just a pipe or a method of getting data off the computer and getting data back onto the computer. Don't worry about any of the underlying details, just regard it as being something where you only see your messages, other people only see their messages. It's as if the other people aren't even there from your point of view. So, that's a lot more convenient than saying, well, today there are 10 processes in the network card, yesterday there were seven, tomorrow there may be 15, there may be different processes, You may be busy doing different things. Gee, how shall I work it out? Don't worry about it. Let somebody else worry about it. Who? The operating system. And we'll do it with abstractions. Now, another issue for abstractions is that there are many, many variations in the hardware and software that we see on particular computers. So this is complex if you're building an application. Let's say you want to build Microsoft Word. There are many, many different processors you could be running on. There are many different types of screens on which you'll display your data, different keyboards that we'll use to input the words you'd like to type into your document. There are going to be different mouses or keypads or touchpads that you were using to move mice around on the screen. You're probably gonna store your Word document. There are different types of hardware you could be storing it on. But you don't wanna write a whole lot of different versions Microsoft Word customized each one of these combinations. What you'd like to do instead is say, "Okay, you got a keyboard. If you're using Word, you probably have a keyboard. Okay, you got a keyboard. I don't worry about what type of keyboard it is. It's a keyboard. All right, fine. That's all well and good. But the fact of the matter is that under the particular computer you're working on, you have one particular kind of keyboard. And that particular kind of keyboard has its own set of instructions, its own way of saying, Here's what happens when somebody presses the E key. Here's what happens when there is a movement on the mouse pad to the left of this many bytes. Here's the signal that goes out. If you want to do something on the keyboard, like you want to put it into a lock mode, here's how you put it into the lock mode. Here's the signal you send on the bus saying put it into lock mode. Okay. How are we going to deal with that? Well, the way we deal with that is we say there is the abstract keyboard, and then there's the real keyboard you actually have. We will have the operating system offer the abstract keyboard, the abstract screen, the abstract storage device, all of these things in abstract form to the applications that are going to use them. And then when they try to use the abstract thing, of course, that isn't quite good enough to make the real device work. We're going to have to do a translation. So we are going to take a step from the generalized keyboard that that we make visible to our applications, down to the specific keyboard we have on this particular machine that we happen to be running on. We'll do a translation. So we offer the abstraction of keyboard, and then the operating system changes the abstract operations that we wish to perform into the very specific operations that this particular keyboard model is capable of performing. So the way we typically do this is we say, there are just classes of things. There are displays. There are storage devices. There are keyboards. There are touchpad devices. There are network cards, et cetera, et cetera, et cetera. And they have certain things in common. Each thing, each class has a particular type of things that that hardware does. And then within that class, there are many, many different particular hardware models. We will need to translate from the general thing that the class can do into the specific thing that your keyboard, your network card, your screen can do. A way we commonly do this is by providing a unified model. We say, here's a general way that any of these things can do things. One you're probably quite familiar with, because it's somewhat more visible to users than many of the others, is PDF, the portable document format. Pretty much any printer that you work with, any modern printer, is capable of taking a file in PDF format and saying, OK, I know how to print this. I know how to make this appear on paper in the appropriate form, in the intended layout that it's supposed to be in. Right point size, right pagination, single-sided, double-sided, whatever it may be. So if you create your document in a PDF format, you can take it from computer to computer to computer, each computer using a different printer, different model of printer, totally different model and you just have to say print this PDF. And the printers are capable of doing that, in part because the operating system at some point, either within the printer or within the computer that is attached to the printer, will do what's necessary to make the PDF understandable to the printer. Now there are other forms of general abstraction that are quite common. So there's an abstraction called SCSI, S-C-S-I, which is a format for storage devices, storage devices that work in block-oriented format. If you're a SCSI type device, well, then we say, "Okay, you're a SCSI type device. You understand how to do this operation, write a block, read a block." All we have to do is say, "Hey, you're SCSI, here it is." Now, that means that at the higher levels, we don't have to worry about that. Somebody has to translate it into the actual details saying, "Well, okay, you're a SCSI device, but here is the particular bit pattern I put on the bus to say write this block of data." That'll be taken care of typically in the operating system in some layer of software So, this is a very important, very powerful mechanism by which we can hide many of the very detailed specifics of the hardware in a particular computer from the applications and from the users who are working with the computer. This is really important because when we write an application, when you are writing an application, you're out in a job somewhere and you're told we're going to build this application and we're going to sell it to 5 million people. those 5 million people will have some computer that is not exactly the same as the rest of the 5 million. They're going to have many, many, many different types of computers. Now maybe they're all going to be Windows machines or they're all going to be Linux machines or they're all going to be Apple machines or whatever, but other details are going to be quite different. And we need to make our application runnable on any of them. We don't want to have a specific version of our application for each of the 5 million machines we want a run on, you couldn't afford to do that. That would be economically infeasible in terms of building software. By building to abstractions, we can say, this set of 5 million computers that are as our customer base, they all support the same abstractions. We build to those abstractions, and we let their machines work out how to go from the abstraction that they all agree is OK, that is usable, that they know how to work with, into the actual hardware they've got on their computer. OK, now abstractions often work with underlying computer resources and then provide higher level resources. There are various kinds of resources that we see in a typical computer-- serially reusable resources, partitionable resources, shareable resources. We'll talk about each of these in a little more detail. First, serially reusable resources. What do we mean by that? Well, this means that we have multiple clients-- applications, for example, multiple processes, who all want to use the same resource. But they'll only use it one at a time. At any given moment, one of them will have that resource available and will use it. And at some point in the future, a different one will use exactly the same resource for its own purposes. This is essentially multiplexing over time. You get it for this 5 milliseconds, you get it for this 10 milliseconds, you get it for this 20 milliseconds, et cetera. So this means that, of course, if you're going to have someone, the operating system, say this process gets to use the screen at this moment, it can display things. You've got to make sure that nobody else is displaying things to the screen at the same time in a way that it's going to be incompatible. So what you want to do there is say, I, the operating system, will ensure that anybody who wants to use the screen have to ask me to use it and I have already given this serially reusable resource to process X. If process X is doing the asking because he's the one who currently controls the resource, we'll do it. If process Y or Z or one of the other processes wants to use that serially reusable resource and display something on the screen, we probably won't do it. We will perhaps say to those processes effectively, there are ways we do this, later when it's your turn, we'll let you use the resource. OK, so that means we have to have what's called access control built into the operating system for serially reusable resources. What we mean by access control is who gets to access it, who doesn't. Somebody does, let them do it. Somebody else doesn't, don't let them do it. Now, another issue, of course, is that in many cases, the hardware that we're working with at the bottom has state. For example, if you're talking about a piece of RAM, a location in RAM, there's definitely state there, you put a word into that RAM. Now, if you want somebody else to use that word of RAM at a future date, not the guy who put the word in, but you want somebody else to use it, maybe that somebody else should not see the word that was put in there by the first guy because he's using it for a different purpose. This requires us for general things, not just memory, but pretty much every single resource that we've got because most of them at the hardware level have some state in them, it means we have to have what's called a graceful transition. That means we have to say, "User X was using it a moment ago. We're about to switch to user Y. We must change the state that was specific for user X to the state that is specific for user Y so that when user Y sees it, he sees it in the condition that he needs to see it in in order to do his work." What are some examples of serially reusable resources? A printer, speakers, and in the real world, not in computers, bathroom stalls. In all of these cases, there's one party who is going to use the resource. So somebody's going to print something. When somebody's printing something, he should print the whole thing. He should be allowed to print his whole document. Then you can move on to print somebody else's. So one guy gets his turn, then another guy gets his turn. For speakers, if you have some music that is supposed to come out of your music application and you also have music coming out of the game application, you don't want those two conflicting. You want one of them to be playing and the other not to be playing. And then later, perhaps you'll change and say, "Well, I'm finished listening to the music for the moment. I want to play the game. Switch over to the game. Give me their audio instead." And clearly for a bathroom, well, yes, you're going to use a stall yourself. When you're finished using the stall, somebody else can use the stall. You hope there will be a graceful transition. So what we mean by a graceful transition is a switch that makes it look like whoever is getting the resource at this moment had it permanently, or at least there was nobody else who had the resource before they did. Frequently what's going to happen, as we will see when we go through the class and future lectures, is somebody will get a resource for a while, then we're going to take it away from them temporarily and give it to somebody else. And then we're going to give it back at some point in the future to that first party. Part of the graceful transition is to say, we wanted to look to that first party as if he always held that resource, as if nobody else ever used that resource. No, that isn't true. The second party got to use the resource for a while. We didn't allow the first party to use it. So we're going to have to, in the first place, make sure, of course, that when we've taken it away from the first party and given it to the second, the first party can't go back and fiddle with it. But also then in the future when we give it back to the first party, we're going to have to restore the state. We're going to have to set it into the same state that it was in when we took it away from that first party, despite the fact that the second party probably has fiddled with the state in the meantime. So this means that we have to be very, very careful about what happens when there's one of these transitions. It must be graceful. It must appear to whoever is getting the resource, if they're getting it for the first time, But it's a brand new resource clean, nothing left over from previous users. If they are re-obtaining the resource without-- while somebody else has had it in the meantime, we need to make sure we restore the state. Now, another type of resource that we see in computers are partitionable resources. This is a resource where we say we've got a big pool of something. We've got a lot of it. And everybody wants some of it. We are going to partition that pool into pieces, Process A is going to get this amount, process B is going to get that amount, process C is going to get that amount. Typically disjoint pieces. So everybody gets their own piece, they don't get other people's pieces. Okay, this is a spatial multiplexing issue, where we say we divide something into separate pieces, pieces of space. A very, very common example of this, of course, is gonna be RAM. Now, we need access control here as well, because we wanna make sure that if we've given you chunk of the resource, not all of it, but your chunk, that you can't get other people's chunks. If we didn't give it to you, you can't see what they've done. And on the other hand, if you have it, other people cannot affect what you're doing with it. You get exclusive access to your piece, as long as you have that piece. So obvious examples, here are RAM, a disk drive that stores data, flash drive, is an example. In the real world, hotel rooms. When you rent a hotel room, there's another hotel room right next door to yours. It's rented by somebody else. You are expecting that what you do in your hotel room is not going to be impacted by what other people are doing in their hotel room and vice versa. Do we still need graceful transitions? Well, yes, because practically always, at least in computer systems, we are not permanently allocating part of the pool of the resource to one party and never again will it be given to anybody else. Typically, they're going to get it for a while. We're actually going to do both spatial and temporal multiplexing on this resource. So we're going to give a chunk of the RAM to this process, and it'll get to use that RAM for that period of time. But sooner or later, we may take away that chunk of RAM from process A and give the chunk of RAM to process B, in which case we're going to need a graceful transition. We're going to need to hide what process A held in the RAM before we give it to process B. If process B thought that that RAM contained data that process B had put in there earlier, we need to make sure that data is in there when we give it back to process B. Okay, one more kind of resource that we've got, shareable resources. These are resources that are usable by multiple clients pretty much simultaneously. If we're talking about a multi-core processor, we might be talking about truly simultaneously except at the very, very tightest level, instruction level. So this means that you don't own the resource exclusively. You share it with everybody else. And you don't own a subset of the resource. Everybody shares a whole resource. Now, what kind of resources could you use there? Well, if there's no limit on the resource, then why bother dedicating a particular part of it to somebody or saying people have to take turns? So when you're sitting in a lecture hall-- we aren't at the moment, but when you are-- and there are 50 students in the lecture hall, We do not say, OK, student one, you can breathe now. Stop breathing, student one. Student two, you can breathe now. Stop breathing, student two, et cetera. We also don't say, this is your hunk of the room. You can breathe the air in this part of the room, but you can't breathe the air in another part of the room because that belongs to somebody else. There's as much air, we hope, in the room as everybody needs. We don't worry about how we allocate it. Now, another example in computer terms that is a shareable resource, an important one, is the operating system. We do not, when we're running a computer with those 300 processes, have 300 copies of the operating system, one for each process. In terms of the code of the operating system, the code of the operating system will be the same for everybody. All 300 processes are going to see the same operating system code. So we have one copy of that code, and it is shared by everybody. Is this safe? Well, you're not supposed to be able to change the code of the operating system. Certainly, you're not supposed to have a process changing the code of the operating system while the operating system is actually running. That shouldn't be happening. So we make arrangements to say that the copy of the code of the operating system that everybody is sharing isn't writable. You can use it, you can execute from it, etc. But it's not writable. That's going to work fine for code. Now, do we still need graceful transitions for these shareable resources? Usually not. Usually the shareable resource isn't changing state based on what one process is doing. Therefore, we don't have to say we're cleaning it up for somebody else because it didn't get dirty. All right, so this is great for those execute-only pieces of code, for example. Now, in particular, if you think about this a little bit, shareable resources are going to be really, really useful for the system and for the processes that run it. they're not going to have to worry about a whole lot of stuff like these graceful transitions. Nobody's going to have to do work to make sure you get a graceful transition. So, this is a general tip in terms of building your own code. You can, in some cases, make use of shareable resources in application-level code that you're building. And if you can, by all means do it, because these are great. You want to design complex systems to maximize the shareable resources, the things that people do not have to worry about, is it my turn, is it my piece? They just use it. Okay, now let's talk about some critical operating system abstractions. These are classes of abstractions. In many cases within the class, there are several different types of abstractions for different types of things, but they all share certain common characteristics. These are important because we're going to use them very, very widely in building and using operating systems. And other abstractions are going to be built on top of these abstractions. There are memory abstractions, there are processor abstractions, there are communication abstractions. So let's talk a little bit about each of these. Let's start with memory abstractions. What do we mean here? Most programs, in fact all programs, are going to require some amount of memory in order to run. They're going to save some data, they're going to use that data, they're going to make decisions based on what that data says. And this is true for people as well. You know, when you're working with a computer, you look at a piece of data and you say, I want to save that email message, I want to reply to that email message, I want to delete that email message, and that's based on the content. You're going to look at what is in the memory that represents, for example, that email message. So there are a lot of different types of memory that we make use of in a computing system. Certainly the variables in your program, you declared an integer, that's a piece of memory. You're going to do allocation of memory on the heap. You're going to say, "Malik, 500 bytes," and put it here. That's a piece of memory. Files are memory. They're a different type of memory, but they're memory as well. Database records, if you're working with a database, that's memory. Messages, actually, as long as you aren't talking about the actual sending and receiving of them, using them, using their content, that's a form of memory as well. Typically, when we are going to send a message, at some level, we're going to build up a message in a piece of memory until we have everything we need there, and then we're going to send it. Now, every memory abstraction, every type of memory that we're talking about here, has some similar characteristics. Generally speaking, you read the memory, you write the the memory. However, there are complications. What complications are there? Well, some memory is persistent. We've already talked about persistent memory in the form of, say, a flash drive where you write to the flash drive, turn the computer off, turn the computer back on. Hey, the data is still there on the flash drive. Same bytes, same bits. Then there's transient memory. If you're running on your laptop computer and you are doing all kinds of things, then you've got a lot of data sitting around in the RAM of the laptop computer. shut down the laptop computer entirely. All that data that's in the RAM disappears. It was transient. It was only there as long as the power was on. Different characteristics of those two kinds of memory. Then there are the issues of the size of the memory operations. Now as I've said several times already, flash drives are going to work with a block, 4k perhaps at a time. That's not typically what you think of when you are writing data that you want to store persistently. You're thinking like I've got five more bytes to put in the file or something like that. Also, of course, it's the case that when you look at the low level, RAM works at the word level. You can move a word in, you can move a word out. That's what you can do with that form of memory, word level. So typically, at the highest programmer level, you don't want to worry about too many of these things. You don't want to say, "Oh, this is a word, therefore I will do the word operation." You know, maybe an integer is the right size, maybe it isn't the right size, you have a big data structure, you want to copy the data structure and so forth. You don't really worry about that level of the size of the memory operation that you can perform. You know, if you have a data structure that's 512 bytes, you cannot in one instruction copy the 512 bytes from one place in RAM to another place in RAM and copy it a word at a time. So this leads to complications, because what you want to do at the high level in your program or when you're a user doesn't particularly well match what you can do at the physical level on the particular devices. And it's different for different devices. Another issue is the issue of coherence and atomicity. We'll talk about these in more detail when we get to file systems. But the basic idea behind coherence is, OK, if I say I am writing this block of data to this device, do you know that from that moment onward where you said, "Yes, the block of data is written," everybody under all circumstances will see the new value? Or won't they? There are differences in how we do that. If we want to achieve certain things, it may be we're going to have to work to also do more effort on coherence. Atomicity. As I said, weak several times. I'll say it many times again, because it's an important issue, it turns out. With a flash drive, we can write 4K at a time, fine. What happens if you have an application that says I've got 8K records, write the 8K record? Well, what that probably means is you're gonna write two or three 4K blocks of data. Each one of those blocks of data is gonna be one operation. You write one, then you write another. If you need a third, you write the third. Each one is separate. Now, can you be 100% sure that under all circumstances, when you say write the 8K, all 8K got written? Will you, under any circumstances, see a situation where somebody else who's looking at the same data sees the first 4K, but doesn't see the second 4K? If you can prevent that, you have atomicity. Things are done atomically as a unit, despite the fact that at the low level, you're gonna have to do multiple operations. If you don't get that effect, well, then life becomes a lot more complicated. You don't have atomicity. Then there are issues of latency. If you say, I'm going to write a word into RAM, effectively, that happens almost instantaneously. Happens at the speed pretty much of an instruction, a single CPU instruction. That's great. When you want to write to the 4K block on the disk, that's not nearly so fast. That's going to have a large latency. If you're using tapes, which we still sometimes use, and you're writing to tape, that's going to have an even larger latency. So you don't necessarily have, for different types of memory, the same latency. And this is going to have some major issues on performance and how we are actually going to work with these devices. This gets more complicated because you could be using many different types of devices to support the same abstraction, such as files. So files can sometimes be stored, of course, on flash drives, or on hard disk drives, or on tape drives, or on writable CDs. Or they could be stored in certain circumstances in RAM. Now, in each of those cases, you're getting a different latency. And we're going to have to do something to make sure that the abstraction that is being used by the programmer is not badly affected by the differences latency because of course when he writes his code for the program he may not be aware of which of those technologies is actually going to be used to store a particular file. After all, four million users out there on four million different computers with four million different storage devices and they can specify what file they want to your application. What's going to be the underlying storage device that uses that particular file at particular time on that particular computer. Certainly the programmer can't know that. That won't be known until the user tries to do it. It'll lead to complications. So, this of course arises because on a particular machine you do not have the abstract device. You have specific physical devices. These physical devices have characteristics specific to that physical device and to no other physical device. They're not changeable. You don't like the fact that it's 4k writable at a time. Tough luck. It's a flash drive. It's 4k writable at a time. That's all you got to work with. You got to live with that. And if it's not convenient because you're writing a byte at a time, well, then you're going to have to live with that too. And if it turns out that you wrote a program that works great on your machine because you're using a hard disk drive for store data and you move it to another machine and it's got a flash drive and it doesn't work so well on that machine, that's not good. We don't want that happening. The operating system had better prevent that from being the case. So the core problem that we have with abstraction here is we're creating an abstract device at the operating system level. We have a physical device at the actual physical level and on different computers, the mapping between the abstract device that everybody has and the physical device that each user as each machine has that is totally different than some of the others, it's not the same. And we're going to have to make sure that whatever we try to do with the abstract device, regardless of what machine it's on, works well with the physical device we have of a particular machine we're happening to run on right now. So a typical file. As you are probably aware, having done some programming-- I hope. If you haven't done any programming, this may not be the right course for you. A typical file, you can read and write whatever you want. want to read a byte, you want to write a byte, you can write a byte. You want to write 500 bytes, you can write 500 bytes. You want to write 10,000 bytes, you can issue an operation saying here's a buffer of 10,000 bytes, write 10,000 bytes. And you expect when you make that system call saying do this right, whether it's for a one byte, 10 bytes, 10,000 bytes, you expect when it comes back from the system call and says sure your data is written, you your data is written. Now, that's despite the fact that you are going to have to do certain things at a low level within the system to make that happen. So we expect, as I've said before, to seek coherence here. So you wrote the 10,000 bytes and you now say, "I'd like to read byte 7,000 of that 10,000 bytes." You have every right, every reason to believe that when you read byte 7,000 after having been told, "Yes, your 10,000 bytes have been written, that you get the 7,000 bytes of the 10,000 you said to write. That's what comes back. That's coherence. When something is supposed to have happened, it actually appears at least to have happened. You see results that are consistent with what you've been told has happened. And you expect the entire thing to occur. The 10,000 byte write is probably going to take like three actual block writes. Okay. Well, you want to make sure that when you come back and say your or 10,000 bytes have been written, you will get the effect of all three of those writes having been, having happened, not just a couple of them, and the third one hasn't been done yet. Either you will see them all written or you won't be told that your write has occurred. That's out of my city. So what happens in particular, if we get into a more complex situation where we have 17 different processes running on your computer, and they're all writing to the same file, And they aren't coordinating too carefully with each other. They're not saying, "It's your turn to write, it's your turn to write." They're just saying, "I wanna write." And they write. What are you gonna see? Well, if there's no organization between them, if they have done nothing to synchronize their behavior, then you're certainly not going to get a whole lot of guarantees about the order of things. But there are some guarantees you should get. You got the 17 different people, each one of them doing a different write. you expect that everybody will see the results of those 17 writes in the same order. So if somebody sees write number 12 having occurred, everybody will see write 12 having occurred. And similarly, if somebody has not yet seen write 5 occur, then everybody who before him had asked about write 5, they shouldn't have seen it either. So you want to have some guarantee of the order of writes that occur to a computer. Now, what's actually going on under the surface? What's happening down there at the bottom? Well, you got something. You got something that stores data, probably persistently, perhaps a flash drive. And we got weird characteristics of flash drives. They're write-once semantics. You know, they can do the erase. But erasing is slow. You erase a whole lot of data, including everything you've already written to that block if you were erasing a sector with that particular block. And it's slow. So typically, what you wanna see from atomicity byte level atomicity. I wrote one byte to this file, then I wrote a second byte to this file, then I wrote a third byte to this file. If I said write byte A, byte B, byte C to the file, all to the same file, and I've come back and ABC supposedly occurred, I want to see those three bytes out there in the file. I don't want to see just one of them out there. I don't want to see an arbitrarily chosen couple of them. If I've been told they were all written, I want to see all three. Now, if I was writing into one block, a byte at a time, I can write that first byte if I choose to into the block, but now the rest of the block isn't writable. So now if I say I'd like to write the second byte, it turns out to be in that block, right after the first byte. I can't write that. It's not writable. What could I do? Well, I could erase it and then write the first and second bytes. And then when the third byte comes along, I could erase it and write the first, again, The second, again. The third, again. Commit that block. I could do that. May not be the right thing to do. So it's also the case that one other characteristic that isn't wonderful about flash drives is yes, you can erase a block and you can rewrite it after erasing. However, you can only erase a block on a particular flash drive, a particular block on a particular flash drive some number of times. It's quite a lot. Thousands, maybe tens of thousands. But there's a limit. After you've erased it a certain number of times, it is not erasable anymore. You cannot write that block anymore. So that's going to lead to some issues. We don't want to keep erasing, erasing, erasing, erasing the same block over and over again. This is not an issue that is suitable for users and it's not an issue that is suitable for application programmers. They shouldn't have to worry. We're going to have to worry about it at the operating system level. So what's this lead to? What do we actually have to do in the operating system to make that work out correctly? Well, we're gonna have different structures for file systems. We'll be talking about file systems in later lectures. And some of the ways that we have built file systems in the past and the ways we build them nowadays have changed based on the fact that we have different hardware architectures sitting under the covers. Things for example, that avoid overwriting data when they don't need to. Then you're going to have to say, well, if it's a flash drive, got these blocks of data that used to have the old version of the data. One way of dealing with the fact that I'm going to have to change it is instead of erasing the old version of the data, I'll find a new empty block. I'll write the new version of the data in the new empty block and do some magic to make sure that when I want to get that block of data, I don't look at the old one that I didn't erase, I'll look at the new one. Well, that means the old one is now empty. I'll have to remember the old one is empty. I'll have to go back and say this is now a block I need to erase at some point and now I can reuse it for other purposes, it's a garbage collection problem. I'll probably want a pool of these sitting around ready to use whenever I need them. Empty blocks that I can use to write when I need them. So I'll have to have code in my file system that does that. I'm going to try to avoid having a particular block erased over and over and over again because of course after a certain number of erase cycles it's no longer usable. It's called wear leveling, trying to make sure all blocks are erased about the same number of times on a particular device. And I'm going to have to do atomicity. I write A or I write B, I write C, each one a one word write. I want to get atomicity of those three writes, and that's going to require some complexity as well. So there's going to be all kinds of issues with dealing with the abstractions of memory that the operating system will need to perform. There's another type of of abstraction that we're going to care about quite a lot. Interpreters. An interpreter is something that performs commands. So at the hardware level, the CPU is an interpreter. There are other interpreters. Processes, they're interpreters. Threads, they're interpreters. There are other forms of interpreters. So this is-- you have to have interpreters on your computer. It can't do anything. Memory doesn't do anything. It just sits there and says, well, yes, here's the data. It's in here. You can get the data. You can change the data. but it doesn't perform instructions. To perform instructions, you have to have interpreters. All right, at the physical level, as I said, we've got CPU, the core of the CPU in particular. But that's a very, very difficult thing to work with. So we prefer to provide a higher level abstraction that is still a form of interpreter. Now, whatever we are doing in terms of interpreters, we have components of an interpreter. For example, we have an instruction reference. Interpreters run sets of instructions, typically a stream of instructions. Do this one, then do this one, then do this one, then do this one. That's what happens when you write a program. You got code in the program. Do this statement, then this statement, then this statement. The instruction reference within an interpreter that is running a piece of code keeps track of what it should do next. It points to the next thing to do. We have a repertoire. Repertoire is the set of things the interpreter can do. What can you do? I can do this, I can do that, I can do the other things. Those are the only things you can do. If you want to do something, you're doing one of those. An environment reference. Typically, we are going to try to keep track of other things, not just what the next instruction is, but for example, which subroutine are you currently working in? If you end that routine and you go back to the previous routine, what routine was that one? And where were you in that routine when you call this one? That's going to be something you have to keep in terms of the environment in which you're working. It's going to probably be a type of memory. And then there's interrupts. There are going to be situations where your instruction reference points to a particular instruction, but because of something that happened in your machine, that's not the next thing you want to do. You want to do something else instead. This is really very, very important in modern computing. So you have to be able to support that. And you have to say, this is how an interrupt will occur. Here are the circumstances where interrupts occur. Here's how we handle them when they happen. So one example of an interpreter that we build as an abstraction in operating systems is, of course, the process. So how does the process implement all of these components we just talked about, the instruction reference? Well, we're going to have a program counter. The program is going to have a bunch of machine language instructions. And for every process that we're running, the operating system will say, here is the next machine language instruction. Here's the address and memory of the next machine language instruction this process should run. Program counter will keep track of our instruction reference. What's its repertoire? For any given program, we have code. The code essentially specifies the repertoire. You can do things that are in the code. You can't do things that aren't in the code. We have an environment. The stack will keep track of which routines have called other routines. The heap will keep track of any memory that we have allocated temporarily, doing a malloc, for example. And there are registers as well, which at a low level are very, very important to everything happening properly. They're invisible, of course, to the application programmer and, of course, to the user. They don't know what's in the registers. But we really need to keep track of that or we don't know what's going on. Somebody better keep track of that. The operating system's going to have to keep track of that. So we're going to have to have that information available for each process that we're running. 300 processes. You're gonna need 300 program counters, 300 different repertoires, 300 sets of stacks and heaps and values and registers and all this kind of stuff. And we need to make sure that each interpreter is separate from the others. Your interpreter is not supposed to be able to fiddle around with my program counter, for example. So how are we gonna implement this in the operating system? Well, if there's only one process, it's very, very easy. However, that's not the case. As we, I will say multiple times, you got 300 processes on your computer. Maybe you don't have 300, maybe you have 150, but you have a lot. So there are gonna be multiple processes, all of which are trying to run when they can. You only have a limited amount of physical memory, of RAM. Generally speaking, in modern computing environments, you do not have enough RAM to meet the needs of all of the processes that you want to currently run. They need a certain amount of memory each to hold all that environment information. You ain't got enough memory for that purpose. You don't have enough RAM. And generally, registers, everybody needs registers. Where are registers? Registers are part of a CPU core. Got eight cores on your CPU, you got eight sets of registers, one on each core. But you have 300 processes and they need 300 sets of registers and you only got eight. So you're gonna have to do something about that. How are we gonna do this? Well, we're probably going to say process is going to, at some point, be sitting on a particular core of the CPU, and it's going to be running. At other times, it's not going to be sitting on any core of the CPU, and it's not going to be running. And we are going to have to move things back and forth. So what's this going to lead to in terms of what we need in the operating system to support the process abstraction? Schedulers. 300 guys want to run. You can run eight of them. Which eight do you run right now? The scheduler is a component in the operating system that decides such things. There's going to be memory management issues. You have a certain amount of RAM available. You have need for more RAM than you have available because all your processes need some and they need more in total than you have. How are you going to decide who gets which piece of memory at which moment? And how are you going to change things when you need to say, I need to take away memory from this process and give it to that process. But we don't want people, certainly don't want the applications and definitely not the human users to know that this is happening at all, to know that, oh, this is my piece of RAM, or, oh, gee, I just lost a page of RAM. We don't want them to know that. We need to hide that from them. We want to give every process the illusion that it has all the RAM it needs. It asked for this amount of RAM, it got this amount of RAM. It wants to use that RAM, it's got it. That will turn out to be a lot of work, and we'll talk about what we have to do to achieve that effect. And then, of course, we're going to want to have access control on other resources, such as files. I create a file for my process. I want to have exclusive access to that file. I don't want other users on the computer to be able to access that file. I need to somehow or other do something to tell the operating system, don't let other people access that file. And I need to have confidence that they really can't. If I have told the operating system, they shouldn't. There's going to be all kinds of work involved in making all of those things work. The third kind of common abstraction that we frequently see in operating systems is communication. Communication basically says, well, we want these interpreters, each of these interpreters processes, for example, to be pretty much separate from all of the other interpreters. We don't want them to be interfering with each other's operations, except when we do. Every so often, you want to set up cooperation between processes. If you've ever on a Linux type machine done a pipe, where you say, LS pipe through more or whatever it is you did, You just had inter-process communication. You got data that came from the LS process, and you needed that data to go to the Mora process. Well, that's going to require communication. So we're going to have to allow that to happen in certain circumstances. It gets more complicated if it turns out that we're not doing communication between two processes on the same machine, but between two processes on different machines. Things get a lot trickier then. Now physically, how are we going to do this communication? From a physical point of view, in a single computer, when we have two processes running on the same computer, we probably are going to do the communication by copying data from one piece of RAM to another piece of RAM. Data in the sender's RAM is going to be copied into the receiver's RAM. Of course, you can't copy data from RAM in one computer into RAM in another computer. That's not possible to do. The only connection you have to that other computer is you have a wire. Essentially, you don't even have a dedicated wire usually. You have something that connects to the internet. And the guy on the other end, the one you want to communicate with, the one who wants to get a copy of that data, he's got a connection to the internet. And somehow or other, you're going to send something across the internet that's going to get to him, and he is then going to be able to make use of it. That means you have cables effectively that are communicating with those computers. You can't do copies across cables. You can send data across cables, but you can't do copies across cables. Now, this is at the very low physical level. When you get up to higher levels, there are other abstractions that have been built in, not always by operating systems, but often by operating systems, IPC mechanisms. So when I say I am going to get some data that is in this process and copy it to that other process or on the same machine, that's going to indeed be copy some data from this piece of RAM to that piece of RAM. But the actual decision of which piece of RAM you copy from, which piece of RAM you copy to when it occurs, how you make sure that it happens in a coherent fashion, for example, that's going to be up to the operating system to deal with. We're going to have to do something about that. And if it's a network, well then we're going to have to use network protocols, which gets more complicated. Now, communication abstractions, as you can see, have some relationship or similarities to memory abstractions, but they're a bit different. First, the performance tends to be a lot more variable, particularly when you're going across a network. There, the variability is extreme. Often, things are asynchronous. When you're saying, "I'm just dealing with memory abstraction. I put something into a piece of memory, I get something out," there may be a delay between when you say you want to do it and when it gets done. But once it's done, it is done. Now, here, though, with communication, it's asynchronous. I'm going to send something to somebody, but I don't know if the guy wants to receive it or not. He's going to run at some point, and maybe at some point he'll say, "I want to receive it," and maybe he won't. It could be the other way around. He says, "I really need to hear from my partner over there. Tell me that my partner over there has sent me this data. I want that data from him." That does not force the partner to send the data. He can choose to send the data whenever he chooses to send it, or perhaps not send it at all. So that's an issue that we're we're going to have with synchronization between the sender and receiver, you typically don't have when you're talking about memory abstractions. And with a read, typically, you only perform a read and you perform the read. You just read whatever's there. On the other hand, when you were receiving a message, you don't get anything unless somebody sent a message. So when you say I'm receiving, that does not necessarily mean you're going to get anything. It depends on whether the sentence occurred or not. And with remote computers, when you're working with a process on a different computer, things get very, very complicated. So how are we going to do this? On the same computer, we're going to do something involving somehow or other, under the covers, copying data from one place to another. Also, as we will see when we get into memory management, we have another option. We can play memory management tricks, which we'll talk about later. So this is possible, but it gets complicated when things are remote. Moreover, in many cases, we want to use the same mechanisms for remote communication as for local communication. We don't want to have something that looks totally different. So what does this lead to? Well, certainly, even if it's on the same machine, we are going to be copying data. Now, one thing about copying data is that it has a performance cost. The more data you copy, the longer it takes to copy it. The more frequently you copy, the more time you're going to spend on the overhead of copying data. Copying is not free. If on the other hand, you say, I'm going to do these memory management tricks, well, yes, you can do memory management tricks and avoid the cost of copying. But there are other complexities involved there. If you're working within the network environment, you're sending to somebody else, somewhere you've got to have network protocols, TCP/IP, for example. Well, TCP/IP is a protocol. It's a set of rules. but it's also implemented as code on a particular machine that is performing one end of the TCP/IP. So there's code there. Where's the code? Typically, nowadays, the code is in the operating system. So the operating system has just grown to include the TCP/IP stack, which is fairly complex stuff in and of itself. We're going to have to have that in our operating system and make sure it works properly with everything else. If we're working remotely, it's also the case that the internet is not a reliable network. You send the message in, maybe the message gets to its destination, and maybe it doesn't. So you have to worry about things like message loss. And one way of dealing with message loss is if you have reason to believe the message was lost, well, you just retransmit it and hope it gets through the second time. That leads to a variety of complexities, such as, for example, I can't retransmit the message unless I've still got a copy of the message. So when I send the message and I'm worried that I might need to do retransmission, I throw away the message that I sent because it may need to be retransmitted, in which case I'm going to need to have those bits around still. More complexity. And of course you don't want to keep them forever. So if you know that it has been received, that's great. But what if it hasn't? What if you don't know? How long are you going to keep those bits lying around? There are so many considerations here. There are also security issues because in addition to not being reliable, the internet isn't secure. You're going to have to worry about that. Okay, now let's move from abstractions to services. The operating system's job is effectively to offer services to other computing entities, things like processes, things like human users. We're going to offer services to them. We're going to make their lives easier. Generally, we will have an abstraction and the abstractions will say here are things you can do with this abstraction. For example, you can read and write a file and we are going to allow people to do those things at the abstract level. And then we're going to translate down into whatever we actually have, whatever we actually need to do at the lower level. That's how we'll provide the service, somehow or other. And there are going to be, of course, the important characterizations that we've seen before, CPU and memory abstractions, persistent storage abstractions, things like Windows on a computer that's got a display, things like virtual private networks when we're talking about communicating across the internet and so forth. How are we going to deliver these services? Obviously, we're going to run some code, but how? What code are we going to run? How are we going to actually make that code work? Well, there are several ways we could do this. The very, very, very old-fashioned way, we're talking 1950s way, is to say, well, the operating system will be a set of subroutines. Whenever an application program wants to get an operating system service, it calls the appropriate subroutine, runs that code, it provides service. That isn't always a good idea. More modern computers say, well, in most cases, in many cases at least, we're going to do what's called a system call instead. We're going to make a big change of context. We're going to have the operating system take over from the process, and instead of the process running code to provide the service, the operating system will do this. There are other ways we could do this. In a distributed environment, we could do it with messaging, for example. Now, there are different layers in the software that we are working with where these services can be offered. Now operating systems nowadays are typically built in layers, layer upon layer upon layer. And we've seen some of these layers already in the first lecture. We're going to take a look at a little bit more detail now. Generally speaking, at the high level, users interact with applications, applications interact with libraries, libraries interact with the system call and that gets you into the operating system. Once you're in the operating system, however, we may have layers of software within the operating system itself. Ultimately, everything gets down to hardware because you can't do anything without hardware. So everything you want to do will ultimately be done in hardware. Okay, so we have within the operating system itself multiple layers. Some of these layers are more visible to higher levels of software, like applications. Some of them are less visible. They are only visible within the operating system itself. Remember, everything ultimately though is going down to that hardware. We're running those CPU instructions, we're storing data in words of RAM, etc., etc. So here's a way of looking at it, another diagram. So here at the bottom, we see we have the hardware down there at the very bottom at those blue levels. The hardware provides an instruction set architecture as we've discussed. Everything above that is going to use the instruction set architecture in one way or another. At the top, the green level, we have applications. And for that matter, above that, we have users, human beings who are working with the computer and telling the computer to do things. The applications are going to also make use of things that are called operating system services. We'll talk about those in a minute. However, in many cases, the way that we're going to get things done from our application, unless it's our very own code that we've written, is we're going to consult a library. To get to the library, we're going to go through the application binary interface. The library is going to be stored in binary form, machine language form. So we are going to go from the machine language application, because of course we can only run applications that are in machine language. You can't run any code that isn't machine language. You can have interpreters that take another form of code, like Java bytecode, and convert it to machine language instructions. But ultimately, when we get down to things, they're all going to be machine language instructions. OK, so if we want to get a service from our application that is, let's say, a string concatenation service-- we've got two strings, we'd like to concatenate them, we're going to use a library to do that-- we will go across the application binary interface into the general libraries. Now, in many cases, for things like string manipulation, we can just do things at that level. We aren't yet running in any privileged mode. We're just running code that puts together a couple of strings, for example. However, sometimes that's not enough. Sometimes we're going to need to do things that only the operating system is able to do. Why is the operating system the only party able to do it? As I hope you remember from the previous lecture, we have the privileged instruction set. If we need to do something that requires a privileged instruction, We must use the operating system. So in certain circumstances, somebody, most commonly the library, is going to request that the operating system do something for the program. Program's going to call a routine in the library. The library is going to make a system call to the operating system. Any of you, I presume most of you, program sum in C. Remember libc? Libc is a library whose primary purpose is to hook together things an application needs to system calls that the operating system is capable of providing. You call a library function in libc, it says, oh, that means you want to do this system call, and it does the system call. And we'll talk about why we do that in a future class. But ultimately, in that case, you're going to get down to the operating system kernel. But everything eventually goes down to the instruction set. Now, one way we can deliver services is by subroutines. Now, as I said, in the very, very old days, 70 years ago, this is what we did. That's all we did. That's how we provided services. It's not all we do today, but we still do provide many services by subroutines, all those string manipulations that I was talking about. We don't need privileged instructions for those, but we sure don't want everybody to have to write their own string concatenation code. So we provide a library that does it for them. They just have to call the library. So this is just a subroutine call. It's a subroutine call just as if you had written that subroutine that concatenates two strings yourself. Just as if you had something called my_string_cat and here's the first string, here's the second string, here's where I want to put them. You can write that code yourself and you can build the entire piece of code that does that yourself if you want to, or you can call the string library's string_cat function. And all you have to do is call that function, provide it the parameters and everything works. Mechanically, in either case, whether you wrote it yourself or you call a library, the same thing is happening. You are calling a subroutine that's going to push parameters onto the stack. It's going to jump to the code in the subroutine. It's going to do whatever the subroutine does. It's going to set up return values when it is ready to finish. It's going to pop it off the stack, go back to the subroutine that called it. And that happens if it's in the library. It happens if it's in your own code. Exactly the same thing happens. So the advantages of doing things by this method, of providing services through this method, is it's very fast. There's very, very little cost performance-wise in terms of calling a subroutine and returning from a subroutine. And we can decide at the runtime, when we are doing things, this is the one I wanna call. I wanna call my string cat because it's better, except this time it's better to call the standard string cat from the library because that one's better for this circumstance or whatever you wanna do. And you can decide that at runtime. You can have an if statement sitting in your program saying, if x do this, else do that. So you can decide at runtime what you're going to call. There are disadvantages. In order to do this, everything has to be in the same address space. As we will see when we talk about memory management, every process has its very own address space. And you can't fiddle with other people's address spaces. So everything here is going to have to be in the same address space if you're to use subroutines. And it can become difficult if some of the code was written in one language and other code was written in another language, making them hook up together can be sometimes difficult. And certainly, you can't use privileged instructions as long as this is the only way that you are providing service. But we do this all the time with libraries nonetheless. So you don't need to write all the code for your own programs, you shouldn't be. You shouldn't write your own string concatenation stuff. You shouldn't write your own cryptographic routines. You shouldn't write these sine and cosine routines for trigonometric functions and so on, because somebody's already written a pretty good piece of code that does all that stuff. It's sitting in a library. Call a library function instead. Okay, so this is just sitting around as a collection of code, which the operating system will in various ways make available to your program if you want to use it. And this is typically done where a library will contain several different modules, each of which is effectively a subroutine or a set of subroutines. So you can call any of the subroutines that are in the library and you will be able to get whatever service is offered by that subroutine. String cat, or something that's going to truncate strings, or something that's going to search through strings, or whatever, if it's a string library. If it's a trigonometric library, here's Here's the one for cosine. Here's the one for sine. Here's the one for tangent. Here's the one for cotangent, and so on and so forth. So one nice thing about this is that typically, because these libraries are available in a very standard binary format, you don't need to recompile them in order to use them. You just have to say link with them. Hook me up with this one using various link options, which we'll discuss. All right, now generally speaking, the modern operating systems you're likely to use, Windows and Linux and Mac OS, they come built in with a whole bunch of these libraries, tons and tons and tons of these libraries. They do all kinds of wonderful things for you. If what you need is something that you're going to use in a lot of programs, but it's not one of those standard libraries, you can build your own library. Build it once, use it in all the programs that need it. So libraries are a very, very powerful tool in computer science for software reuse. Where do libraries live? They live right there. They're below the application binary interface. They're sitting there, ready to use, ready to link up within your program. So libraries have a lot of advantages. They're reusable code, which means that you write it once, and it's used as many times by as many different programs, and as many different programmers, and as many different users as need to use it. If you have a good library, well, it's probably very well written. Somebody who has built, for example, the trigonometric library for Linux has undoubtedly worked very, very hard to get the fastest possible correct implementation of each of those functions. It would be hard for you to write a better version of their assigned function than what they've already got. And if it turns out there's a bug in their code, they will be aggressive at fixing that bug and getting it out to everybody who's got a copy of the library. You're going to encapsulate a bunch of complexity here when you have those 3D rotations of graphical objects. Those can be very, very complex routines. You don't have to worry about the complexity of that if you're using the library function. Just need to say, well, OK, here's the routine name. Here are the parameters I need to give it. Here's the return code it's going to get. I just set everything up. I call it. I don't worry about how it does it. It just does it. Now, with libraries in modern systems, we have different bind time options. What do I mean by bind time? Bind time is the moment at which you say, here is a routine that I want to call. Where is the code for that routine? I need to find the code for that routine and bind it into my program. I need to hook up the call I make in my program to the piece of code in the library that is actually going to perform that function. That's bind time. Now, we can bind at different times, depending on how we want to do things. One thing we can do is static binding. This is where you say, I have built my program, I've written a bunch of code. Maybe it's in several different modules, and they're all to be combined together. And also, I've used some libraries. What I can do if I'm using static libraries is I can say, I'll take all of the code I wrote and all of the libraries I'm going to use, and I will combine them together into one executable program. I'm going to bind the library code into that executable program. It's part of the executable program. So it's in the load module at link time. When you link together the program, it's in the load module. So when you load it, the code is right there. Another choice is shared libraries. Many libraries, such as libc in particular, are used by many, many, many different programs. And this is code. This is not data. It's code. They don't get changed. They are not changeable, at least not without recompiling them. Certainly when you're running, you're not going to change it. We don't do self-modifying code in modern computing, code that changes its own values at the time you're running. So we can say, all right, fine. If there's something like libc that many, many programs want to use, we don't need to have a whole lot of different copies of that libc code sitting in every load module. in your load module, yours, yours, yours, yours, yours, all 300 processes that are using libc. Why do we need 300 copies of that code? Everybody wants the same code. Everybody is going to execute the code. They're not gonna write the code. That's a shareable resource. Shareable resources are great, as we said before. So if we are going to have that kind of library, why don't we have something where we say explicitly, this is shared. Now, what that means is that at link time, All you have to do is remember that it's shared. You don't have to hook it into your load module. There's yet another binding choice, dynamic libraries. If you're familiar with a program like Microsoft Word, Microsoft Word is a very complex program, and it can do many, many, many different things. Now, most times when you use Microsoft Word, you're going to do a few of the many things it can do, and you're not going to do any of the others. However, sometimes, for a different document, For example, you may end up doing one of those things that you weren't doing for most of your other documents. So somehow or other, when you're running Microsoft Word, you're going to need to be able to get to the code that does each of the things that Word is capable of doing. All right. Well, one thing we could do is just say, well, load all those libraries. Load every single one of them. You're not going to use 90% of them, but hey, load them anyway. Take up space and memory. Very wasteful. You'd use up a lot of memory space for that purpose. Instead, what we could do is say, well, why don't we just load up the stuff we know we need, the basic stuff, and if it turns out this time you're going to need something that's going to do 3D object rotation within your document, then we'll load up that library. So in that case, we'd like to figure out at runtime when you start running the program, or maybe even in the middle of running the program, halfway through. Oh, I need this library. I don't have a copy of this library. I'm going to get a copy of this library, bring it into my application and run it. That's dynamic loading. One thing to remember though about library code, regardless of which of these approaches you use, it is only code. You could have written that code yourself. It has no privileges that your code doesn't have. So let's talk about the ways in which we can share libraries. Well, the static library is just loaded into the load module, which means if you have 500 different applications running on your computer, they all use libc. And if you use libc as a static library, which you shouldn't, then every single one of those load modules is going to have a copy of that. You're going to use 500 times as much disk space on your disk because each one has to have a copy of the binary for that. When you load it, everybody is going to get their own copy of it. And you're not going to know that they have their own copy that's just part of the load module, it's going to be very wasteful. However, it does have the advantage of saying that, you know, this is my specific library. And if this is exactly what I want, I want to make sure nothing else is going on here. You get that. So if, for example, you say, I like version 1.7 of this library, and I really don't want 1.8 or the later ones, because I want 1.7, you might load version 1.7 statically. Then you'll have version 1.7, regardless of everyone else using 1.10. So if you, on the other hand, say, I just need a copy of this library. I want to use the standard copy of the library. If there's an update, I'd like the update to be applied to my program, too. You make it a shared library. So then when you run, you just say, hey, there's a shared library. Go get me access to the one copy of the shared library we're all using. I only need to keep one copy in memory. I only need to store one copy on the disk drive. Much cheaper. OK, so this has advantages. And the first, obviously, is that there's very much less memory used for a shared library. 300 people using Live C, one copy of Live C, not 300 copies. Big savings. Also, you don't have 300 copies sitting out on the disk. You have one copy, which is shared by everybody. This means also that you get a faster startup of your program. In principle, at least, when you say I'm going to start running a program, the operating system is going to have to create a process for that program that's going to copy in all of the code of the program, because it needs to have the code of the program sitting in RAM in order to run it. Well, the more code you need to copy in, the longer that's going to take. And if you have a shared library, with luck, if it's Live C, for example, it's very high probability there's already a copy of Live C sitting somewhere in memory, being used by one or more of the other processes that's running. Well, in that case, you don't need to copy a new version of Live C, and you just need to point to the one that is already there, and say, "Here's the copy you need to use." Updates are simplified. If there's a bug or if there's an improved version of the library, you just say, "Okay, fine. I will change the library on my computer. I will install a new version of that shared library. I'll make sure that nobody is using the old version. From now on, whenever somebody wants to use the shared library, they get the new copy. They didn't have to do anything to their programs. They didn't have to update their programs. They didn't have to recompile their programs. They didn't have to relink their programs. They didn't have to One of the characteristics of shared libraries is they're not writable. This implies that there is no variable in a shared library that you can write. Now, that doesn't mean you can't have within the functions variables that are defined within a function. Because remember, where do those get put? I hope you all remember from your earlier classes, like CS33, that when you call, when you say, you know, this is a function foo and it's got variable x defined in function foo, variable x will be stored in a stack frame, which means everybody who calls foo will get their own stack frame and their own version of variable x. So you don't have to worry about sharing that data. But you can't do it with a global variable. You can declare global variables in regular programs, but you cannot declare global variables in something that's going to be a shared library, because then you have 300 people who are all sharing global variable x. That's not going to be good because somebody is going to write to that and the other people shouldn't see what's there. Now another disadvantage of shared libraries is if you don't actually need the shared library this time you're running, too bad you get it anyway. It's going to be available regardless of whether you want it, need it, or not. You have to know what you're calling at compile time. So you know you can say I can call this or that. I can use an if statement to call this or that, but I have to know that that's the possibility of what I could call. And if you don't like those things, you don't like those characteristics of the library, you have another choice. You can have a dynamically loaded library, and we'll talk a bit more about those in a moment. So in an attempt to clarify something about where is the code for this library, let's take a look at our options. So let's say we have two applications, app1 and app2. And we've got a library x. And library x will be used by both app1 and app2. Of course, we have secondary storage. And then we have RAM. Secondary storage is used to permanently store things. RAM is used to store things while they're running. So with static libraries, where is the library? Well, let's say we compile up app1. In that case, we're going to put the library into App1. There'll be code in App1, in the compiled version of App1, that represents the library. And if we compile up App2, there's going to be code in App2 that does the same thing. Okay, so we've compiled up those two apps, and we're going to store them permanently, semi-permanently, as long as we want them, on the disk. And each of them will have a copy of the bytes of that library stored on the disk. Okay, so now we're going to run App1. What does that mean? Well, we'll bring in the code for app one into RAM somewhere, along with a copy of the library. We run app two, we bring in a copy of the library for app two, along with the rest of app two. So now we can see that we have two copies of library x taking up space in memory. We do not share them between these two applications. That's a static library. Shared libraries, same situation, App1, App2, LiveX. So we compile App1, but we say we want LiveX to be a shared library. So we'll just put a little marker in App1 saying, "Hey, you want a shared version of LiveX." We compile App2, do the same thing. We're going to put a little marker in App2 saying, "You want a shared version of LiveX." And we'll store those on the secondary storage. There is not... The secondary storage will have a copy of LiveX somewhere, but it'll only have one copy even though these two programs and possibly many others are all interested in sharing. We run app one so we bring in app one and we say okay fine we're going to run app one. Well gee we better have a copy of livex because that little marker said we needed one. If there isn't one already in memory already in ram copy one into ram and we say hey app one here is your copy of livex and then when we run app two we bring it in and this time since app one is already running, sorry, brought in a copy of this library, we say, oh, you get to use the same library. We point to the same library located in RAM for the two apps. So, unlike with the static library, we have one copy of libx in memory, not two. How about dynamic libraries? More complicated. Okay, so here's the same situation. Libx will now be a dynamic library. So we compile up app1 and we make a mark in app1 saying this is a dynamic library we might be using. We might be using Libx as a dynamic library. It's a marker. It's not the same marker as with the shared library. Different type of marker. We do the same thing with app2. It says I want this to be a dynamic library. We say fine, we'll remember that it's a dynamic library. And we We store them on the secondary storage device. Now we run app one. We bring it in along with its marker saying, hey, this is a dynamic library. Now that's all we do at this moment. Sooner or later, app one says, I'm going to use that dynamic library. Maybe it would on this run. Maybe it won't on the next run. But if it says I'm going to use it, then we're going to say, oh, yeah, you're going to use it. Well, we haven't got a copy of that. So we better figure out that this is a dynamic library. We better get a copy of that dynamic library, store it in memory, and make it available to app one. You're only going to load the dynamic library if you actually use it. If you don't use it, or say you're going to use it, then we're not going to load it at all. And when do we load it? We load it when you say you're going to use it. We don't load it preemptively when you start up your program. And when you say, "I want to use it," which can be at any point in the program, here it is. Okay. Now, all of this was relevant to saying one way of providing services is by subroutine calls, subroutine calls in libraries in this case. There's another way, an important way, of providing operating system services, system calls. System calls is where a piece of application code, a program somebody wrote, says, "I want to call the system to do something for me. I'm making a call to that system, do this for me, operating system. Now, what is going to happen here is this is going to force an entry into the operating system. And that effectively means that, yes, you are going to call a routine in the operating system, but you're not calling it directly. And also, there is the issue that when you made the system call, you were running in non-privileged mode. When you get into the operating system to run the code that is going to perform the service that you requested, you're going to be in the operating system, you're going to be in privilege mode. Now, having made the system call, you can do a bunch of things you couldn't do before you made the system call. Well, the operating system can do them for you. You couldn't do that with the library routines. When you call a library routine, you do not enter privilege mode. You just remain in non-privileged mode, in the standard mode. Okay, so the actual implementation of whatever you're going to do with a system call is in the operating system's code. It is not in a library, it is not a user code, it is in the operating system's very own code, the kernel of the operating system. Now, the advantages of providing a service this way is that you can use a bunch of stuff that you're not allowed to use when you're running a non-privileged mode. This includes the privileged instruction set, it also includes any memory or data structures that the operating system has set up internally for its own use. Things like process descriptors, which we'll talk about. Also, as we've said before, processes are pretty much segregated from other processes. Each process is supposed to be separate. It shouldn't be able to interfere with other processes. If you want to communicate to another process and that process wants to communicate to you, you're going to have to get the operating system's help. So if you want to do that, You're going to have to make a system call for this purpose. You can't do that with libraries. Now, what are the disadvantages of this? There are always disadvantages that go with things. It's not well as their advantages. A big disadvantage here is speed. This is between 100 and 1,000 times slower than subroutine calls. Vast difference in speed. And that's important. That's important for how fast will your computer run. Okay, so what do we actually provide in services in the kernel and through system calls? Anything that requires privilege. So basically this is the privileged instructions. Anything that has to do with I/O, pretty much anything that communicates with one of these peripheral devices, speakers, network card, flash drive, keyboard, whatever. All these peripheral devices will only be accessible by the operating system. You want to access them, you got to ask the operating system. Also, interrupts. If you want to do something where you say, "I would like to generate an interrupt. I would like to deal with an interrupt. I would like to not see interrupts." Only the operating system gets to deal with those kinds of things. Allocation of physical resources. I don't have enough memory. My process is running. I need more memory. How do I get more memory? I make a system call and the operating system in its wisdom does or does not give me the memory I asked for. If I want to make sure that what I'm doing is private or if I want to make sure that I can communicate to somebody, then I will have to ask the operating system. And if it's something that's going to involve critical resources, things I absolutely must not be allowed to screw up, but I should be allowed to have some effect on, for example, processes frequently have priorities. How important is this process? How much work should we do to make this process run? I might be able to lower the priority of my process. Well, that's a pretty critical thing to do. I can't lower the priority of my process myself, but I can ask the operating system to do so. Now, sometimes when I ask the operating system to do so through a system call, the code that is primarily going to do whatever is supposed to happen isn't actually code in the operating system at all. Some of it is in the operating system, But it may say, you know, a lot of this, I need to do a whole lot of work here. But I don't need to do most of this work in privileged mode. Perhaps I need to have particular access that I wouldn't give to an ordinary user, but it's just access, you know, particular files that they can open, things like that. Why don't I have another process that's just sitting around waiting to do stuff for me. Somebody calls me, the operating system, and I say, "Oh, I need to use this other process." I communicate to that other process, they do this work for me, they tell me what gets done, they do it, come back to the operating system, the operating system then tells whoever made the system call, it happened, it's done. These are typically done with server processes or system daemons. Daemons are things that are run in the background all the time. The server process is more likely to be something that is started and ends. It's also the case that some of the things I do in the kernel are going to be provided by stuff that isn't actually there in every single version of that operating system, even within the same release of the operating system. For example, device drivers. Device drivers are things that work with a particular piece of hardware. If I don't have that hardware on my computer, I do not need the device driver for that piece of hardware because I'm not gonna work with that piece of hardware, it's not on my computer. On the other hand, if I do have that particular piece of hardware in my computer, I need that in my computer. I need that in my operating system. So typically, we use that kind of thing as a plug-in. If we need it on this particular computer, we plug it into the operating system. There are other things we can plug in or not plug into the operating system if we choose to, file systems being one example. So how is this all going to work? We are going to be providing services at the kernel level. So what happens here? We are going to provide the services in that red block, primarily. However, as I said, we can have some system services outside the kernel. So if we don't need to access the kernel data structures, we don't need to execute privilege instructions. We just want to make sure that the right thing happens here. And we want to make sure this is not influenceable by an individual user or an individual application that we don't trust so much. We may have a trusted piece of code, application-level code, but trusted. And we may, as the operating system, ask that trusted piece of code to do the work for us. One example that we frequently use is login. When you show up in front of a computer and you type in your user ID and password, somebody is looking at the user ID and password and saying, yes, let them log in. No, that's not the right user ID or password. Don't let them log in. Where is that code running? Typically, it is not running within the kernel, primarily. It is running instead in a special login process, a trusted process, but a process that is not running in privileged mode. So that's going to do the work of saying, I'm going to get the information about what the user ID is and get the information about what the password is. I'm going to compare them and see if that's the right thing to do. Log in the user if it is. Tell the operating system, yes, that should be done. And then the operating system will take care of a few things that require privilege, such as, for example, setting up a new process for the user who just logged in. OK, now some of these are just things where it's a system type thing, but it doesn't the operating system to work on this. So, for example, send mail. You could have a mail server that runs on your machine and it sends mail. It doesn't require privilege in and of itself. Now, some of the things it does do require privilege. Accessing the network card. But that's no different than anybody else accessing the network card, really. Most of what's going on with send mail is stuff that's, you know, worrying about the headers and formatting the mail and, you know, figuring out where to send it to and that kind of thing. We don't need to have the kernel doing that. As long as we have a piece of software, a send mail program, it's going to do it right. So we can run that at the application level. So when you want to send something, we're going to use this application level thing to send the mail instead. The NFS server, this is a distributed file system server where you say I'm going to access files on another computer. That requires a certain degree of trust, but not necessarily privileged directly. So the system service layer, where do these happen? These happen in this orange box here. They are sitting above the application binary interface, and they're well above the system call interface. So they are not running a privileged mode. Now, I have already alluded to some interfaces that we have here. Interfaces are another very important element of computer systems, operating systems in particular. What do we mean here? Well, nobody buys a computer because they want to run this operating system. I really want to run Mac OS, give me a Mac computer. You buy a system because you want to run applications. The purpose of the operating system is to support those applications via the abstract services. It's going to be very general. Everybody's going to want to run different sets of applications, different times, they may change over the course of time. Your operating system needs to be able to support a lot of different services that people might want to have. So, many different programs that are going to run on your operating system. You need to be able to access those programs somehow or other. The user, or if we're talking about programming level, the programmer has to be able to specify, "Here's how I access this service." Interfaces are key to making this work well to saying I can, as Linus Torvalds and the other people who build Linux distributions, I can specify here's what my operating system will do for you and here's how you make it do it. This is done with interfaces. There are two very, very important interfaces we're going to talk about here because these are interfaces between what users, human users, want to have happen and how we make them actually occur. The first is the API, standing for Application Program Interface. This is an interface that is specified at source code level, so at a programming language level representing source code. It specifies various kinds of files, various kinds of data types, what constants are like, and what's a zero here. It also specifies, here's how you call the operating system, and here how we do macros, and here how we have parameters for functions and things of that nature. This is how we get software portability. When you want to write a program, you say, OK, I'm going to write a program for Windows. You go to the Windows API and you say, well, if I want to open a file, I perform this system call. If I want to send a message, I perform that system call. If I want to obtain more memory, I perform this third system call. The API says, here is a source level description of what you need to do. All right, so this means that we can write that code for Windows. Now we've got a program that is designed to run on Windows. Now you can't actually run the source code. You've got to compile the source code into machine language in order to run it on a real computer. So of course, Windows supports many, many, many different types of CPUs and other platform issues. So when I want to compile my program, I've got to make sure the compiled version will run on my particular computer with a particular instruction set architecture it happens to have. OK, so when I've got this program, I could take it to a computer running ISA1 and program and compile it for that. Now I've got something that runs on ISA 1 on Windows. I can take it to a computer that runs Windows on ISA 2, compile it for that one. Now I've got a second version of the program that works on ISA 2 for Windows, and so on and so forth. In order to make this work, I'm going to probably have to have a bunch of OS-specific libraries. They're a version of libc or a similar type of thing. So I'm going to have to hook up with those. The operating system will probably be set up to make that easy for my program compile. And once I have compiled it up for this particular computer, ISA1 running Windows, I can run on any computer that is an ISA1 architecture running the Windows operating system. It should work. I can take the compiled version from one of those to another one to another one to another one to as many as I want. And as long as it's the same ISA, same version of Windows, it should work. It doesn't matter what the other characteristics are. If, on the other hand, I want to take it to a Windows machine that runs ISA 2, I'll need to recompile it for ISA 2 with Windows. Then I can take it to any ISA 2 architecture computer that runs Windows and so on and so forth. Won't work if I'm doing ISA 1 with Linux, though, because Linux has a different system call interface. But what is neat about this from the point of view of the industry is that if I have a good API, then I can tell my programmers, we want to sell to the Windows environment, build to the Windows environment, use their API. And if I, indeed, have my programmers do that, then I know it'll work for Windows. APIs help you write programs for your operating system. They do not help users directly. They help the programmer say, this is a program that I can run on Windows machines. We'll worry about the ISA issue, the instruction set architecture issue, when we get around to doing compilation. Now, there is another interface that we have on all of our computers that is related to the operating system. the application binary interface. As the name suggests, this isn't binary, so this isn't bits and bytes. So this is an interface that specifies things like the data format. How long is the word on this computer? How do I go about making a system call in detail? What do I have to actually do to cause transfer into the operating system? It's got things like, how do I do those dynamically loaded libraries? That's gonna be very specific a particular binary format. So this basically says, if you have an API, an API for Mac OS, and you've written a program for Mac OS, well, there are different architectures, different versions of the CPU for Mac OS. And I don't necessarily know that if I build for one, I can then move it to another one. If the API is good, maybe I can. But I got to compile. And when I compile, what's going to happen is I'm going to convert on the machine I do the compilation, the API that is general for everybody into an ABI that is specific for this particular ISA and this particular operating system. So that's the phase at which you go from an interface that makes sense for any version of this-- any computer running this version of a particular operating system to something that goes-- work on this particular ISA for this version of this computer, this operating system. This is how we actually get binary compatibility. I said before, you know, okay, we've compiled up for ISA1, for ISA2, for ISA3. If I'm in the business of selling software, I do not usually want to sell my source code version, the thing that was programmed up to the API, because I can't use the source code version without doing a compilation. And even if I have open source software, I really don't want to make my users have to do the compilation of the software to run on their computer. Very, very few users want to do that. Many, many users are not capable of doing that. They don't even know how to. Why should they? They're not computer scientists. They just want to run the damn program. So I have to have some way of getting a running version of the program to their computer, to their version of Linux running on ISA3. How do I do that? Well, if I've compiled up to the ABI for Linux on ISA3, I now have a version of the program that will run on any computer running that version of Linux with ISA3. So I can give that copy to everybody. I can sell that copy. I can distribute that copy across the internet with a download, whatever I want to how you actually sell software to customers. You sell them something that meets a particular ABI. So while a typical customer has no clue that the ABI even exists and need not have such a clue, the ABI is actually for users. That's who it helps, the individual user on a particular computer. Because if you have a good ABI and if people follow the rules and converting their API to the ABI, then you can just install the resulting load module on any computer that matches. This is how you get a binary to your computer and it just works. Now, for both of these kinds of interfaces, there is a strong issue, which is these interfaces specify how you make something work on your computer, how you make something work on a make something work on a version of Linux or whatever it may be. What happens if they change? Well, if they change, things work differently. The old version of Linux used this interface, the new version of Linux uses that interface, which means whatever was written for the old version of Linux maybe isn't going to work on the new version of Linux because you changed the interface. That would be very, very, very bad. People who build operating systems, particularly people who are doing it for commercial purposes like Microsoft, have a strong interest in maintaining the stability of their interfaces. And they want stable interfaces of both types, both API and ABI. They want stable versions for the API because Microsoft is very, very interested in having many third parties, many separate companies that aren't part of Microsoft, building applications that run on Microsoft products, that run on Windows operating system. So somewhere out there, there's somebody who wrote a big, big program that's gonna be used by many, many users. They're selling it to many, many users. It runs on this version of Windows. If the API for Windows is changed, then probably that version isn't gonna run in binary form because probably the binary version will change too. But certainly if you try to recompile that program, It was compiled, it was written for the old API. There's a new API, it is the same. Probably you are not going to link that program successfully. Your code isn't going to work anymore. You can't even compile your code and build it into a load module anymore if the API has not remained stable. It's worse for the ABI because with the ABI, you're telling an individual ordinary user, not somebody who's a programmer who can deal with complex issues of compilation and so on, But an ordinary user, here, take this and put it on your computer. OK, I haven't changed my computer. Same computer I had yesterday, last week, last month. The hardware is exactly the same. But I just had an installation of the new version of my operating system on my computer. And look, none of my programs work. Why not? One reason they might not be working is because somebody in that operating system development team changed the ABI. So the binary that you got yesterday that worked just fine before you installed the update of the operating system no longer has matching ABI on the new version. You try to use whatever you did in the old version, it doesn't work on the new version at all. Your program probably crashes. At best it is not going to operate the way it was supposed to. So you don't want the ABI changing either. You really, really, really want to make sure if you are one of these manufacturers or people in Linux environment, the people who build and make distributions of Linux. You want to make sure that the API and the ABI do not change. Now, from the other perspective, from the user perspective, or perhaps from the programmer perspective, there's a related issue called side effects. What's a side effect? Generally speaking, in computing environments, a side effect is something that occurs when you have an action on something. You're supposed to do something and it's supposed to have a particular outcome. And perhaps it does, but if it also has another outcome that was not specified as being what should happen when you do this thing, that extra outcome is called a side effect. In particular, when we're talking about interfaces, interfaces will specify, if you make this system call, this will be the effect. If you have a side effect, then yes, Maybe it is indeed the effect when you make that system call, but there's something else that happens too. But that wasn't specified in the interface. And remember, interfaces are supposed to be stable, but only the interface is supposed to be stable. We're supposed to allow the people who are developing the operating system to change their implementation, provided they don't change the interface. So if they change the implementation, so the side effect is no longer there, then hey, the side effect is no longer there. They didn't change the interface. It was supposed to do x. It did it before the change. It did it after the change. It met its interface. But if there was a side effect of y before, and there isn't after, well, anybody who wrote a program or who used a program working on the expectation that that side effect is still there, their program is going to break. Now, this is unfortunate. However, this is not the fault of the people who built that program, who built the operating system. They told you what the interface was. The fault here is of either the programmer or the user who said, "Oh, look, in addition to what's supposed "to happen here, look at this wonderful thing, Y, "that also happened. "I'm going to work on the assumption "that Y will just keep happening forever. "It won't necessarily." In many, many years ago, back when computers were not nearly so powerful, if you wanted to run a game on, let's say, a Windows system, You may not get the performance you were hoping for in terms of updating the screen rate or whatever. And people would sometimes discover that, "Oh, gee, if I don't do things exactly the way I'm supposed to do them, the way the interface says I can do them, I can make things happen faster on the screen. I can get my update rate going faster." So some programmers of games would do that. They would say, "Okay, I can make use of the side effect and look, everything runs faster. Look how much blindingly faster my game is on the computer now." Well, the problem with that was when the next version of the software came out, the operating system came out, their game would rely on the side effect and the side effect wouldn't be there. It wouldn't work anymore. So, why do side effects happen? Their game, when a version of the software came out, the operating system came out, their game would rely on the side effect, and the side effect wouldn't be there. It wouldn' t work anymore. So why do side effects happen? In many cases, the reason they happen is because there are different modules in the system, the operating system for example, that have an interface to them. And you're supposed to work with the interface, but there is some hidden relationship between things in different modules. It isn't part of the interface, it wasn't specified as part of the interface, but because of how they program their system, there is this relationship. And the side effect may depend on that relationship occurring. Since it isn't part of the interface, they may have changed it in the next version. The relationship no longer exists. Side effect goes away. Anybody relying on the side effect no longer is able to use it. So generally speaking, from your perspective as people who are likely to be programmers, you do not want side effects in your code. They can happen in the operating system, but they can also happen in complex applications. You're going to have the same thing. say I have interfaces in my application and I have definitions what happens when you interact with my application and there may be side effects that you have not intended to put there or at least have not specified as being part of your interface. This is bad. You don't want side effects in your system because they're going to lead in the long run to bugs and these bugs are going to be very very hard to find and they may make it very very difficult for your customers to work if they rely on the side effects. So you want to, if you're building a big complex system, avoid side effects. Try hard not to have side effects in your system. Okay. So in conclusion for this lecture, operating systems are pretty much all about providing services. That's what they do. They provide services. And they can provide services at various different levels, different layers in the software stack. It's very important that that the services they provide are specified in well-defined, stable interfaces. The APIs, one of these important interfaces, it helps develop programs, helps make it easy for programmers to build something that will run properly on a particular operating system. And the ABIs are very, very important for being able to distribute versions of usable software directly to customers without expecting the customer to do a lot of work. So they're both very, very important. They have to be stable. They have to be well-defined. OK, that's all we have to say in this lecture. We will meet again in the next class, and we'll start talking about some deeper issues in operating systems.