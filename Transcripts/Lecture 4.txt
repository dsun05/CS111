This lecture is going to be discussing the very important operating system issue of scheduling. We'll talk about what we mean by scheduling, and in particular we'll talk about how one chooses to perform scheduling based on the goals that you have for how your system should behave. We'll talk a little bit about what resources we should schedule, and we'll go through several example scheduling algorithms and talk about what effects you expect to see if you use those various algorithms, which in turn then describes why some of them are used in certain circumstances and not in others. First, what is scheduling? Well, the operating system of course is in charge of the computer and as we've discussed a few times there are many many different processes currently running on almost every modern computer. Dozens, hundreds, quite a few. And at any in moment, several of those processes would like to do something. They would like to run. They would like to send a message on the network interface. They would like to read a file off of the file system or various other things. Now, in general, the resources the operating system has to offer that will allow these applications to do such things are limited. So there may be one core. There may be four cores. There may be eight cores. but only that many processes can run at the same time, even if other processes are ready to run and would like to run. A network card can send one message at a time. A disk drive, flash drive, for example, can read or write one block of data at a time. So there's always going to be a question of, if you have several different things that the operating system could ask a piece of hardware to do, which one gets to be done next? So there are many circumstances in which this happens, And there are many solutions that you probably want to use, depending on the specific circumstances. Now, one of those issues, of course, is not just who gets to use it next, but for how long. In some cases, this is pretty obvious. So if you say, there are many messages that many applications would all like to send, sending one message takes a certain amount of time. And you don't interrupt that. Once you start sending a message, you continue until the message is sent. And then, at that moment, you can choose to send a different message. It could be a message, another message for the same application. It could be a message for a different application. But clearly, you have made the choice to send a particular message. And when that message has been completely sent, you can make a different choice to send another message for the same or a different party. With the CPU or CPU core, the question is a little different. Because unless you run into something that causes the process to block, as we discussed in the previous class, Processes can just keep running and running and running. So you then have the question of saying, if I choose to schedule a particular process to run on this particular core, core two, how long does it run on core two? Does it run until it blocks? Does it run potentially forever? There are many ways in which we can make these various decisions about what we are going to do. That's what scheduling is. So we schedule what job. By job, we mean process most commonly. But we also have work that we need to do not on the behalf of any single process, but for the overall system, work done by the operating system. What do we run next? When we say somebody is to run on this particular core, how long do they run? If we have a bunch of requests to, let's say, read different data off of a flash drive, which probably means opening different files and reading bits and pieces of different files, probably for different processes, which ones do we do next? If we have a lot of messages that we want to send from many different applications. In what order do we send them? Now, there are other applications of scheduling as well. These aren't the only ones. But generally speaking, we are going to talk about this primarily in terms of scheduling processes to run on cores. The others, you can probably use what we've discussed to get some sense of how they would be scheduled. but we'll concentrate on scheduling processes to run. How do you do it? How should your operating system go about scheduling processes to run? Well, generally speaking, the way we're gonna do this is we are going to say, there's a goal we wish to achieve. There is some performance-related goal, typically, that we want to achieve, and we are going to use a scheduling algorithm that we think will help us achieve that goal. As we will see when we go through some examples, different scheduling algorithms try to optimize some performance metrics, some aspect of performance of the overall system or of individual applications. We should choose an algorithm that is likely to optimize a goal that is going to help us achieve whatever it is we've decided is most important to achieve. This proves to be important because which scheduling algorithm we choose for CPU cores for scheduling jobs to run will have a very strong impact on the overall system behavior, which means it's going to have a strong impact on how processes run, how our users observe what's happening on the computer, whether they're happy or not. OK, so as we discussed in the previous lecture, there are different processes running on the system. Each process has associated with a process descriptor. Effectively, what we are going to do in almost all operating systems is maintain a queue. This is going to be a queue of processes. Normally, it's going to be something like a queue of the process descriptors or perhaps pointers to the process descriptors, depending on how you choose to implement it. This queue is basically going to be used for scheduling purposes. As a rule, we will say whichever process is currently sitting at the head of the queue is going to be the next thing we run when we make it our next scheduling decision, the next time we decide, is somebody going to run? Who is it going to be? This means, of course, that we're going to keep this queue in order so that whichever process according to our algorithm should be next to run is the one that gets to the front. And the one after that typically is one that should be run after that one that's at the front, and so on. So many of the scheduling decisions are actually made before we do the genuine assignment of a process to a core. The process is sitting there on that queue, ready to go, and sooner or later, we make the decision that, gee, somebody should go on a core. He's the one at the front. How it was determined that he was the one at the front has been determined at some earlier point. So that's when the scheduling really occurs. Okay, so there are various other aspects of doing the scheduling, which is we can only schedule, of course, processes that are ready to run, or in other words, processes that aren't blocked. And some processes will be blocked. So for example, when you wanted to read a piece of a file off of the flash drive, often you are going to end up having your process blocked until we can go out to the flash drive and pull in that data. Can't run until the data has been brought in. It's going to be some period of time when you are unable to make any progress, there's no point in scheduling your process on a core if you can't effectively run any code. So therefore, we're going to list you as being blocked. Many of the processes that we are going to have sitting around on our computer at any given moment are likely to be blocked. They're not thus eligible to run. So an obvious question is if you're using this process queue to keep track of which process you need to run next, What do you do with the processes that aren't ready to run? There are choices. One, you can say this queue only contains processes that are ready to run. So if a process isn't ready to run, it's not in the queue. Another choice is to say, well, all the processes will be in the queue, but everybody who's blocked will be sitting at the end of the queue. They will not be listed in the queue early on. So when we're saying, who do we run next? We're always gonna be looking at somebody who is ready to run, unless of course nobody's ready to run. And also, another option is to say, well, you know, that's not necessarily the best choice because what if there's a process that's really quite important? We'd like it to run next, but it's blocked. Well, when it becomes unblocked, we really do want it to run very, very soon. So we'd like to have it at the front of the queue. This means that either we have to keep it towards the front of the queue, but listed as blocked. So until it's unblocked, we don't actually choose it to run. Or alternately, when we unblock it, we're gonna have to go to the end of the queue where we put it because it was blocked, and move it up to the front of the queue, or move it from the queue of blocked processes into the queue of unblocked processes. This is a design decision. It is not necessarily right to do it one way or another. Now, as I've already stated, what we try to do with a scheduler is to say, there's a goal we wish to achieve. We are going to choose an algorithm, a scheduling algorithm, that we believe will help us achieve that goal. Now, you might say, well, we want good performance. Isn't that our goal? Yes, but good performance has different aspects. And let's consider some of those, some goals we might have, all of these being performance related, but different. So for example, maximizing throughput. By throughput here, we mean the amount of work that is done on behalf of application processes. So work done by the operating system, that doesn't count. We don't count that as part of our throughput. So we want to maximize the amount of work that user processes do. This sounds good. You do a lot of work for the processes that people actually want to run. Wouldn't that be great? Another choice is to say, well, you know, processes may be sitting in the scheduler queue for a while. It's not their turn to run. Why don't we try to minimize the average time that a ready process sits waiting in that queue? And that means that if you have an interactive process, then you don't have a long, long wait before something is done on behalf of your interactive process, which means we don't have people sitting in front keyboard saying when is this computer going to do what it's supposed to do. It does it pretty quickly if it possibly can. Another thing you could do is to say, well, I got these two, three hundred processes running on the computer. Wouldn't it be best to give everybody their fair share of the computer resources? One two hundredth, one three hundredth of the processing resources in the case of scheduling a CPU core. That would be nice, wouldn't it? That would mean we're fair. Everybody Another thing you could say is, "Well, no, of course fairness isn't quite right because there are important processes and there are less important processes. This isn't, you know, a democracy here. There are things that we really need to get done and things that we'd like to get done but they're not quite as important. So should we not prioritize the very, very important things and give them scheduling priority over the less important things?" Sounds like a pretty good idea too. In that case, perhaps we're going to do something that leads to scheduling of the more important processes more frequently, for longer periods, something like that. Another issue is that there are some things we do on a computer that are interactive with things happening in the real world, sometimes with people. So for example, when you are playing a video on your computer screen and you're displaying it to the user and the user is going to watch the movie, the YouTube clip, whatever it may In order to get smooth video, you need to show a certain number of frames per second. This is the way we show video. We don't have a continuous stream of changes. We show a frame, then another frame, then another frame. 24 frames a second, maybe at higher frame rates, sometimes at lower frame rates. At any rate, if we do not keep up with whatever frame rate we are supposed to be achieving, you don't get good video quality. And if you are too far off, you can get unwatchable video. So if the guy's watching a video, we really have to give him his frame every 1/24 of a second or whatever the frame rate is. If we don't, it's going to be an unacceptable viewing experience for the user. There are other cases where this is important as well. So for example, when we were talking about controlling a assembly line in a factory. If we have a computer that's doing that, the assembly line is moving the parts through various stages of the assembly lines. Possibly we have robots doing something at each stage. The line is moving at a particular speed. The robots are being ordered to do their things at particular moments. We got issues as orders at the right time, or things don't get done properly. OK, so given that we have different kinds of systems that are doing different types of things-- some of these are, for example, big server machines that are doing data analysis for data mining in turn to feed AI algorithms. Others are showing videos to people on computers. Others are running assembly lines or handling power plants or things of that nature. We have many, many different things our computers are doing. And they probably have different goals on what they would like to see achieved. So how do we schedule our cores? How do we schedule those various four or eight or 16 cores we have on the CPU available for us to run code? Well, it kind of depends on what we want to do. We have the choice, for example, of saying this is a time sharing system. That would be the kind of system that you guys are using to perform your labs on UCLA server computers when you run into, when you log into those Linux servers at, on campus. They are machines that are shared between many, many, many different students, all doing their work on the same machine. We want to make sure that every one of those users gets pretty good response time because this is pretty much interactive stuff. And on the other hand, we also want to make sure they also are treated fairly. Everybody is treated as more or less equal. One student isn't favored over another. On the other hand, what happens if we have a big computing facility, one of these big data warehouse things where we're going to do immense amounts of data analysis on vast quantities of data to build AI models? Okay, we want to get the most work out of this we possibly can. We're spending a lot of money for this facility. So we want the facility to be really good at producing those AI models. We're probably not doing much of anything else on those computers. That's what they're doing. Therefore, we want to get the best possible throughput we can out of those machines. It's not the case that we care as much about individual processes being delayed. It's more like we need to do a lot of work. There's a whole ton of work we need to do overall, we need to get through that whole set of work as quickly as we possibly can. If you have real-time needs, such as things like running the videos or playing music, has similar issues with video in the audio realm, you are going to have to have real-time guarantees. Miss the real-time guarantee, miss the real-time deadline for doing something, and you don't get the experience that you need to provide. Then also, in modern computing, a very, very popular way of providing computing service to users is cloud computing. In cloud computing, you have these big data warehouses, but instead of saying we're doing just one big data mining job here, you're saying all of the 10,000 machines in this warehouse are being shared by many, many, many different users. They have paid whoever runs the warehouse, whoever runs the cloud computing facility, to use this facility for their own purposes and we may have hundreds or even thousands of different users each using part of our facility to do whatever it is they're supposed to do. They're paying us money. That's what this whole business model is like. We provide compute services, you pay us money to use our compute services. Now when they pay us money to use our compute services, what they want to do is say, "I'm going to pay you this amount of money and you're going to give me that amount of computing, amount of CPU cycles, that amount of memory, that amount of persistent storage, that amount of network capacity to interact with the internet or whatever other characteristics they need from their computing. Now this is done in cloud computing using what are called service level agreements. When you sign up with a cloud computing facility and give them some money, you are given some guarantees in return saying, "I will on demand whenever you ask for it, provide you with the following set of computing services, this many CPU cycles per second, for example. OK, now, if this is the case and I am the party who is running the cloud computing facility, I want to make sure that all of my agreements are met. I promised you this. I promised him that. I promised this other guy some other set of things. I need to meet all those agreements or I will suffer financial penalties. I'll make less money. So then I want to have a scheduling algorithm that says I'll meet all service level agreements. Whatever it is I promise this guy, I make sure he gets it. Whatever it is I promise the other guy, I make sure he gets it, and so on. So I schedule to ensure that. All right, now, in many areas of operating systems, we have a division between policy and mechanism. Policy says, here's what should happen. Mechanism says, here is the set of things we do, the actual operations we perform to make whatever it is we're doing, scheduling in this case, actually occur. It's desirable generally to divide policy from mechanism in operating systems. The reason being policies change. Sometimes you need to behave one way, sometimes another way. You know, here I am sitting on my laptop computer. If I'm running video, I need to do something that's going to ensure that the process running the video is meeting real time guarantees. If, on the other hand, I am simply reading a PDF, there's no real-time guarantees really required for that. And maybe we want to have other things going on while I'm sitting there absorbing the words I see on the screen and effectively not asking the computer to do anything new. So we want to have a scheduling mechanism in our computer's operating system that will support many, many, many different policies. Whichever policy happens to be right for this computer at this moment is the policy we should use. If it turns out that later another policy is more important, we'd like to be able to switch to that policy, but we aren't going to rewrite code in order to do that. Which means we have to have a mechanism that supports any one of those policies and also supports the ability to switch from one to another on demand. Scheduling has this characteristic. As you should guess from what we've about having these different scheduling goals, we're going to have different scheduling algorithms. Which algorithm we use is going to at least imply what policy we want to use. And thus, we want to make sure that the mechanism can support any one of those policies. So the policy will tell us here is what we should schedule. The mechanism will make sure that whatever it is that we are scheduling-- CPU cores in this case-- actually gets done. So we've chosen job X to run next on core three. How do you make job X run on core three? That's a matter of mechanism. Further, you remember we talked about the scheduler queue had to be kept in order. How do you keep it in order? When do you reorder that queue? How do you make sure that the queue is properly in order? Do you ever go through the queue and change the order? And if so, for reasons other than I just scheduled the guy at the front, if so, how do you change it? So that's also part of the mechanism associated with doing scheduling. So dispatching is the actual process, the actual operations for moving a process onto or off of a core. If it's on the core, it's running. If it's off the core, it's not running. So when we say we want to schedule job x on core 3, that means we need to do a dispatch operation that will put core x on-- put job x on core 3 in such a way that it can start running the instructions properly. There's going to be all kinds of mechanism associated with doing that. OK. But how we chose x, why it was x we put there instead of y, that's policy. That's different. So we are in scheduling, as in many other areas within operating systems, we're going to try to separate out policy and mechanism. The goal here is to say, we've built a body of code that allows us to do certain things mechanically. And because we have built it in a general enough way, We can support many policies of what we want to do with exactly the same set of code, exactly the same mechanisms. OK, now let's talk about different kinds of scheduling that we could perform. There is, as in so many areas in operating systems, a dichotomy between different types of scheduling, preemptive versus non-preemptive. So what's this? So we're going to schedule a piece of work. Now here, of course, we're talking in the context of putting a process onto a core run. When we schedule that piece of work, when we put process x on core 3, what are we going to do? Are we going to let it run until it finishes or blocks? Or are we going to run it for a while, and then whether it wants to or not, we're going to take it off of that core and put process y, for example, on that core? So which of those are we going to do? If we always wait for the process to say I'm done or I'm blocked, we are running a non-preemptive scheduler. If we, the operating system, at various moments, without the process telling us I'm ready to stop or I've got to stop or I can't run any more instructions, without that, the process would like to keep running. If we stop it in its tracks, take it off of the core that it's currently running on and put a different process on that core, that's preemptive. So we have preemptive algorithms and non-preemptive algorithms. What's good about each? Well, let's look at non-preemptive algorithms first. Generally speaking, if you have a non-preemptive algorithm, you change what's being done, what scheduling has to do less often. You only do it when you have to. When effectively, in the case of jobs on cores, process can no longer perform useful instructions on the code it is sitting on. then we will have to do scheduling. So that means we'll do scheduling less often. We can get high throughput this way, because effectively, since we're doing scheduling less often, we are letting jobs run and run and run and run and run on a core. As long as they are running on that core, they aren't blocked, we aren't scheduling something else, there is no overhead from the operating system associated with the CPU instructions on that core. We are doing useful work. We are getting throughput. A lot of throughput is good because the more throughput we get, the more user work we do. Also, generally speaking, non-preemptive algorithms tend to be simpler, simpler than preemptive algorithms. There are fewer complexities involved. There are downsides, of course, to non-preemptive algorithms. We can get very bad response time, response time at the user level. A user may be sitting in front of his keyboard for a long time waiting for the next thing his process gets to do that shows up on his screen because we haven't preempted what's running on any of the cores and he isn't one of them. His job is not running on that core. Also if you think about this, you may very well at some point in your careers as computer scientists have written a job with an infinite loop in it. If you've written a job with an infinite loop, then that process would continue to run and run and run, and it could be running until the very end of your life if nobody says stop. If nobody says don't run anymore because it's an infinite loop. Okay, well, that wouldn't be good. So, we would like that not to happen. With non-preemptive algorithms, unless we are careful, that could happen because we're not going to stop the job until it says I'm ready to stop and in an infinite loop it won't stop. Generally speaking, non-preemptive algorithms have difficulty achieving fairness. They tend to favor whoever happened to have won recently. If you didn't win recently, you're not getting any of the resource you're waiting for, CPU seconds, for example, and that isn't fair. It's also hard to do real-time and priority scheduling in most, though, not quite all circumstances using a non-preemptive algorithm. We will talk about one case for real-time scheduling where a non-preemptive algorithm is absolutely what you want to use, But mostly, it's not going to be good for what you want to do with real time. Okay, the other choice you have is preemptive scheduling. Fine, what's good about that? We can get better response time there. We can ensure that every one of the jobs that's ready to run is running within a few milliseconds of when it becomes ready, and it gets to run for at least a little while, maybe not forever. So if it needs to do just a little bit of work to make progress to show the user who asked for something to be done what happened. We can do that pretty quickly. We can also get better fairness. We can say everybody gets to run for 10 milliseconds, then somebody else gets to run for 10 milliseconds, then somebody else gets to run for 10 milliseconds and so on. And the first guy doesn't get to run again until all the other parties that wanted to run have had their 10 milliseconds. It could be quite fair. We can, in more cases for real-time and priority scheduling, achieve what we want to achieve. We can say, okay, if there's a deadline coming up in real-time or if suddenly a high-priority job becomes available to run, bang, we can do it right now. Downsides of preemptive scheduling are, well, it's more complex. It's more code that you have to do, more things you have to do, more care that has to be taken. And in particular, one thing you must be able to do if you're going to try to do preemptive scheduling is you must be able to point to a running process on a core, stop that process at any given moment, regardless of what that process is doing, and save its state. Now, we've already said in previous classes that all of the state that we have for a process or for any other object in the operating system is represented by a set of bits. To save the state of something that is not changing its bits at the moment, all we have to do is find the full set of bits that represent the state of whatever it is we're trying to save and store it somewhere. And then if we can put those bits back into place later, we can restore the state at a later moment. We can do that with processes clearly. So we have to be able to do that. This means we have to be able to track down every single bit that represents a piece of state of that process that's running on Core 7 and copy it somewhere safely where we can get at it again before we can say, "Okay, now we can move that process off of Core 7 and put a different one onto it." There may be throughput issues here because every time that you interrupt a process that would like to run its next instruction and say, "You're not running the next instruction," we are then going to throw in some overhead we didn't actually absolutely have to have. Because all of this stuff of tracking down all those pieces of the state, all of the bits that represent the state, copying it somewhere else, putting it there in a way that we can restore it later. So I'm going to take some instructions. Those are instructions that are not part of any user process. They are overhead. Overhead in the operating system is never desirable. It's inevitable in some cases. But it's not desirable. So we want to limit the amount of overhead that we have to pay in order to do things in our operating system. Preemptive scheduling adds overhead. And that reduces throughput, because you have a certain number of CPU seconds. If you're spending it doing overhead operations for the operating system, you're not spending those CPU seconds doing user work, doing things that particular processes actually want to have done. All right, so let's talk a little bit more in a little more detail about how we schedule the CPU, or more precisely, one core of the CPU, when we have, as we do most cases nowadays, multiple cores on a CPU. OK, so let's say we create a brand new process. What happens? Well, a brand new process gets put into the ready queue. That's the queue of processes that are ready to run. Remember, it could be everything with the ones that are not ready to run sitting at the end. But let's for the moment just assume that it's only the processes that are ready to run. So it goes into the back of the queue. And as time goes by, it makes its way to the front. And eventually, as we can see here at this stage in the diagram. It is the next process that should be scheduled. Through whatever mechanism, we've decided now is the moment to schedule the next process. So what do we do? Well, we call the dispatcher. The dispatcher is going to say, OK, this is core five that you've decided you want to schedule something on. Here's the guys who schedule. I'm going to do something to save any state required of what's currently on core five to find all of the state of the new process you want to run to get it ready to go. Then we go to a component called the context switcher. This is the one that actually is going to set all of the registers in the core to say here's a proper set of registers values for this new process. And once the context switcher has set that all up, that process is sitting on the CPU, and it is ready to run. In fact, it is running. Once we set the program counter on the CPU core to be proper for this particular process, it's running. It's just doing instructions on that core. And it's doing them and doing them and doing them until one of a few things happens. One thing that can happen is the process can voluntarily yield. It can say, well, I could run the next instruction, but I've been programmed in such a way that every so often I say, give somebody else a turn, let somebody else run. In which case, what's gonna happen? I'll get put back into the ready queue. And sooner or later, it'll be my turn again. Alternately, what might happen is there's a resource request. I've decided I want to send a message. I've decided I want to display something on the screen. I've decided I want to write something into a file. At any rate, this is going to require me to make use of one of the hardware resources on the computer. And that means I'll have to ask the operating system to do that. And that probably means, since these devices are slower than the CPU, that I'm going to be blocked for a significant period of time. So I go to the resource manager, whichever piece of code in the operating system it is that handles the flash drive, the network card, the screen, whatever it may be. And I say, I'd like to do the following thing. And the resource manager will perhaps have to put me in its own scheduling queue if there are a lot of people who want to do something on its resource, or if no one else wants to use it, it has to get started on doing whatever it's going to do. Because the hardware is relatively slow, that'll take a while. So at that point, I'm sitting in the resource manager effectively, my process is, for a long time. Blocked. I cannot run. Sooner or later, the resource manager finishes whatever it's doing and probably gives me whatever I asked for. The resource is granted. I am unblocked by the resource manager and I am put back into the ready queue, ready to run. Another option which is shown at the top is instead of yielding voluntarily, the operating system could have decided, yeah, I know you want to continue running, but I'm not going to run you anymore. I'm going to preempt you. If we have a preemptive algorithm, that could happen as well. And in turn, when you have a preemptive algorithm and you preempt a running process, that process isn't blocked. That process could run. So in that case, the process will also get put into the ready queue and eventually will make its way to the front of the ready queue and will be run again. All right, now, the reason that we choose particular algorithms for scheduling is we want to achieve particular performance goals. So we're going to say a few words about performance here. There are a lot of times we care about performance in operating systems, and we're not going to get into a lot of detail about how we go about measuring and characterizing performance, but there are a few things you should know, a few very general things that apply to scheduling and apply in other areas of performance of computer systems. Scheduling will have a big effect on the performance of your system. Performance has different aspects. There is response time. There is fairness. There is throughput, et cetera. It is not usually possible to optimize all of them for all cases. You can make one very good, perhaps at the cost of another being not so good. Or sometimes all of them look pretty good, But when you get into different modes of what's happening in your computer, suddenly some of them become very bad. So another thing you need to know about performance is all aspects of performance, and in particular scheduling are very, very different in terms of the effects you see when you have a light load on the system or when you have a heavy load. Light load meaning there's not much work to do. Most of the time there's nothing to do. Heavy load meaning you got a whole lot of work to do. Maybe you have more work to do at the moment than you can actually do. You don't have enough resources to do all the work you'd like to do at least for the moment. So, you need to understand a little bit about performance issues in order to understand why we are going to have certain effects by doing certain types of scheduling. Here's some general comments on performance. And this doesn't apply simply to scheduling. This is to all aspects of computer system performance, even aspects that are not about operating systems that are about applications or distributed systems. Generally speaking, if we care about performance, we need to have goals. We need to have a goal saying this is what we want to achieve. And that goal needs to be quantitative and measurable. Quantitative means there needs to be a number that represents how close we are to the goal. Is it a high number and we're very close to the goal? A low number, we're very close to the goal. It doesn't matter whether it's high or low. It just matters that it is a number. not enough to be a number, it must be a number we can effectively measure, that we actually can measure that number and say this actually represents, this number represents what's going on in this computer. Generally speaking, if you want goodness out of a system in any aspect, you have to be able to say this number represents good, that number represents bad, we want to get closer to the number that represents good, further from the number that represents bad. This implies not only do you have to have such numbers, but you have to be able to measure them. You did something to try to move you closer towards the good number. The only way you're going to be able to tell if you have moved closer towards the good number is if you can measure what number you're getting. Is it better than what you had before or is it not better or is it worse? Now, the way we do this is we use what are called metrics. Metrics are a combination of what is it that we're trying to get, throughput, and what are the units of throughput, cycles per second, you know, applied to a particular job, and how do we get them? That's also part of metrics. You don't have a metric if you can't measure it. Okay, so if we're going to do anything to try to improve performance of our system, we have to choose a characteristic saying this is what we want to measure, and we have to find a unit that measures that characteristic, and then we have to have code or something, maybe even hardware, that actually allows us to measure that particular quantity. So that's just briefly the description of this. There's a lot that can be said about how you go about measuring performance. It can be quite complex, very, very, very complex. But let's get back to particularly scheduler performance. How are we going to quantify scheduler performance? Got to choose a metric. Let's think about a metric. Well, throughput. Why not throughput? What kind of throughput metric could we have? How many processes do we complete per second? Processes start, they run, eventually they finish. How many of them do we finish per second? The more we finish per second, perhaps the more throughput we have, except of course, different processes have different run times. So a process could be very, very short. And if we finish a whole lot of very, very short processes, that's kind of different than not finishing a very, very long process. We might actually be getting more work done in the latter case than we are in the former case. Further, the scheduler doesn't control how long it's going to take for a process to run. It controls when the process runs, but not what instructions the process performs, and that determines how long the process runs. So we can schedule well, we can schedule poorly, and we're going to have a limited effect on the throughput. You know, if there's a process, if all of our processes take a very, very long time to run, it doesn't really matter from the point of view of this metric, which one we schedule next. We cannot control how long each process will run. Okay, and also, of course, that doesn't deal with particular characteristics of goodness we might like to have. Let's look at another metric. Delay. That's a good metric. That says I want to do something how long before it's done. And what units might we measure that in? Well, how about milliseconds? We wanted something at this clock time, how many milliseconds before it was done? Which delay? There are various kinds of delays we could have. We could say, for example, I ask this process to start running at time x, it finished running at time y, some number of milliseconds in the future. That's turnaround time. And that is a delay metric that makes sense in certain circumstances. But another delay metric is, well, okay, this is my screen controller, a process that determines how I, a human being, interact with a screen. I click on things, I move the mouse around, I see that stuff displayed on the screen. It's less important for that purpose to say, how long is it between when I start that screen manager process and when the screen manager process ends? I'm only ending the screen manager process when I perhaps log out. That's not all that important. What is important is I typed a key. I typed the E key. Did the E appear on the screen? I moved the mouse to a new location. Did the mouse move the icon on the screen that matches the mouse movement move quickly and steadily? I clicked on something. Did the effect of the click happen apparently immediately? That's getting some response. Now that doesn't mean I have to run my job to completion, it just means I need to provide enough CPU cycles of that process to handle all of these little things that are being done. Typing a letter on a keyboard, following a mouse movement, displaying something on the screen. That's a different type of delay. Now also it's the case that this has a problem from the point of view a scheduler to say, well, some delays are not the scheduler's fault. There is nothing the scheduler can do to improve that delay. So, for example, I want to read a block of data off of the disk. The disk is going to take some amount of time from the hardware perspective to get that block of data. Nothing the scheduler can do is going to reduce that time. So I can schedule well, I can schedule poorly and I still get exactly the same delay on that operation. And if it turns out that the resource is very busy, so it's something where yes, I know this is always going to take at best a millisecond, but it turns out there are 30 other parties in line ahead of me, then no matter what I do, somebody is going to pay for having those 30 parties in line ahead of me and I'm going to get a bigger delay. So if I cannot control the delay, then I cannot optimize using the scheduler. Maybe there are other ways I can optimize, but the scheduler can't do anything about it. Software can't optimize what it doesn't control. Very important lesson in performance. Now here's another important lesson in performance. A typical throughput versus load curve. This isn't about scheduling per se. This is about everything. Everything in computer science, we care about the throughput. How much work do you get done in a unit time? And we also care about load. How much work do you apply to the system? Now systems are real things. There's a real computer sitting in front of me right now. It can do a certain amount of work based on the hardware that is in that computer, based on the CPU, based on the amount of RAM, etc., etc. It can do that amount of work and perhaps on a good day it will do that amount of work, but no matter what I do, it can't do more than that because that's all the hardware is capable of doing. So, as I offer load to the system, I expect that I'm going to get a curve something like this. This is the ideal curve. I offer more load, more work gets done. I offer yet more load, more work gets done. I offer yet more load, more work gets done. I get higher throughput as I offer more load. However, my hardware can only do a certain amount of work. When I get to the amount of work that it can do, if I offer more load, I don't get more work done, because it can only do a certain amount of work, and it's already doing that amount of work. So what I would expect to see with this offered load versus throughput curve is it goes up, up, up linearly until it hits the maximum capacity of the system, the maximum possible capacity right there, in which case it says, "Okay, that's the best I can do. I can keep up with this." Now you note this is marked as being the ideal curve. It's marked as being the ideal curve because we never get that. In real systems, we never see that curve. This is more like what we see. This is a typical curve. As we have low load, low offered load, not a lot of work to do, we approximate the linear increase in throughput. Offer a little bit more, we get a little bit more throughput. A little bit more, we get a little bit more throughput. However, we get to a point way short of our maximum possible capacity where we offer a little bit more load and yeah we get a little bit more throughput but not a linear amount and then somewhere around the maximum possible capacity we're way short of the ideal because we've tailed off from what we're getting by adding more load but once we reach that maximum possible capacity or somewhere around it suddenly as we offer more load we get less throughput less work gets done That's not good. Why did this happen? This happened because scheduling isn't free. Scheduling CPUs isn't free. Scheduling hardware devices isn't free. We run instructions to do it. So whenever we do a dispatch of a process, for example, that takes time. We copied all those bits. The more dispatches we perform, the more overhead we have, the less throughput we get. Less time to run the actual processes. So that means that every time we offer load, we have to say, oh, here's a new piece of load in the case of a scheduler, new job to run. Okay, we're going to have to put that in the scheduler queue, we're going to have to put in the right place in the scheduler queue, we're going to have to save information about where is all of its state when its turn comes. All that's going to take some instructions, that's all overhead, that's going to reduce our throughput. The more often we do it, the more load we offer, the more the throughput gets reduced. So how do we minimize that performance gap between the ideal and the real. Well, first, this is overhead. If we can squeeze instructions out of the cost of performing a dispatch, we'll get less overhead and thus better performance. The other choice is to say, well, let's not do very many dispatches. Let's do this infrequently. Now you can't, depending on exactly what's going on, do all of this infrequently, because sooner or later you have to run a job or that job never gets anything done. Fairness will suffer, for example. But if you can minimize the number of times, then you're going to have less overhead due to dispatching. Now, this is, again, a general type of observation of performance. This happens, for example, in networking in the internet. Okay. Now, throughput, of course, is one important performance metric. Response time is another. Let's look at what happens with response time as you offer more and more load. Now, ideally, what we'd expect to have happen is something like this. More load offered, takes longer for the jobs to get done. But linearly, you know, you add more load, yeah, you got to get in line and you are in line for some period of time and it takes longer before you get your turn. And sooner or later you get your turn and the response time is okay. You've gotten your response. Not wonderful that it keeps increasing this way as you offer more load, but you know, that's the way life is. Except that isn't the way life is. Here's how life really is. For a while, as you offer more and more load, yes, your delay goes up more or less linearly. At some point, it explodes. It goes to infinity. Actually, literally to infinity. That's really bad. Infinite response time is not at all pleasant. Why does this happen? It happens because we're dealing with real systems, by which we mean systems that are actually implemented in hardware. They have hardware limitations that cannot be overlooked, that cannot be improved. You have a certain amount of RAM in your system. You have a certain number of CPU cores capable of doing a number, a total number of CPU instructions per second. You can send and receive a certain number of messages per second on the network equipment that you have in your machine. And that's it. That's all you got. You can't do more than that. Okay, fine. Well, what are you gonna do when load gets to the point where you can't do more than that? Well, you're going to save the work that you were unable to get to right now. So you put in a queue. So all of the work that you can't get to right now because you're busy doing other work gets put in a queue. Fine. What's it really mean to be put into a queue in real terms, in terms of what really happens in a real system? Bits of data are saved somewhere. Where? Well, maybe in RAM, maybe on a disk drive of some kind, something of that nature. They're saved on a piece of hardware you've got available to you. What happens when you use up all of the space you have to save information about what's in the queue? Well, you've fused up all the space that you could use for the queue to hold the information that you've got, and more information comes in. And you haven't yet been able to get anything out of the queue, because you're offering load faster than you were dealing with what's in the queue. Well, you've got to put it somewhere, but there's nowhere to put it. Well, maybe you try to increase the amount of space you have for the queue. Fine, you increase the amount of space. Eventually, if you keep doing that and you keep getting more load, there's no more space to increase. You've taken all of the memory space you can, wherever it is, RAM or disk drive, and there is no more. You can't get any more. At that point, what are you going to do? You got 5 million queued requests. The 5 millionth and first request has come in. You have no room to store it. You can't put it in a queue because there's nowhere to put it in the queue. There's no space to hold it. What you going to do? Well, you can drop it. Or you can take one of the items that's in the queue, throw that away and put this new one in its place. Those are your choices. In either case, what's happened is you've thrown something away. Now, when you throw something away, that means you've forgotten that it even existed. It's not in the queue anymore. If it's not in the queue anymore, the scheduler that uses that queue is never going to find it because it isn't in the queue anymore. That discarded piece of work, that discarded request to do something is going to be handled in infinite amount of time, i.e. never. It will not be done. So whoever wanted that done is going to wait forever for it. Now in real systems, people often say, well, we know this can happen, so we're going to do something to try to deal with the condition that, gee, this isn't ever going to get done because somebody forgot about the fact that it needed to be done. For example, in networking, the TCP protocol will say, well, if I haven't heard about a message being delivered in a certain amount of time, I'll assume that it got dropped somewhere and I'll send it again. You can do that. if you were in a permanent overload condition, then you drop the 5 million in the first request. Now the guy resends it. It's the 5 million second request. You still have 5 million requests sitting ahead of it and no room to put this new one. You're going to drop that one too. So if you have persistent overload, you're going to be having to deal with requests you cannot even remember faster than you can fill any requests. So this has a tendency to lead to very, very, very bad performance when it occurs. We don't want that happening. You can even get in a circumstance where all your computer is doing is dealing with the overload. So you know, you got 5 million things in the queue and something at the head of the queue is supposed to be running and slowly will get pulled out of that queue. But what are you doing? You're getting a request, request, request, request. You cannot even say I'm dropping this request without running instructions. Running those instructions is overhead. If you spend all of your time running the instructions to say I'm dropping, I'm dropping, I'm dropping, I'm dropping, there's no time to run the thing that is eventually going to allow you to move one piece of work out of the queue. You get nothing done. So you end up with low throughput, perhaps even zero throughput if things get bad enough. So we would prefer to have systems experience what's called graceful degradation. What do we mean by that? Well, the situation we're worried about here is your system's overloaded, too much work. When is it overloaded? Well, you have goals. You say, "I want, for example, to achieve a response time of no more than 10 milliseconds per process. Every process gets some compute time within 10 milliseconds." If you don't meet that, then you are overloaded. If you are going to not meet it for a fairly significant period of time. It may be okay to miss it once or twice, but if it's persistent, you've got a problem. So what do you do when you're overloaded? Well, you have options. They aren't great options, but they're better than nothing. One, you continue service, but with degraded performance. I don't manage to get to everybody within 10 milliseconds. It takes me 20 milliseconds, but everybody does get service within 20 milliseconds. That may be possible. I reject some work. I say, "There's no way I can meet my 10 millisecond request if I keep putting things in the queue. So I'm going to stop putting things in the queue so I can continue to meet 10 milliseconds for everybody who is in the queue." And you hope that sooner or later, the overload condition ends. You stop getting all of those new requests coming in. Things settle down. Then you want to, if at all possible, return to ordinary service as quickly as you possibly can. stop dropping work, try to meet the more desirable performance goals. One thing you want to make sure your system doesn't do when it's overloaded is you never want throughput to drop to zero. You always want to be doing some useful user work to have as good a percentage as you can manage of your overall compute resources being devoted to doing actual user work, really getting things done. And you also want to make sure that you don't allow response time to grow without limit, at least for things that you haven't rejected. When you throw something away, of course, it's infinite response time for that poor sucker. But everybody else who's sitting in the queue, we should not be in a situation where their response time grows without limit. Okay. Now, returning to scheduling per se, in particular scheduling algorithms, Let's look at our two options, non-preemptive versus preemptive scheduling. Non-preemptive scheduling, what's that mean? Well, typically that means we're going to put a process onto a CPU core and it's going to run and run and run and it will eventually end or it will yield or it will block or something like that and at that point it's going to not be on that core anymore. We're going to run something else. Our scheduler will choose something else to run on that core. When are we going to want to do that? Well, this is a decent choice if you have a very simple system, which we mean, by which we mean there isn't a lot of complex stuff going on. It's kind of predictable, never really overloaded. It's got limited requirements for every piece of work it needs to do. And another time that this is useful is when you have what are called producer-consumer relationships. This is something where you have one process that produces a piece of work, A second process, once the piece of work has been produced, consumes the piece of work. After it's consumed the piece of work, the producer produces another piece of work. Now in this case, the consumer can't do anything until the producer has produced. The producer can perhaps get a little bit ahead and build up an inventory of a few things waiting for a consumer to be ready to consume the work. But it's not going to go for very long. It's going to say, "Well, I produced all the work I want to produce right now. I got to wait for the consumer to use some of it." in such systems to say, we're going to work in turn. Producer produces, stops. Consumer consumes, stops. Producer produces again, stops. Consumer consumes, stops. Generally speaking, non-preemptive scheduling is likely to give us good throughput. We're going to do a lot of work because we're not going to interrupt the processes that are running user code for any purpose other than to say they can't run anymore. We are expecting processes to either regularly block or to voluntarily yield or to simply end, short processes, they don't ever run for very long. If we have a process that doesn't do one of those things, if it just keeps running and running and running, we're not gonna preempt it, we're not gonna stop it. And if it doesn't stop itself, and if it doesn't stop, do something that blocks it, it's gonna sit there forever, using up our resources. So what non-preemptive algorithms are there? Well, there are a few. First come, first serve, shortest job next. We're not going to talk about shortest job next in this lecture. It's discussed in the readings. And then there are real-time schedulers. And I said before, we don't want non-preemptive for real-time schedulers. But we're going to discuss a particular type of real-time system where we do want non-preemptive schedulers, very much want them. But first, first come, first serve. Now, this is an algorithm that you're probably all somewhat familiar with. You know, a lot of people do their shopping online now, but we still, many of us go to the grocery store and buy a quart of milk. And when you buy a quart of milk at the grocery store, if it's a busy time at the grocery store, you get in line behind other people who are buying whatever it is they wanted at the grocery store. And you stand in the line and you stand in the line, the people ahead of you check out, they pay for their groceries. Gradually, everyone ahead of you has paid for their groceries. You reach the head of the line, you have your groceries scanned, You are given a total that you're supposed to pay, you pay and you go. This is almost always done with first come first serve. The line gets set up and the line maintains in that form until such time as you have been served. If you were the first in line, you get served first. If you were the last in line, you get served last. So in terms of doing this in a computer system, it's very simple. Whenever you have a new job, you put it into the ready queue at the end of the ready queue And you move through the ready queue with other jobs completing before you eventually get to the head of the ready queue. Next time that something is available to schedule, a score becomes free. You get put onto that core and then you run and you run and you run until perhaps you complete, finish checking out your groceries or you yield, which you wouldn't do in a grocery store, but in a computer system is possible. And whenever you have finished whatever you're doing, whoever is next in line gets to run and they too run until they complete or yield. Now, this has highly variable delays, as you may very well have seen if you've gone into a grocery store. If you're in line behind two people who each have three items in a little basket, that's one kind of delay you expect to see. If you're behind two people who have a full basket with 73 different items in the full basket each, it's going to take longer. So this is going to depend very much on how long you stay in line on what is happening ahead of you, which you have no control over typically. Now in this kind of system, one good advantage of this is assuming you don't have infinite loops, everybody eventually gets their turn. Everybody eventually gets to be served. Here's a simple first come first serve example. So let's say we have five jobs. Dispatch order is the order in which they enter the system. So they enter zero, one, two, three, four in that order. Okay, so process 0 is going to take 350 milliseconds. Process 1 will take 125 milliseconds. Process 2, 475. Process 3, 250. And process 4, 75 milliseconds. So, the way this is going to happen is process 0 shows up. It starts running at time 0, runs for 350 milliseconds. At time 350, process 1 gets to take over. It's going to run for 125 milliseconds. It's done at time 475. time 475, process 2 takes over, and so on and so forth. And overall, it's going to take 1275 milliseconds, assuming no overhead, to run all five processes to completion. So it takes 1275 in duration, but the average wait time is nearly 600 milliseconds. And the reason for that, of course, is because there's some long processes near the front of this set processes it's running. In particular, process zero is going to contribute 350 milliseconds of delay to all four of the other processes. This leads to a whole lot of waiting if you have somebody towards the head of the queue who's slow. When would we use first come first serve? Well, it's very simple, which is nice. Simple is always good. It doesn't give you very good average response time. It all depends on how long everybody runs and so forth. If you have a lot of processes that all have very, very short times and there's nobody who has a very, very long time, then probably you're going to get reasonable average response time, assuming the load isn't too high. So it makes the most sense when, first, you don't care about response time. Batch processing is an example of this. We don't do batch processing much anymore, But batch processing was where you said, "Okay, I got a big job to run. A lot of people have big jobs to run. We're all sharing the hardware. Let's just throw our big jobs at the hardware at the end of the workday and say, 'Hey, when I come in tomorrow morning, I want my job to be done. I don't care whether it's done at 9 p.m., 1 a.m., 5 a.m. or whatever. As long as when I come in at 8 o'clock in the morning, there it is, it's done.'" So in that case, of course, the computing service can say, "I'm not going to do anything with preemption here. I'm going to start one job and I'm going to run it to completion. Then I'll do the next one, run it to completion. Then I'll do the next one, run it to completion." Less overhead, higher throughput. So this would be a good use for first come, first serve. If overhead is very expensive compared to what you're doing, in particular, if you have a piece of hardware that is especially expensive, so let's say you have something like a big supercomputer and it's cost you 50 million dollars to buy the big supercomputer. You want to get the most work out of that supercomputer you can, which means you want the least overhead you possibly can, which means maybe first come first serve might be a good idea. It's also the case that there are other kinds of systems at the far end of things, at the far different end of these big expensive systems, such as the computers that are in set boxes. Now in the case of these kinds of things, they only do a very limited amount of stuff. They aren't really general purpose systems that run arbitrary computation. They do a little bit of code that says let's change the channel, a little bit of code that says let's pause this stream of video, or things like that. Not very much stuff. And nothing they do is going to require a whole lot of processing before it reaches the end of whatever it's supposed to do. So in this case, The computations are brief and there's perhaps a natural producer consumer relationship. So you can have that good benefit where nobody runs for very long before somebody else gets a chance. Okay. Now, there's another type of non-preemptive scheduler that we should talk about. This one's very different. Real-time schedulers. Now, as I already said a couple of times, in most cases, real-time schedulers are probably going to be preemptive, but there's an important, a vitally important exception. For certain types of systems that we control with computers, it's really important things happen at particular times. So industrial control systems. So you got a assembly line and the assembly line has this robot that's whacking on the widgets as it goes by. If it whacks on the widgets properly, great, then you get a bunch of useful widgets. If it doesn't whack on the widget at the right time, then you don't. Okay. So you want to make sure that you send the command to the robot saying whack the widget exactly at the right moment. It can't be early. It can't be late. It's got to be exactly the right moment. Now for these kinds of systems, you have actually a choice in real time. There are two types of real time, hard real time, soft real time. Talk about hard real time now, because that's where we're going to worry about non-preemptive scheduling. Hard real-time systems means we're serious. We're very, very, very serious. When we say this is a deadline, we mean it's a dead line. You do not meet it. Very, very bad things happen. Not just, "Oh, you got a widget you got to throw away because it wasn't stamped." You got something which resulted in, oh, let's say, the meltdown of a nuclear power plant. That would be bad. If you don't meet this deadline to pull the rods out of the core under the following circumstances, you get a meltdown. We cannot afford that. So we must, if we're going to use a computing system to control this kind of thing, which we probably have to do, we must make sure we meet our deadlines no matter what, regardless of what happens. In all circumstances, we meet our deadlines, period. Hard real time. How can we do that? Well, we're going to have to be careful. We're going to have to be really careful. And it turns out that their time to be really careful with such systems is long before we start running them. The time to be really careful of them is when we start designing them. So we're going to say for such systems. Okay, here's the following CPU we've got. Here are all of the other resources, hardware resources associated. Here is the complete set of every single thing we are going to do in this system, every one of them. And we're going to figure out, based on the hardware we've got, based on the code we intend to build, how long will it take to do operation one? How long will it take to do operation two? How long will it take to do operation three? We will also figure out, What is the complete set of orders in which it is acceptable to do operations? We can do it in the order 1, 2, 3, never 1, 3, 2. We can do it, you know, operation 75 will only occur under the following circumstances, and it will never interfere with what's happening in operation 60 through 64. Once we know exactly all of the possible things that can happen, we then say, okay, here are all the possible things that can happen and the possible orders in which they can happen. Let us figure out, do we have the necessary computing resources to always meet a deadline? Will we always do Operation 87 in time so that Operation 87 does not cause a nuclear power plant to melt down because it wasn't performed in time? Will there ever be any circumstance in which we don't get operation 87 done fast enough? The answer must be no. How can it be no? Well, we have to understand every possibility, everything that could happen in our computing system. The simpler it is, the simpler the set of things we have to do, the easier that analysis will be. The more complicated, the more complicated the analysis will be. But effectively what we're going to do is we're going to work out ahead of time all of the deadlines for every single thing that's going to happen in our system. And we're going to say, here's the order in which we're going to do things. We're always going to do this one, then this one, then this one, then this one, then this one. Never anything else. And our scheduler then has a very simple job. It's got a list of the order in which things could happen. We've already told this is the order in which things should happen. Do it. Just make it happen. Now, in order to make this feasible, possible, we really have to understand what's going on in the code. We have to have a very, very deep understanding of all of our code. We have to say, this is the number of CPU instructions this is going to perform. Here is the speed at which that set of CPU instructions will be performed. We know it's going to take 72 microseconds to do this set of instructions. It's always going to take 72 microseconds. It's never going to take 70. It's never going to take 75. It's always going to take 72. We know that because we've measured everything. We've worked out everything. exactly which instructions we're going to perform to do this operation. This means we have to be very careful not to have non-deterministic timings. Non-deterministic means we cannot predict that it's going to be exactly this. It's always going to be 72, not sometimes 74 and sometimes 76, always 72. Okay, so if we are able to do that and if we are able to guarantee that we never, ever have anything happen at a non-deterministic speed, then we can set up these schedules. And what does that mean? Well, that means we have to be able to predict what's going to happen in our computer. How can we do that? If we don't have interrupts in the computer, if we've turned off all the interrupts in the computer, and if we know exactly what code we're going to perform, and if we've carefully measured how long will that code take on the hardware that we have available to us, then we can know this is how long it's going to be performing and it's never going to have a non-deterministic timing. Of course, the scheduler must be non-preemptive because things like preempting a job, well, who knows how long that will take? That tends to introduce non-determinism. We must not have it. So what are we going to do? We set up the predefined schedule. We work out this is exactly what's going to happen. It's always going to happen in this order, never ever in any other one. If we fail to do this, if we don't do it right, we melt down. Now that's hard real-time scheduling. This is not a suitable thing to do when you say, "I'm going to play this video off of YouTube." Who knows how long that's going to take? With soft real-time schedulers, then we would like to meet our deadlines. If we don't meet our deadlines, we are going to get undesirable results. We're going to get garbled speech if we're trying to do a phone call over Zoom. We're going to get video which has blocks of colors in the middle and jumps from one thing to another and is essentially unwatchable. The game is going to stall at particular points and we aren't going to be able to play it effectively. That's not good, but it's not like nuclear power plants melting down. So what we'd like to do is say we're probably going to meet all our deadlines, but occasionally we may fail to meet a deadline. Maybe it's going to come late, maybe we're just not going to be able to do it in a reasonable amount of time at all and we'll just have to forget about doing that piece of work. So in that case, our scheduler has a different goal than in the hard real-time systems. The goal is minimize the amount of deadlines you miss or perhaps minimize the total lateness of all the deadlines that you miss. The expectation is, well, that's a good goal to shoot for, we don't expect perfection. Now we may have different classes of deadlines within what we do. So for example, in some video formats, we are going to show multiple frames per second. Some frames are key frames. We really have to show those. Other frames are going to be deltas off the key. If we miss a delta off the key, that may not be that bad. If we miss displaying a key frame, that could be quite bad. So we want to make sure we hit the deadlines on the key frames and we can be a little bit less rigorous about the others. Now if we do this, we're not going to need nearly the level of analysis we needed for heard real time. And also, it's not quite vital to ensure that tasks run to completion to meet their deadlines. We can say, "Okay, we don't think we're going to meet the deadline on this task that we've started. Stop it." Or, "Here's the queue of things. It doesn't look like we can do everything that's in the queue by their deadlines. Let's throw some things out of the queue." Or, "This one's going to be a little late. We know that's undesirable, but we can do it a little bit late. It won't be perfect, but that'll allow us to eventually catch up and meet our goals. So what are we going to do in a soft real-time system when we can't meet a deadline? Well, it depends on what we're doing. We might just say, "This is a piece of work that we can't meet the deadline for. Well, forget about it. Let's not put a whole lot of work into this when we know we're not going to meet the deadline on it." We might say, "Okay, we're going to lag a little bit. We're going to show everything. We're going to do every piece of work. But this may mean that our video slows down to a crawl. There are periods in the video where it stalls and no movement occurs for a while or whatever. Not desirable. But maybe if we let it fall behind for a little while, we can catch up later. Maybe we're going to say, OK, in order to meet these jobs that we currently have in the queue that we really need to meet, there's something else that's further along in the queue that's important, throw that one out. For each kind of system that we're working with, it's going to be very well defined. Now, what kind of algorithm are we going to use for soft real-time? A common one is earliest deadline first. It makes a lot of sense. There are things you have to do in particular real-times on the real-time real-world clock. Order them by when you have to have them done. And we will have a deadline associated with it. We'll be keeping an eye on what the real-time clock is. We'll say, are we going to meet all of deadlines because we probably have an estimate of how long each one's going to take. Sort our queue by these deadlines and whenever a job completes, pick off the first one. And maybe we are going to occasionally have to preempt one that is so that we can meet a more important deadline further in the queue. Certainly we are going to have to throw out the ones that are in the queue that we know we can't make the deadline for because the deadline's already passed. And the goal here is typically-- not necessarily, but typically-- minimize total lateness. Try to meet as many of your goals as quickly as you can. And this may be a preemptive algorithm. Let's talk about preemptive algorithms. So this is, again, we're talking here in the context of CPU scheduling. For different types of scheduling, such as scheduling a network card, preemption may not make any sense. But here, it does in CPU scheduling. So we've chosen a thread or a process that's going to run. going to run for a while. How long is it going to run? Well, it could run until it yields, or it could run until the operating system chooses to interrupt it, one or the other. Whenever that happens, whenever either one of those happens, yields blocks, operating system interrupts it, we'll choose another, we, the scheduler, will choose another thread or process to run in its place. We'll take the one that was running on that core off the core, put this one onto the core. Typically, when we've interrupted somebody this way, or they've yielded, they're still ready to run. They could run. It's just they're not getting their turn on the core that they were running on a moment ago. They're sitting in the ready queue waiting for their turn to come again. Okay, so there are implications of forcing preemption. When it isn't a yield, when the process hasn't blocked, it's just running along, it would love to run its next instruction, but we're going to interrupt it for the operating system. We do not have a good idea, from the perspective of the operating system, of when is a good time to tell a process stop. We know that we can force a process to stop after every instruction. We know it'll complete the instruction because the instructions are atomic in the hardware, but we don't know if this is, relatively speaking, a convenient time for it to stop or an inconvenient time for it to stop. And we need to be able to make this happen whenever it is important to happen. So it may be that its turn has been used up. It was supposed to run for a certain amount of time. It's run for that amount of time. It may be that something more important that we have to work with is turned up. It may be that there was a hardware interrupt that we have to deal with, and we need to use the core to deal with that hardware interrupt. There are various reasons we're going to have to stop this process from running. This doesn't This means that the process may not be in the best possible condition for SOPing. However, it's always in a condition where it's got a set of bits. And it's always being stopped after an instruction has been performed. So the set of bits is consistent with the last instruction having been performed and the next instruction not having been performed. So if we can make sure we have all of those bits in the state saved, we can restore the state later and start up again after any instruction that we completed. Now, once we've done this, once we have this built, we can do fair share scheduling. We can make sure that everybody gets more or less the same amount of computing resources. The cost of doing this is we are going to have gratuitous context switches. This means context switches that are not dependent on the behavior of individual processes. This process wanted to run, but we didn't let it run. We forced the context switch instead. Context switches are expensive in terms of performance, for reasons we'll discuss. They're overhead. They will impact the overall performance of the processes that are running on the computer, impact them negatively on the whole. From the point of view of fairness, from the point of view of meeting important things, from the point of view of not making people wait too long, they are good. But from the point of view of getting a lot of work done, higher throughput, they are less good. And there are also the possibilities of resource sharing problems, problems that are related to synchronization-- and we'll be discussing synchronization in future classes-- can arise because you're doing a lot of context switching. This process thought it was going to continue to do instruction x, x plus 1, x plus 2, x plus 3. But after it's done x and x plus 1, it gets interrupted without its choice, without being told, is now a good time to interrupt? And maybe that will result in something that means x plus 2 isn't going to run quite the way he expected once he runs again. So how do we implement preemption? If we want to say this process thinks it should keep running, but we, the operating system, do not want it to keep running, we need to stop it, how are we going to do that? Well, we have to have some way of getting control away from the process. In order to stop it to run somebody else, we're going to have to run some CPU instructions ourselves. How are we going to do that? If you had a single core machine, how could you possibly do that? Because after all, here's this process sitting on the single core machine running, running, running, running, running. It's not stopping. It's not yielding. It's not blocking. How can you run other instructions? Interrupts. Interrupts, remember, are things that cause the CPU to do something other than what the next program counter value indicates it should do. And in particular, interrupts tend to happen because of hardware. Hardware sends a signal on a bus saying, "I would like to interrupt you because something has happened on this piece of hardware." A message has arrived on the network interface. A block of data that you requested to be read is available from the flash drive. Somebody has hit a key on the keyboard. That's a kind of interrupt. Now, once an interrupt has occurred, that means we're going to switch into the operating system, just as if it were a system call, and run operating system code. Now, we would, of course, run operating system code that would deal with a particular interrupt, such as, oh, gee, a message has arrived. What are we going to do with the message? But it also allows us to do other things like say, gee, would now be a good time to schedule a different process. Now, if we have multiple cores, it's a little bit more complex. But we typically do not, in operating systems, say one of these cores is the operating system core. That'd be kind of wasteful. Instead, what we do is we say, all the cores are running user code. And we will deal with interrupts in some expected way. But it's going to have to use one of the cores. And we'll interrupt some process that's running on one of those cores. So if we get an interrupt, what we probably are going to do in most cases is do something in the operating system once we've trapped to the operating system related to handling the interrupt, doing whatever type of work the operating system needs to do to deal with that interrupt. But also, we are going to say, oh, yeah, is now a good time to do some scheduling? Maybe it is. Maybe it isn't. If it isn't, we just return the job that was running to the process, to the core. If on the other hand, this is a good time to do scheduling, well, then we are going to choose to run somebody else on that core. We'll pull this job off and put a new job on. And there are various reasons we might do that. It might be priority-based. Higher priority processes run. A process that was blocked before that should be ahead of the scheduler queue, it's not blocked anymore, perhaps because of the interrupt. Current process has a priority, but its priority was when we scheduled it. No longer the most important thing to run. So what do we do? The scheduler then just, because it's interrupted, whatever was running, goes to the scheduler queue, that queue of processes, chooses one from the top, puts that onto the core. Of course, having taken the old process off the core first. It would have had to do that anyway to run code itself. The operating system must run code just like any process must. And therefore, if it's going to to handle the interrupt, which means it must have the ability to run code on a core, which means that whatever was running on that core before has to be moved off as part of handling the interrupt. Now, in particular, if we know we are going to do preemptive scheduling, which we do on most modern computers, we're going to make use of a feature called a clock interrupt. Modern processors built into the CPU have circuitry that contains a clock. This is circuitry that automatically, without any instructions being run anywhere in the CPU. Just this circuitry does this. It goes click, click, click, click, click. It doesn't do anything else except that it can be told, here is a interrupt I would like you to generate. I would like you to generate, for example, an interrupt 10 milliseconds from now. And it'll remember that it's supposed to generate an interrupt at that particular moment. And it goes click, click, click, click, click. And once that moment arrives, it generates an interrupt. Now, this interrupt is like other hardware interrupts. It goes to the CPU. The CPU says, ah, it's an interrupt. It then goes through that whole mechanism, similar to the trap that we saw in the previous class. It says, oh, what type of trap is this? Well, this is a clock interrupt. Let's go to the clock interrupt code. It goes to the clock interrupt code. And what's it going to do when it gets to the clock interrupt code? If we're running a preemptive scheduler, it's going to say, time to schedule. So it's going to look at whatever process is running on a particular core. Maybe this interrupt was set for core one. And it'll say, okay, he's had his chance. He's had his 10 milliseconds. We are now going to consider running somebody else on core one. So we've stopped that guy in his tracks. We are going to go ahead and start looking through our queue to see if some other process should be run in preference to the one we just stopped. If so, we will dispatch him. We'll do a context switch to put him on core one. The guy who used to be running, he'll go back to the ready queue. Sooner or later, his turn will come again. Now this has a number of advantages. Of course, it allows us to do fair share scheduling. It also deals in an effective way with infinite loops, because now the guy is in an infinite loop. He'll get to run for some period of time, but sooner or later, the clock interrupt occurs. He stops running. He gets put into the ready queue. Everybody else will get a turn before we go back to his infinite loop. We're still running an infinite loop in this process, which is unfortunate, but it is not hogging the entire core anymore. It's getting a share at most. So this is actually the key technology for preemptive scheduling. You gotta have clock interrupts if you really wanna have effective preemptive schedule, which means that all the modern general purpose CPUs have this clock interrupt capability. Okay, so given that you are every so often gonna get one of these clock interrupts and your operating system is gonna get invoked and it's going to say, oh, I can do some scheduling, what algorithm should it use? Round Robin is one choice. So if our goal in our scheduling algorithm is to say we want all the processes running on our computer to get a fair equal share, more or less equal share, of the CPU resources, Round Robin is one choice for doing that. So what do we do in Round Robin? We have what's called a time slice. A time slice is a period of time that a process to run before a clock interrupt stops it. Typically, everybody will get the same time slice. This will be something that's chosen as part of the scheduler algorithm. For example, we could say everybody has a 10 millisecond time slice. What this means is when we do scheduling of a process, we put a process onto a core, we are in addition to doing all the work to get the process running on that core, we're going to send a signal to the clock saying, I want to interrupt for that core in 10 milliseconds. Okay, so what will happen then is the process will start running. It'll run if it doesn't block, if it doesn't yield, if it doesn't end. It'll run for 10 milliseconds. Then the interrupt occurs. At that point, the scheduler will get invoked and will say he's finished his 10 millisecond time slice. It's somebody else's turn. We'll go to the scheduler queue. We'll choose whoever is next on the scheduler queue. We will put that other process onto the core that just got the clock interrupt. The process that was on that core will get put at the end of the scheduler queue in a ready state, not blocked. And we will go ahead running the new process on the core that received the clock interrupt. It too will have a 10 millisecond time slice. So we'll set another clock interrupt for 10 milliseconds, and it'll run for its 10 milliseconds. The clock interrupt will occur. A third process will get put there, and gradually we will work through everybody getting a turn, a 10 millisecond turn at most, to run before anybody else gets to run a second time. So, of course, it may be that something else happens. So, for example, you had your process put onto a core, it runs for two or three milliseconds, and then it does a read operation on a file. It's going to block. Okay, that'll block, it'll get blocked, and we'll put somebody else on the core. So eventually, somebody who has been kicked off of the core because of this time slice issue-- not an error, not a bug, preferably not something that's even visible to the users, or for that matter, to the process-- sooner or later, everybody is going to get stopped and is going to get stopped in their tracks. But they will get started again without even noticing that it happened. So round robin means we do it in turn. If you have 12 processes-- 1, 2, 3, 4, 5, 6, 7, 8, 9, 12. 1, 2, 3, 4, 5, 6, 7, 8, 9, 12. 1, any of these processes as quickly as we could have finished them, because a guy is just getting going, running through his 10 milliseconds. He'd like to keep going, but he can't because a clock interrupt occurs. Now, if you have interactive processes, in particular, processes that typically are not going to have 10 milliseconds of processing before they have their next I/O event, before they are saying, "Move this three pixels on the screen and display it," or, you know, "Follow the mouse movement," get an interrupt on that. That's going to be a big win. The downside is you're going to get a lot more context switches, a whole lot more context switches. Every context switch is overhead, more overhead. And a good benefit I've already mentioned is that these runaway processes that are not behaving responsibly or in infinite loops or very, very, very long loops, they do less harm. They take one nth of the time if there are n processes running instead of the entire time. So you're going to get interrupted by round robin scheduling, your process will, if your time slice expires, but also if you block for IO or for anything else, or you yield. So in that case, the scheduler hasn't halted them. They've just been unable to finish their time slice before they have to stop. So some percentage of the time, whenever that happens, it's actually behaving quite a lot like FIFO. Let's take a look at an example of this. say we have the same set of five processes that we saw for FIFO, but now we're doing round robin and we have a 50 millisecond time slice. You may hear the term quantum. Quantum is in the context of operating system scheduling used in the same sense as time slice. Means the same thing. Okay, so here we have these five processes, zero through four, they each have the same length they did with FIFO. What's going to happen? Well, at time zero, we're going to start running process 0 and it'll run for 50 milliseconds. Then at time 50, we're going to stop process 0, we'll start running process 1. It will run for 50 milliseconds. At time 100, we'll stop process 1, we'll start running process 2. It will run for 50 milliseconds. At time 150, process 2 has finished its 50 millisecond time slice. Process 3 will start running. It too will run for 50 milliseconds. At time 200, process 3 stops, process 4 starts running for 50 milliseconds. It ends at time 200. Then process 0 starts again, because it's round robin. We're going back to the beginning. Process 0 will then say, okay, fine, it's time 250, I get a 50 millisecond time quantum, starts running for 50 milliseconds, it gets interrupted at 250. And we see the other processes all running for their quantum of 50 milliseconds, except when we get to process four at time 400, its time slice would say you can run till 450, but it's already run for 50 milliseconds in the first round here, and only has 25 more milliseconds to go before it completes. So it'll just run for 25 milliseconds. Okay, which means that in the third round, process zero starts at time 475. Process one completes during this round. So we see these other three processes gradually getting hold of the processor every 50 milliseconds. Eventually, of course, every one of these other processes completes. Process zero is completed there. Process two is completed there. The only thing left, process three is completed, the only thing left is process two. Of course, now since it's the only thing there, it's going to keep running until it completes. Okay, so when did things finish? Well, here's when things finished. Process four, that very short process, only 75 milliseconds of time to run, finished at time 475. Process one at time 550. Process three at time 900 and so on. Last process, we're assuming here zero overhead, which isn't true, but easier for computation. Last process finished at time 1275. How many context switches? Seven, three, ten, five, two. Those are the ones for each process. So we finished at time 1275. We had 27 context switches. How long did we wait on average? 100 milliseconds. That's how long we waited on average to get something done. That's not how long you waited at the worst, but on average, that's how long you waited. When was the first process completed? 475 milliseconds. That short process, process four. Comparing them, comparing this example to the same set of processes under FIFO. Under FIFO, there were five context switches, one for each process. Here, 27. That's more than times as much overhead. When was the first job completed? It was completed at 475 milliseconds. First job in FIFO was completed at 350. That was quicker. Average waiting time, 100 milliseconds for round robin, 595 for FIFO. That meant that in FIFO, some processes had to wait a very, very long time, nearly six times as long before they got a chance to do anything. Okay, now if you're going to do something like this, whether it's round robin or one of these other preemptive algorithms that uses the clock to determine when to stop things, you need to have a time slice. You need to choose the time slice. It can be long, it can be short. What should it be? How much overhead you're going to have will depend very much on how many context switches you have. How many context switches you have will depend in part on how long your time slice is. Shorter the time slice, the more context switches you're going to get. So if you have long time slices, you're not going to have many context switches. So that's great for throughput and utilization of your CPU. You're going to spend more time running user code. But on the other hand, you're going to have worse response time because everybody is going to have to wait for the clocks of all of the other processes that are ahead of it to run out before they get a chance to run. So how do you balance it? Well, one way of looking at this is to say, how expensive is a context switch? Is it really that bad? What are the costs I'm paying when I have a context switch? Well, first of all, you're going to have to enter the operating system. You were running user code, you had an interrupt. We go through that process we saw in the last lecture when we were talking about traps. All of that stuff, some of which is running a significant amount of code, is going to happen simply because you entered the operating system. You were in user code, now you're in the operating system. Then you're going to, of course, have to actually call the scheduler. Now, the scheduler is going to only have to look at the head of a queue, which might not be too long, and maybe adjust some other things in the queue, a few pointer changes perhaps, or depending on how you do your scheduler, it could be more complex than that. At any rate, it's going to run some code. Then you're going to have to say I have to save the entire context, whatever it might be, of the old process somewhere, and I'm going to have to take the current context of the process I would like to run and put that onto the core that I'm about to run it on. I'm to have to fiddle with their stacks. I'm going to have to do something about saying this process is no longer there. It's now over there. There's going to be code associated with that. Also, of course, there are going to be addresses. We talked a little bit, we'll be talking in the next few classes more about memory management. Every process has what's called an address space. You're going to have to say, we used to be using the address space of the old process. Now we're or using the address space of the new process. As we will see, switching from one address space to another has some fairly serious costs associated with it. Another thing that we're going to have to do is modern CPUs rely very heavily on caching. They get their speed advantage by saving a lot of information in an on-chip cache. So in many cases, even though you think you're going out to the RAM, you're not going out to the RAM at all. using cached information that's on the chip. This is a tremendous time savings, really makes things run very, very much faster. Now it's caching. This implies that most of what's in there is going to be stuff you used recently. As long as a process is running, a particular process is running, then it will have built up information about what it did recently and that'll be in the cache. When you switch to another process, everything that's currently in the cache is no longer relevant to the process that you just put onto that CPU. It's all relevant to the old process that you took off the CPU. So you will no longer be getting much benefit from the cache when you switch to a new process. That turns out to be the most important performance cost that we see on modern computers from context switches. Turns out to be a very, very big deal. Okay, now round robin is one way of doing scheduling with preemptive algorithms, but there are others that are also quite important. One, for example, is priority scheduling. This is something where you say some processes are simply more important than others. And generally, the more important processes should get a higher priority in making use of the CPU. When they're ready to run, we should be more likely to run them than we are to run the low priority processes. Okay, so how are you going to do that? How would you do your scheduling algorithm there? The obvious thing to do is to say, I will create a priority number for every process that's running. Maybe a high number is good and a low number is bad. I can do it the other way around. Low number is good and high number is bad. It doesn't matter. At any rate, there is a good number and there is a less good number, a top of the range, a bottom of the range. And when I have a bunch of processes I could run, presumably I'm going to give preference to the ones that have the higher, better number. Okay. Now, if this is non-preemptive, it's not a very exciting thing to do. It's just, you order your Q by priority. That's all there is to it. If it's preemptive, then it's a little bit different because what's going to happen in preemptive systems is you are going to say, "I'm running a process at priority X." A brand new process suddenly shows up. It's got a better priority the next. What am I going to do? Since it's a preemptive scheduler and you should be running the higher priority process before the lower priority process, maybe I want to preempt the one that's running with the lower priority and replace it with a higher priority one. For example, we're going to use the same set of five processes we've been using before. Let's say that at some point we are running, there are only three of them that have shown up so far, Processes 0, 1, and 2. They're the same run times as we've always been discussing. We've assigned priorities to the various processes. Process 0 has priority 10, process 1 has priority 30, process 2 has priority 40. We're working with a system where higher is better. Higher priority processes should get better treatment than lower priority processes. So if we're doing a pure priority scheduling and this is what we have, at time 0, what are we going to do? Well, we're going to say, okay, 40, best priority we got, highest priority we got. Run that guy. So we would run process 2. Now let's say that process 3 suddenly pops up. Brand new process or whatever. Process 3 has priority 20. Well, so what? 20 is less than 40, so we're not going to change which one we're running. So we keep running process 2. Then process 4 shows up. Process 4 has priority 50. 50 is better than 40. We will switch from running two to running process four. So we run that for a while and it's a short one remember, so it runs for a short period of time. Probably it's going to run to completion. So it completes. We don't need to run it anymore, so it's out of the scheduling queue effectively. Now we're going to say, okay, who's got the highest priority? Oh, process two still has the highest priority. So we'll switch back to process two. We'll run him until he's finished, which would be at time 550. Now, if we were doing any kind of priority scheduling system, there is an issue we have to keep in mind. It's called starvation. What's this mean? What if you're a low priority process? If you're a low priority process in pure first come, first serve, not run with priorities, just the order in which you showed up, sooner or later you get to run when you get to the head of the queue. If it's round robin, you'll get a turn sooner or later. What if it's a priority scheduling system like the one we just discussed. Well, then we have an issue. If you are a low priority process and new processes keep popping up, what if they always have a higher priority than you do? Are you ever going to get to run? Is it ever going to be the case that with your low priority, you are the thing chosen by the priority scheduler as the next thing to run? If it is not the case, is that really what we intended when we said we were doing priority scheduling? Did we really say that you're such a low priority that we don't give a damn if you ever run? You never run, so what, who cares? Sit there forever waiting, we got more important things to do. Probably not quite what was intended. So, typically in priority systems, we do something to deal with starvation. The way we do it is we adjust priorities. So one thing we do is we say, well, if a process has been running for a very long time, probably because it had a high priority, let's temporarily lower its priority. Alternately, we can say, well, if this process has been sitting in the scheduler queue for a very long time, hasn't ever gotten to run because it has a low priority, how about we temporarily increase its priority to make it more likely that it will run? Typically with these temporary changes in priorities, they disappear after a while so that we go back to the original priority scheme And we are likely to remain in the realm of doing what was done by priority order, but without full starvation. The low priority processes will still get less desirable treatment, but they will get a chance to run every so often. Okay, now let's talk about a somewhat more complex priority preemptive scheduling algorithm, multi-level feedback queue called MLFQ. Now, when we were talking about round robin, we said, okay, everybody's got a time slice. Everybody has the same time slice. Well, it turns out that isn't exactly ideal. So consider, for example, an interactive process. This is one where you're getting interrupts all the time. People typing on the keyboard, moving mice around, that kind of thing. Chances are pretty good that you are not going to spend very much time processing before you have the next interrupt. The next time you have to do something with the hardware, update the screen, read the mouse, whatever it may be. In which case, you really don't need a very long time slice because you're probably not going to use it anyway. On the other hand, let's say that you are a heavy duty computing job. You're doing analysis of a big chunk of data to build an AI model. You're probably not going to be doing a lot of IO. You're going to be doing a lot of processing. Now in that case, you really don't have an interactive need exactly. I mean, you're not getting anything out of this process until you've completed the full building of the model. So it's not very helpful to say, every few milliseconds, I get a chance to deal with user input, because there is no user input in such processes. All right, so in that case, we don't really need to say this guy gets an interrupt every so often to get fairer behavior. But we do need to make sure he gets a good deal of processing when he runs. So what we'll do here in an MLF queue system is we will create multiple different queues. Instead of having one process queue, we'll have several. Each queue will have associated with it some particular quantum, some particular time slice length. And we will say, if you are the kind of job that finishes very, very quickly and needs to wait for other stuff to happen before you can go on, a lot of blocking. We're going to give you a short time quantum, put you in the short time quantum queue. If, on the other hand, you're a job that can use all the processing time it can get, but we really don't need to worry about interactive issues, we'll put you in the long queue. We will run things out of the short queue a lot more often than things out of the long queue. But when something runs out of the long queue, it's going to run for a long time. OK. We may have multiple queues, maybe not two, maybe three or five, whatever it may be. We expect that under many circumstances, each queue will contain several processes. What are we going to do about that? Round robin within the queue. So each queue is going to have round robin done within it. Now an obvious question here is, okay, somebody creates a brand new job. They start running a new process. What queue do I put it in? I don't know when I've created a new process whether it's going to be interactive or non-interactive, whether it should go in the short-time quantum queue or the long-time quantum queue. Now, if it's in the short-time quantum queue and it's one of these things that keeps running and running and running, then it's going to generate a whole lot of interrupts that we really didn't need. You know, the short-time quantum queue jobs that are in the short-time queue, they're probably going to block anyway, so they would have generated that interrupt, that context switch, regardless of how long the quantum was. The long one, though, if it's sitting in there, it's going to generate a context switch when it really didn't need to. So we don't want that there. But on the other hand, if we put one of these short-time quantum things into the long quantum queue, and we give the long quantum queue lower priority so we don't do things in there nearly as often as we do for the short, then we may not even get around to running the guy we just put in this low queue until he's been sitting there a long time, in which case you won't get good interactive response. So what do we do? Generally, what we do is we say, "Okay, fine. We don't know where this guy belongs. Let's put him in the short queue." Now maybe that was the right thing to do. Maybe that was the wrong thing to do. If it's the right thing to do, then what's going to happen? We're going to see him fail to fill out his time quantum very often. He's mostly going to block before he uses up all his time. Oh, great. He's in the right queue. On the other hand, if he's one of these long running jobs, every time we run this guy, we have to interrupt him without him blocking. Gee, he's in the wrong queue. If he's in the wrong queue, move him to a lower queue and keep doing that. But of course, what happens if we make a mistake or if the behavior of the process changes? It started off doing a whole lot of work to build up a big data area full of stuff, and that was all processing, no interactive. But now it's going to be interactive. If we've shoved him down to the low Q, he's not going to get good interactive behavior. So what do we do? Also, there's the fact that in most of these systems, sometimes they work by saying, "This percentage of the CPU will go to Q1, this percentage to Q2, this percentage to Q3," and so on. But in other cases, they say, we're going to deal with everybody who's in the short queue. And as long as there's anybody in the short queue, we're not even looking at the other queues. In that case, what are we going to do to make sure that the people who are down there in the low queues aren't starved, just as with priority scheduling? They're effectively in low priority queues. And if we're going to use pure priority scheduling, people in the high priority queue will always be scheduled, and the people in the low priority queues might never be scheduled. What do we do about that? Every so often, bump people up a queue. Move them up to a higher queue. So here's how it might work if we have, let's say, three queues, a high, medium, low. So here in the high queue, we've got three processes sitting. In the medium queue, we have two. In the low queue, we have one. Short time quantum in the high queue, medium time quantum in the medium queue, long time quantum in the low queue. New job comes. OK, now here, what we're going to do in this particular implementation of MLFQ is every job, while it is sitting in a queue, will get a total amount of CPU time it gets to run. Now, every time it runs, it will decrease that time. If it decreases the time to zero, it will get bumped to the lower queue. So we initially give this brand new job 30 milliseconds. OK, so here he is in the high queue, 30 milliseconds. And we run the people in the high queue, and we run that guy, and we run that guy, and we run that guy, and then we run that guy. And let's say that we have a 10 millisecond quantum here. And it turns out he uses whole 10 milliseconds. So he gets interrupted after 10 milliseconds, meaning out of the 30 that he was given as an allocation, he's now down to 20. We do it again. He does the same thing again. It goes down to 10. it again. He does the same thing again, down to zero. We move him to the medium queue. Okay, and then we give him some allocation in the medium queue. Medium queue's got longer time quantum, so it's going to get a bigger allocation. But he's only going to run in the medium queue when there's nothing in the high queue, in this particular implementation of MLF queue. So we continue. So let's say that after a certain period of time, we've used up all the jobs that are in the high queue. So there's nothing there anymore. Then we start running in the medium queue. We run in the medium queue, and we run that one, then we run that one, then we run that one, we're doing round robin here, and he gets to run for the 20 milliseconds or whatever the time slice is for the medium queue. How do we handle fairness? Well, here we have a situation where as long as are people in the high queue. We're not going to run any of the other queues. Periodically, we promote everybody. Everybody goes up to the next highest queue. So they all go up to the high queue and the low guy goes to the medium queue. And we reset their time slices accordingly. And if it turns out they should have been in the higher queue, they'll pretty much stay there. And if it turns out that, yeah, they were in the right queue to begin with, they'll go back down to that queue pretty quickly. All right. So what do we expect to get out of MLF queue? Well, we expect that for the interactive jobs, they're going to be sitting up in that high queue most of the time. So they'll get good response time. If there's other kinds of jobs that aren't interactive so much, but they have regular external inputs, so for example, are always writing the disk, well, they're in the right place. They won't be there too long before they're scheduled again to get to do their work. but they're not going to run for a very long time. They don't need to. For the non-interactive jobs, then they are going to be sitting most of the time in these lower queues. When they run, they'll get to run for a significant period of time. They aren't interactive. So we're not going to worry about people waiting for them because there's no person or no other process waiting for them to do work. If they get starved, sooner or later we priority boost them. They get some time. We expect to get dynamic automatic adjustment of scheduling. So if a process changes its behavior, goes from non-interactive to interactive, eventually it'll end up in the high queue and it'll stay there for a long time. All right, now these aren't the only ways of doing scheduling, but this is what we intend to talk about in this lecture. So in conclusion on scheduling, frequently an operating system could do many, many different things on behalf of many, many different processes. It has to choose which it does next. Scheduling is how we do that. We schedule who gets to use resources such as CPU cores, such as network cards, such as flash drives, who gets to use them next. We can do scheduling in a preemptive or non-preemptive fashion. Those have different characteristics. And speaking generally, different scheduling algorithms optimize different performance metrics. We want to choose for our system, a scheduling algorithm that optimizes the performance metrics that we care about.