We're going to start talking about a major important topic in the class, which is how we synchronize ongoing parallel activities.

What if different processes wish to communicate with each other?

What if we have portions of the same process trying to run independent streams of control, different instructions, and they need to make sure everything happens in the right order?

This turns out to be a very difficult and significant issue for operating systems to take care of.

So we'll spend several classes talking about implications of those issues and what we do to ensure proper behavior.

We're going to start by talking about threads, interprocess communication, and the general concept of synchronization.

That'll be the subject of today's class.

So we will spend a little bit of time talking about what is a thread.

Why do we have threads?

What issues do they lead to?

We'll talk about interprocess communications.

Processes are normally meant to be segregated from each other not to interfere with each other, but sometimes you want the processes to actually communicate for your own purposes.

How do we do that?

And then we'll talk about synchronization.

Synchronization is an important topic.

It's essentially, how do we make sure things that are supposed to happen in one particular order do happen in that order?

We'll talk about critical sections and asynchronous event completions.

First, we're going to talk about threads.

So threads are effectively another form of interpreter.

We already had a perfectly good form of interpreter, processes.

Processes were interpreters.

They provided us with a number of useful ways of performing computation.

So why aren't they enough?

Then we will talk about exactly what we mean by a thread.

And then we will talk about how threads fit into modern operating systems.

Okay, so you got processes.

We've already done a lot of work to try to build processes.

We talked about that in previous classes.

So why are they not sufficient for our interpreter needs?

Well, there are a number of reasons.

They are relatively expensive.

Expensive of course in terms of performance.

Normally in this class when I say something is expensive, what I mean is it has a high performance cost.

It takes a lot of time, a lot of work, a lot of overhead.

Why are they expensive?

Well, you have to create the process.

To create a process, you're going to set up a process control block.

You're going to fill in a bunch of fields.

You're going to set up a page table.

You're going to fill in a bunch of entries on the page table.

You're going to devote some page frames to that particular processes page table.

You're going to put it in the scheduler queue, and so on and so forth.

Now, of course, also, every so often, we are going to say this process should run.

It isn't running yet.

Let's start running it.

And when we do that, we are going to, as we discussed in the previous lectures, switch to a different address space.

And we talked about why that has important performance implications, not good ones.

So that's going to be very expensive, expensive to create processes, expensive to switch from one process to another.

Different processes, of course, are also very, very distinct.

They typically do not share an address space.

So that means that they don't share other resources as well.

For example, typically you do not have the same file opened in the same way for two processes.

They may be able to open the same file, but they won't really be well integrated with what each other is doing.

So that can be very inconvenient.

And in many cases, you want to perform some computations where you really do not want to have total strong separation.

You're doing something cooperatively.

A good example of this would be, let's say a web server.

What's a web server gonna do?

Well, it's going to run a bunch of code that is meant to handle requests coming in from, one hopes, a large number of different clients.

Now you could say I'm going to have a different web server for every different client, but that's going to make it very difficult to share those resources that the web server is going to be using.

So for example, if the web server creates one web page for a client and it turns out that another client needs the same web page, well, that would be kind of difficult to share if you were using processes.

It's also the case that for these kinds of complicated processes like a web server, or for that matter, a web browser, and another example being something that's doing a large quantity of data analysis, you may want to have multiple things going on at the same time, maybe even using multiple cores of the same machine at the same time.

Now, all of those elements in such applications tend to trust each other.

That would be particularly true if you were talking about one of these big data analysis tasks that has multiple streams of control, all working on different parts of the same dataset.

They all trust each other.

They are all trying to achieve the same goal.

They all should have the same privileges.

Well, processes aren't built like that.

They are built very carefully to avoid trusting each other.

And there's costs of doing that if you need to interact with other elements of control.

So sometimes processes just aren't what you want.

So we have threads instead because threads provide some capability, some abilities that processes do not have, at particular costs, of course.

So what is a thread?

Well, a thread is an interpreter.

It runs instructions.

However, it is only a unit of execution and scheduling.

All the other things that the processor model of interpreter does, threads don't do.

So they have to have their own stack because in order to perform execution of code, you have to have a stack.

You have to have your own program counter.

What's the next thing that this thread of control is going to do?

You have to have your own set of register values.

When the thread is working on a particular core, it's got to set the register values properly.

However, all of the other resources that might be associated with an interpreter are shared with any other threads that are part of the same process.

Effectively, we say that a process can contain one or more threads.

All of the threads in the process are going to share, for example, the same address space, which is very important, because then if we switch from one thread to another, we are not going to change address spaces, avoiding some of the overheads that we talked about in the last lecture, where you invalidate caches and things of that nature.

Also, they're going to share things like open files.

One thread opens a file, the other threads in the process, in the same process, have access to the same file in usually more or less the same way.

So they're going to be much cheaper to create than a process would be.

You want, got a thread already in your process, create a second thread in the process.

You don't need new address space.

You don't need a bunch of new file descriptors or anything of that nature.

You just need a little bit of a control structure for the thread itself.

And when you switch from one thread to another, processes thread one is running on the core, you want to switch to process thread two on that core, it's gonna be a lot cheaper.

So this is going to make them more suitable for particular kinds of computation.

So you have two options in threads, two important options.

So many of these things in operating systems have dichotomies, they're one way or another.

True with threads.

One way that you can build threads is you can say, well, the operating system is not gonna support threads.

However, we can, within a single process, have special code that's part of the process whose purpose is to create multiple threads of control within the process.

It's created the multiple threads of control, but because it's not something the operating system knows about, the process is going to have to deal with it.

You should obviously see this would have some issues with scheduling.

Scheduling is done on the basis of processes.

If you don't know about the fact there are multiple threads, how can the operating system schedule multiple threads?

Well, it really can't.

Somebody else better do the scheduling there.

That's going to typically be library code associated with a process that is supporting multiple threads.

However, we can, and often in modern operating systems do, have threads be a kernel level entity, something the kernel knows about.

The kernel knows that a process could consist of more than a single thread of control.

Now, in that case, the operating system can manage things associated with threads, such as scheduling.

So you have a choice when you're building a piece of code, a program that is to run.

You could say this is going to be one process, or perhaps, if it makes sense, several distinct processes.

Or you could say, this is going to be one process with multiple threads, or even multiple processes, each of which has multiple threads.

So when should you stick to only using processes?

Well, if you're running truly distinct programs, different pieces of code-- this one's a compiler, that one's a word processor-- you don't want those to be a single process with two threads, one for the compilation, one for the word processing.

The code is too different.

It is too unrelated.

So you're probably not going to have a lot of good sharing between those.

Also, if you're not creating or destroying things very often, processes are a good choice.

They offer you many strong advantages at a cost of being expensive to create and also expensive to destroy.

But if you're not going to create or destroy them very often, you don't care as much about that expense.

If there are different pieces of code running, or even the same piece of code, but you need to run it with different privileges, different rights to access, different files or other resources on your machine, then it becomes a lot simpler and a lot safer to say, well, everybody wants set of privileges A will run as one process.

Those who want privileges, set of privileges B will run as a second process, even though they're running the same code.

That becomes a lot easier to manage, a lot easier to provide those restrictions.

So if this is what you need to do, you might want to use processes.

If you do not have a lot of interaction between the multiple things going on, the multiple streams of instructions, if they rarely do anything that consults with each other, then we do have mechanisms that will allow processes to communicate with each other.

They have costs, but if we're not doing it very often, those costs are less important.

So if you don't have a lot of interactions, if you don't have a lot of shared resources, such as a large data structure that you're building up that both streams of control need to use, if you don't have that, then processes may make sense.

If you're worried about streams of control, even though they meet some of the other requirements and could be threads, but you're worried about them interfering with each other, improperly updating data that they shouldn't be touching, for example, it's better to have processes, because one process cannot properly access the data of another process that's in its data space.

And if you're worried about the fact that, gee, this set of instructions may crash, and if a set of instructions crash, then certainly the process that controls that set of instructions crashes.

If there are multiple threads within the process and one thread crashes, it's going to take the process with it.

However, if there are multiple processes, the failure of one of those processes will not necessarily cause the other processes to crash.

So if you want to have the ability to have some of your code continue to work, even though another portion of your code has failed, you probably want those two portions of code in different processes, not as threads in the same process.

One should use threads instead.

If you are doing parallel things in a single program, the web server is a very good example.

It's handling 100 clients more or less simultaneously.

All are using the same code, all are using the same data.

You're doing the same kind of things, building web pages for each of the clients, but they're doing parallel activities that normally don't really interact with each other, but may need to share certain types of data if you're creating or destroying them very often.

So one thing you can do with a web server is you can say whether it's multiple processes implementing this web server or a single process with multiple threads, is you can say, if I'm overloaded, I create more of whatever's doing it.

If I'm using processes, I create more processes.

If I'm using threads, I create more threads whenever I'm getting a whole lot of traffic because I need to do more things in parallel.

Well, it's expensive to create processes.

It's cheap to create threads.

So if that's the kind of thing that happens, you probably want to use threads.

If everything runs with the same privilege, you know, this is in the case of a large data analysis problem where you're looking at multiple terabytes of data and you're having different pieces of code operating on different pieces of data so that you can do things in parallel and get faster performance, well, they probably all have the same privileges.

They're all probably allowed to look at anything they need to look at.

So you don't need to protect one of them from the others.

If as a result of doing this data analysis, they're building up some important data structure, it may well be this is a data structure where they need to consult with what others are doing.

They're counting the number of words an English language text, a big body of English language text.

Well, everybody sees the words, everyone of the streams of control that you're running, sees the word camel or whatever, and it needs to update the count of camel in the data structure that's keeping track of this.

Well, we all need to update that count as they see the word.

They're sharing a resource.

There are other resources beyond data structures in the data area that get shared, like files.

In that case, you probably want to have threads, because it's going to be easier to share those resources.

If they talk to each other a lot, if they're sending a lot of information from one to the other, there is IPC that allows processes to communicate, but it has costs, expense in terms of performance.

With threads, it's much, much cheaper.

It's all going to be writing data, reading data out of RAM.

And if you do things right, that'll read very, very cheap.

If you're not worried about protection issues, if you don't care that one of them crashing brings down everything, because maybe that's what should happen if you have a crash for this particular application, then threads make sense.

OK, now, threads, of course, are a unit of execution.

They execute streams of instructions.

This means they need to have some resources on a per thread basis.

Every thread needs its own set of resources.

It has to have its own registers, because you need to say this is what, for the stream of instructions I'm doing, should be in the registers.

Obviously, a program counter for each thread.

They may be doing a new instruction, a different instruction than all of the other threads as their next instruction, and a processor status word that is per thread.

They also have to have their own stack.

And this is because they are streams of instructions.

Each thread may have an instruction that calls another routine, which means there's going to be another stack frame and then another stack frame on top of that.

And all the other threads that are running may need to have their own separate set of stacks.

So this is very, very, very much simpler if you simply say there's a different stack for every one of them.

Completely different stack.

We're not trying to shovel a lot of stuff in the same stack.

So when we looked at the simple memory management techniques for where you lay things out in memory, we said, oh, you could put the stack here and the data area there and they grow towards each other.

As long as they don't meet, you're okay.

If you have more than one stack, that doesn't work.

So what are you gonna do?

Well, what you're going to do is say, I will designate within the overall space of my memory different places for the stacks.

Now, they all have to be part of the same address space because they're part of a process.

The process has one single address space.

But there are different places in the address space.

And these are chosen strategically to say, well, they're pretty far away from each other.

If you have a 64-bit address space, it's real easy to find places that close to anything else.

But nonetheless, you have to say, "Here's this stack, here's that stack, there's that stack, three different stacks for three different threads.

They're going to grow in some way and they may run into each other and that must not happen because they will overwrite information that the other thread needs in its stack.

So what are you going to do?

Typically, what you do is you say, "Well, for each of them, I will set a maximum stack size and that is as as big as that stack could get.

And if the stack exceeds that maximum stack size, I just will say this thread has failed.

So that means you have to specify at the time you create the thread, here's how big its stack would be.

And then whatever it is that's creating stacks for threads, whether it be a library, if they're user-level threads, the operating system, if it's kernel-level threads, we'll have to find a suitable place to put that stack where it can grow to the maximum allowable size any chance of running into anything it shouldn't run into like another stack.

Okay, and of course when the thread exits you're going to have to say I'm not using that space for that thread stack anymore because thread doesn't exist.

And that makes it relatively easy to deal with the stacks as long as you can have enough space for them.

Now as I've said, we can have threads that are built at the user level or at the kernel level thread, kernel level.

Kernel threads are something where the operating system says threads are something I work with.

I create threads.

You ask me to create a thread.

I create a thread for you.

I manage threads.

I schedule threads.

I do preemptive scheduling on threads or whatever scheduling algorithm I'm using.

I understand that there are separate threads of control within this process that are going to run separate streams of instructions and I will schedule them with that in mind.

So they're all going to still share one address space.

They're going to have one data area that they share, for example, their own stacks, but one data area.

However, the kernel will decide at any given moment what thread shall I run next, not just what process will I run next, but within the process I'm running, which thread will I run.

Now, what this implies is that on a multi-core machine, say a 16-core machine, you could say if I have a process with eight different threads, maybe if nothing else is going on, if the scheduler agrees that I have the priority to do this or the turn to do this if you're doing round robin or something, the kernel could say, "Okay, I'm going to schedule all eight of your threads simultaneously.

This one will be on core three, that one will be on core seven, this will be on core five, etc., etc.

They'll be on eight different cores all running truly simultaneously their stream of instructions."

If the kernel knows that these threads exist, the kernel can do that and will if that's the way the scheduler says.

Modern Linux systems provide kernel thread so they can do this.

This is important, of course, because in modern systems, not only do our computers, the laptop I'm using, or server machines that you work on, typically have quite a few cores, but we also build a lot of processes that work in multi-threaded fashion.

So, this will allow such processes to run a lot faster and to be a lot more competitive with other things going on on the machine than if you had only one process that is not understood by the kernel to have multiple constituent threads.

But it is possible to have multi-threaded programs even if the kernel has no capability of understanding threads.

It doesn't create threads, doesn't schedule threads, doesn't know what a thread is.

You still can have multi-threaded programs.

How could that be?

Well, it's provided at the user level.

Typically, you will have a library, like the pthread library, that will say, oh, OK, fine, kernel with no bad threads, I can help you out.

It's going to have internally, within the process, within the multi-threaded process, its own scheduler.

It will set up its own stacks.

It will take care of switching from one thread to another.

Perhaps in the case of yields, perhaps it will catch trap in the instructions it would have blocked the entire process and instead say, well, I'll just indicate in my own schedule whether that thread's blocked or I'll switch to a different thread.

It takes care of everything.

And it has to, because the operating system doesn't know about the threads, can't do anything.

So those are options that you have.

And depending on what operating system you're running, you may have one or the other.

And you should be able to think about this and say, well, what's going to be good about kernel threads and what's going to be good about user-level threads.

Now, as I've stated a couple of times already in this lecture, just because processes are segregated pretty strictly from each other doesn't mean we can't ever communicate between processes.

It's just that we're going to have to use some functions provided by the kernel to perform such computation.

By default, one process may not interfere with another process. the kernel will offer opportunities for processes that wish to cooperate to communicate with each other in various ways.

And this is something that's common, not ubiquitous, but common in modern computing.

So, modern operating systems provide mechanisms that allow this to be done.

These mechanisms are called inter-process communications, IPC for short.

In the operating system context, if you see the acronym IPC, it means inter-process communication.

So if we want to provide in our operating system an IPC mechanism or a set of IPC mechanisms, what are we trying to achieve?

What would we like to achieve?

Well, there are many possible goals we could have for an IPC mechanism we're going to put into our kernel.

It would be nice for it to be simple so people understand how it works.

It would be nice for it to be convenient so they don't need to do a lot of complicated setup or a lot of different options to figure out how to make it work.

On the other hand, they'd also like it to be general.

They'd like it to be able to support many, many different ways in which processes could communicate with each other.

Of course, they'd like it to be efficient.

We know if it's a kernel-provided service that they're going to trap to the kernel to use it.

That's going to have its costs.

So you want to say that once we get into the kernel, we want to provide the IPC mechanism overheads to be as low as they possibly can be, to do as little in the kernel as is necessary to make the IPC work and not to have a lot of extra overhead.

Of course, we'd like them to be robust, meaning that they don't fail.

We'd like them to be reliable in the sense of saying that if we send the bytes XYZ from one process to another, we are quite sure that what gets received at the other end is XYZ, not ABC.

Okay, so These are all wonderful, obvious goals, and unfortunately, some of them contradict each other.

And the way we handle that in operating systems is we say, let's build a few different IPC mechanisms.

Some of them will be very good at meeting some of these goals, but not so good at meeting other goals.

The second mechanism might be good at meeting the goals the first one isn't good at, but suffers a little on the goals the first one does do well with.

The way we perform IPC in modern operating systems as I suggested via system call.

So there will be a set of system calls that you use to perform inter-process communication.

Now, as a rule, one process is not able to force communication on another process.

As a rule, the two processes, the one that wants to send information, the one that wants to receive information, must do cooperative things to say, "I am doing my part of this communication."

The sender would have to say, "I want to send."

The receiver would have to say, "I want to receive."

And usually, they would have to say this in a way that more or less specifies, from the sender's end, who am I sending to?

From the receiver's end, who am I receiving from?

But in most cases, this is going to be mediated at each step by the operating system.

So the sender will make a system call saying, "Send this."

The receiver will make a system call saying, "Receive this."

The operating system we'll have to get system calls for both of them.

Now the operating system may send information, signals to the receiving process, for example, saying you could receive if you wanted to receive, but it could not typically force the receiver to do anything about it.

So how do we actually in the operating system handle inter-process communication?

What we're basically doing in any form of inter-process communication is one process has a set of bits.

It would like the other process to know about those bits.

The second process would like to know about those bits.

So this will amount to saying, somehow or other, we're going to get the bits from one process to the other.

Now, if you're talking about two processes on the same machine, this is going to amount to one of two things.

Either you're going to copy the bits.

There's a set of bits in process A.

You're going to copy them into the address space of process B in an appropriate place.

That's one way of doing it.

Another way of doing it is to say, well, the bits that process A wants to send are sitting in the following page of memory and in the following page frame.

What I can do as the operating system, since I have this possibility, is I can say I'm going to take that page frame away from process A, and I'm going to give that page frame to process B at a suitable page location in its memory.

Process A would then have lost that page, or perhaps we would give them a different page with zero content.

So in that case, we don't have to copy bits.

Copying bits always costs in performance.

Under some circumstances, it may be cheaper just fiddle around with page tables, but there are issues with doing that as well.

Now, there are a number of dichotomies, these either-ors that we get in the context of IPC.

One of those dichotomies is some IPC is synchronous, some IPC is asynchronous.

What do we mean by that?

Synchronous IPC means that if I'm process A and I want to send a process B, I will call the operating system and say, I want to perform the following IPC for the following data to process B.

If it's a synchronous IPC, process A will block until something has been done to get the data into process B.

Now, there's a little bit of flexibility on exactly what that is.

Maybe the data has been delivered to process B.

Maybe process B has been notified that the data is ready for delivery.

You know, it could be varying, depending on how you define things within a particular IPC mechanism.

But process B is essentially going to be able to make use of that data.

Process A at that point would be unblocked and could go ahead and perform other things.

Now, this is, and on the other side, if process B says, "I'm expecting data from process A," it could say to the operating system, "I want data from process A and block me until the data arrives."

Then, sooner or later, presumably, process A sends the data using an operating system system call.

The operating system says, "Okay, process B wanted to be blocked until the data is available.

Here's the data.

I will unblock process B, make the data available to him."

Now, what's good about this approach is it's very, very simple to understand what's going on from the point of view of programming.

Process A says I get ready to do the IPC.

I ask for the IPC.

I don't do anything until the IPC is complete.

Process B says I can't go and do more work until I get the data from my partner via IPC, so block me until we get that data.

It's very simple to understand what's going on there.

But we do have another option. we can have asynchronous operations.

This basically says, okay, at process A, I want process B to get some data.

I will make an IPC call to the kernel and the kernel will simply say, okay, I'm gonna take care of it from this point onward.

I'm not promising you that process B has gotten this data.

I'm not telling you when it's gonna get the data, but it'll get the data sooner or later.

Don't worry about it anymore, process A, it's in my hands.

Now, if it's an asynchronous IPC mechanism, at that stage where the operating system system call has come back and said, okay, then process A can go ahead and work without blocking any further.

So this is not going to get any real confirmation that process B has done anything.

All you know is the operating system is going to make it possible for process B to do something with the information that was just sent at some point.

Now, of course, things can go wrong.

You probably, if you have an asynchronous mechanism of this kind, have to have some way of telling process A that things went wrong.

So for example, what happens if process B crashes before whatever transmission was supposed to happen has occurred?

You probably want to be able to notify process A, hey, you remember that IPC that you said you were going to do?

It didn't happen.

Process B crashed.

So that and other kinds of errors will probably need to be handled by notifying a process that as a sender during asynchronous operation.

On the other side, on process B, I think sooner or later, process A is going to send me some data.

If process A's data happens to be available, gee, I'd like to go ahead and work on it.

If it isn't available, well, I'd like to go and do something else.

In that case, you'd like to have an asynchronous mechanism from the receiving end that asks the operating system, is this IPC communication actually available at this moment?

If the answer is yes, then you want to get the data.

If the answer is no, you just want to be told no.

OK, so this means you're not going to block.

You ask to read data.

The data is either read and put where you need it, or alternately, you're blocked until you're allowed to go ahead and do something else.

You don't have the data.

You're told you don't have the data, but you aren't blocked.

So depending on exactly what you're doing with IPC, being asynchronous or synchronous may be the right thing to do.

So what do we typically do when we perform operating system IPC activities?

Well, it's normal to say, if there is any IPC at all, frequently it's kind of ongoing.

There's going to be a fair amount of data created and sent from A to B.

So we typically will create something called a channel.

So there'll be an operation that is performed saying, let's create this channel.

Of course, sooner or later, you may be done with your communication, in which case you want to be able to destroy the channel and get rid of it. that you have such a channel, you may want to be able to say, OK, channel's been set up.

I'm ready to send some data.

Let's put some data into the channel.

Let's send the message.

Let's write into this location.

That is the kind of thing where you put data into one of the channels you set up.

On the other end, you would want to get the message, receive or receive the message or read the data that's been put into the channel.

It may be that you want to be able to ask about the channel.

You might want to say, for example, on the receiving end, "Gee, I've been told there's some data in the channel.

How much?"

Or you might want to be able to be told, "Is the sender in an active state or is the sender blocked?"

In which case, it's not going to send any data for a while because it's not performing any instructions.

So, you may want to be able to make a query about what's in the channel.

And you may need to have some help in figuring out how to set up the channel.

There's somebody out there who is my communication partner, but I don't know who.

How do I figure out who?

Where is the endpoint?

Where is my sender?

Where is my receiver?

Is the sender ready to send?

Is the receiver ready to receive?

Can we set up this channel or do we need to wait?

Sometimes you have IPC operations provided by the kernel that will answer such questions.

Another dichotomy we see in IPC is that sometimes we use what are called messages in IPC and sometimes we use what are called streams.

So what's the difference?

If you're going to a bunch of bytes, byte after byte after byte after byte after byte, that is created by the sender, and all going to go to the receiver, often you will have this done as a stream.

A good example of this, which we'll talk about a little bit more, would be a pipe.

You've probably all done pipes.

You know, you do LS pipe through more or something, where you want to see a screen full of data at a time, not the entire thing zipping past you without your having a chance to look at it.

Well, the sender, the LS process in this case, would be reading the contents of the directory and saying, OK, here's an entry, here's an entry, here's an entry.

It'd be creating characters that it should send to the more process.

So it'd be a stream of characters.

Now, it's probably not creating the whole thing and then sending the whole thing.

It's probably starting to create, saying, here's a record, send the record.

Here's another record, send the other record.

So typically, that would happen with a stream mechanism on the sending end.

On the receiving end, you're going you get saying OK, bytes come in, I show the bytes, bytes come in, I work with the bytes, bytes come in, I work with the bytes, and you continue doing that as long as the stream persists.

Now, sometimes, streams will just be a bunch of bytes.

It's just a bunch of bytes coming in.

Other times, it will be something like saying, here's a bunch of bytes and here's a delimiter saying I just sent you a full record.

Here's another bunch of bytes.

Here's a delimiter.

Here's a second record.

That's a second record.

Here's a third set of bytes.

Here's the delimiter. a third record and so on.

That would presume that the sender and receiver understand the type of records they're sending and understand what delimiters will be used to say, "Here's the end of a record."

Now the other choice you have, another choice you have in IPC, is messages.

These are called datagrams frequently.

Now sometimes these are messages that are going from one machine to another, but you can send a message from one process on a machine to another process on the same machine.

In terms of mechanically what's happening under the covers, you would expect different things to happen when you are sending between two processes on the machine and when you're sending between a process on one machine and a process on the other machine.

But to a large extent, what it looks like from the point of view of the code is quite similar.

You're just sending a message.

Now, typically, if you're using a message-based IPC mechanism, you are specifying this is a message.

This isn't a stream of bytes.

This is a message.

Bang.

Here's the whole message.

Send it.

Here's another whole message.

Send it.

Typically, if this is the IPC mechanism the sender is using, the sender creates an entire message, then makes a system call that says send the message.

Then it creates another entire message, does a system call saying send the message.

It's different from streams.

With streams, you set up the stream, and you say put something in the stream, put something in the stream, put something in the stream, put something in the stream, put something in the stream, and you keep doing that.

Now you may have those delimiters in there, but as far as the IPC mechanism is concerned, those delimiters are just more bytes.

When you're using messages, there is a very strong distinction between this is message one, bang.

This is message two, bang.

This is message three, bang.

Every one of those would be a distinct system call and you would not make that system call sending in until you had every single byte of the message set up.

On the receiving end, similarly, you would receive entire messages.

You'd say, I have received all of message one.

I haven't received half of message one.

I haven't received a stream of bytes and there might be more bytes in the message.

I've received the whole message.

Here's all of message one, bang.

Then I'm told message two has arrived.

I receive all of message two, bang.

I don't receive anything in between message one's receipt and message two's receipt.

Even if half of the bytes, in theory, could have been available for message two.

I don't get any bytes until the entire message is that my receiving end.

That's what we mean by a message IPC mechanism.

It's either delivering an entire message or it's not delivering anything at all.

So depending on what you're doing, sometimes you want streams, sometimes you want messages.

And it's important to remember that while in either mechanism, streams or messages, You could have records, things that are data structures that are sort of being transmitted via the IPC stream.

If you're using an IPC stream, the streaming type mechanism, the record delimiters are understood only by the endpoints, not by the operating system in the middle.

If you're using a mechanism that's message-based, then the operating system understands these delimiters.

It knows about here is the beginning and end of a message.

Here is the beginning and end of another message, and so on.

Okay, now an important concept whenever you're doing IPC, from the point of view of typically the operating system, though it may sometimes impact directly on the code that is written for this purpose, an important issue is flow control.

What do we mean by flow control?

Well, you have a sender, you have a receiver.

The sender is going to create data that it wishes to get to the receiver at some particular speed. the receiver is going to be capable of handling data sent by the sender at some other particular speed.

If the sender is faster than the receiver, the sender creates data faster than the receivers can possibly deal with it, then you're going to, in the long run, if this mechanism keeps working for a long time, if a lot of data gets moved, you got a flow control problem.

Because what's going to happen?

Well, if it's not blocking in particular, the sender is going to create, create, create, create, create, create, create, and it's going to just keep creating bytes of data that it's going to try to send to the receiver.

And the receiver, being slow, will be consuming the bytes of data that were created at a much slower rate.

What are you going to do with all of the bytes that have been created but not yet received, not yet consumed by the receiver?

Well, typically, given the IPC mechanisms that are handled by the operating system, what's to have to happen is the operating system is going to have to buffer that.

It's going to have to queue up all the data that the sender has sent that the receiver has not yet been able to receive.

Okay, what's that mean?

Well, it means somewhere you're having a big queue in memory of all this data.

Now, typically, you cannot afford to have that queue grow without limit.

You're going to have a limited amount of memory, a limited amount of RAM, for example, which you can use to hold the data that the sender is sent but the receiver has not received.

So, if you have situations where for whatever reason the receiver isn't getting things fast enough, which may be because it has to do a lot more work per byte than the sender had been using the bytes than the sender had to do in creating the bytes, or it may be that the receiver has other things it should be doing and is only occasionally going to get around to receiving data because it's got to do its other business, you may have situations in which a lot of data builds up.

So what are you going to do?

Sooner or later you will fill up all the space that is possible to hold this buffered data.

Then what?

Well, one thing you can do as the operating system is you can say, "Okay, I am going to block the sender.

The sender is blocked until we've cleared out some of this buffer space," which means you have to wait for the receiver to get that data.

Or, alternately, maybe I won't block it.

In particular, if it's an asynchronous mechanism, maybe I won't block it.

Maybe I, the operating system, will instead respond to the sender's request to send this data with a rejection.

I will say, "No, you can't send that data.

There's not enough buffer space."

And then the sender can choose what to do.

He can block himself if he'd like to, or he can go do something else because he can't get any more data into this channel.

What about on the receiving side?

If the sender keeps sending messages, for example, I may on the receiving side keep getting these signals from the operating system saying message, message, message, message, message, and I got to do something about those signals as the receiving process.

What do I do then?

Well, maybe I can ask the operating system somehow or other stop the sender from sending me so many messages.

I don't want to see all these messages.

I can't handle that many messages."

Or maybe what you do is you can say, "Okay, fine, keep sending me messages.

I can't handle all of them.

I'll throw away the ones that have been received in the past so I can receive the ones in the future."

Whether this is a good idea or not depends very much on what's going on, what's happening here.

So there are ways that you have to build this into your application.

Now, if we are talking about doing things across the network, where we have a sender on one side and a receiver on another side and maybe the entire internet in between, then we have a somewhat harder issue because in this case neither the sending operating system nor the receiving operating system is in full control of how quickly data moves from one place to another, the internet in the middle maybe in charge of that.

And the internet is not going to be all that cooperative with the sender and receiver.

The internet generally speaking is going going to make best efforts, attempts to deliver every packet of data that has been submitted to it.

It'll try.

But if it can't, it's going to drop packets of data.

And all it'll do when it drops a packet of data is drop the packet of data.

It's not going to do anything where it informs operating systems at each end of I dropped this packet of data.

It's just going to drop them.

So you're going to have to have something that make sure that when that happens, good stuff continues to go on for the sender and receiver. particularly because in many cases, the congestion, the overflow of whatever is happening in the internet is temporary.

If you respond to it by say, slowing down or waiting till later, then it may work out just fine.

But that means of course that the sender has to slow down or wait till later or whatever.

This is typically handled by network protocols.

Those are built into operating systems frequently nowadays.

And they have ways of doing this, which aren't the problem in this particular class.

But flow control is a very, very big deal in network communications, particularly in the internet.

Now, typically, we're going to have some way, when we have this flow control problem, of telling whoever is creating the data and sending the data, we can't work at this speed.

We're going to do something.

We can block them.

We can give them a signal saying, slow down, and maybe they're prepared to respond to that.

We can reject each request without blocking them.

We're going to do something.

And of course, the code in the sender then must be responsive to that.

If it's blocked, of course, the response is simply it doesn't run.

But if, for example, it's a rejection of a message, it has to say, well, now, I wanted to send this message.

It got rejected.

What should I do now?

It'll have to do something.

OK, now we talked a little bit about reliability and robustness.

You might say, well, if I'm talking about I process A on one machine, process B on the same machine.

Where is the problem?

I'm not going to accidentally lose or corrupt data if I don't have bugs in my operating system, which I certainly shouldn't have.

In that case, if I say I'm sending ABC, the receiver will get ABC, assuming that the operating system doesn't say the receiver's crashed or something like that.

But certainly, the receiver is not going to get XYZ.

He's going to get ABC because the operating system is designed to give him what I told to give to him.

Across a network, this gets to be a lot chancier.

There are all kinds of bad things that can happen to data that flows through the internet.

However, even if you're talking about two processes on the same machine, there is no guarantee that the receiving process is going to do the right thing with your message, or even is going to accept the message.

The receiver might have gotten into an infinite loop and is never going to get to the code that's supposed to receive the message.

The receiver might have other things that are more important for it to do.

If it waits long enough, maybe it'll get your message, but it's busy doing other things right now and it's not interested in your message because it's got more important stuff to do.

Or it might have crashed, in which case, you know, what are you gonna do?

So you, the sender, have given the operating system a bunch of data that's supposed to go to this receiver, and for whatever reason the receiver is not accepting that data.

It is not taking that data.

What's the system going to do?

Well, how long is it going to hold on to that data, hoping that the receiver will eventually want to receive it?

So, in particular, probably when sender makes a system call saying, "I'd like to send some data to this other process," the operating system at some point has to say, "It's done.

It's okay.

The data has been sent."

When do you tell the sender, from the operating system's perspective, "Your data has been sent"?

One One thing you could do is you could say, well, I put in a queue here.

Fine, it's sent.

Sender, you're okay, go ahead.

Another thing you could do is you could say, well, perhaps this IPC mechanism has built into the mechanism a queue of messages, for example, at the sending and receiving end.

I have put this message into the receiver's queue, so it's in his address space somewhere and he should know that it's there.

Maybe at that point I tell the sender your message has been delivered.

Possibly, I want to wait until the receiver has pulled that message out of the queue and actually perhaps taken some action, at least pulled it out of the queue.

Maybe that's the point at which I want to tell the sender your message has been received.

Or maybe I never want to tell the sender that, but I want the receiver to do something to explicitly say, inform the sender that his message has been received.

There's code in the receiver that would do that, for example.

Of course, I would, as the operating system, when that system call is made, say, oh, receiver has received message 12.

I will inform the sender message 12 has been acknowledged.

Now, if we are talking about two processes on the same computer, the easy case, one way or another, if the receiver is going to receive the data, it's going to receive the data.

The operating system is going to do whatever it does to make the receiver receive the data, such as copy the bits into its address space. and at which point, of course, the operating system is legitimately reasonable in saying, I've done my duty.

I did what I was supposed to do.

The message bits are in there.

I'm done with this.

But if you're talking about a network, the internet, for example, then when I, the operating system on machine X, send the message from process A to process B, which is on machine Y across the network, I've kicked that message out into the network.

When am I going to say, I'm done with that message?

That's it.

Am I going to try to send it more than once?

Now, if I'm sending it, for example, over a Wi-Fi network, it's not uncommon on a Wi-Fi network to have a collision, by which we mean two people in the Wi-Fi network are trying to use the Wi-Fi network to send the message at exactly the same time.

They garble each other's bits.

Now, probably at the Wi-Fi network level, We want to resend those messages.

And there are mechanisms built into the Wi-Fi protocols that do that.

So when do we, the operating system, say your message has been sent?

If it's a Wi-Fi network, do we wait until, oh gee, there was no garbling of the message, it went out across the Wi-Fi network, the Wi-Fi network has said at this link layer, yes, the message has passed.

Well, then we can say, fine, we're not gonna have to resend it right away.

But of course, bad things could happen later on in the network.

What are we going to do there?

If we keep a copy of the message in question around on the sending end, then if bad things happen and we are either notified that a bad thing has happened or we deduce somehow that the bad thing has happened, we could resend the message.

Because we kept a copy of it, we can transmit it again.

So that is something that we do in certain network protocols.

TCP, for example, does retransmission of lost messages.

Other protocols don't.

UDP does not.

So we have that option.

Now, yet another option we have, which is of higher reliability and a higher semantic level, is to say, what if we have multiple servers at different locations in the internet, and any one of them can service this request?

We can do this, for example, with things like DNS lookup.

Don't worry about what it is if you don't know what it is.

But there are many DNS servers out there, and any one of them could perform the lookup for us.

So if we know about multiple ones, we could send it to the first of them. if we don't get our response in sufficiently quick time, we, the operating system, could send that message to the other one without notifying the process that asked for the result of what happened.

We could just say, well, let's try another one and see if he can respond.

And that can be done for other types of services as well.

In which case, in order to do that, we're gonna have to save the message that was supposed to go out to the DNS server so that we can retransmit it to somebody else.

Now, another issue, of course, is that if we're working in a distributed system, we may have this ongoing relationship.

Like, we've got a web browser on our machine.

There's a web server over there.

We're doing all kinds of interactions with the web server.

What happens if that web server crashes?

Now, as we will see when we talk about distributed systems, in certain cases, the way that we handle web servers that have to provide a whole lot of service, that have a lot of clients, need a lot of processing, is we have multiple machines create the web service.

Now in that case, maybe what we can do is say, well, if one of those machines that was talking to me crashes, maybe I can just switch over to one of the other machines and interact with that one instead.

And if we can do this with sufficient transparency, hiding the fact that there was this crashing recovery, perhaps my application on my machine, my web browser, doesn't even notice that it just got switched to a different server. it just gets its results.

That would be nice.

It may also be the case that we just have a temporary failure on the web server we are talking to remotely.

It comes back.

If it comes back, does it remember what it was doing before it failed?

Maybe it does.

If it does, then we can just send it our request again, and we can expect to start up where we left off.

That would be nice.

Some services can do that and some can't.

Okay, now, because we have all of these different types of IPC mechanisms that we could use, different things we want the IPC to do, it is very common in a particular operating system to have several different IPC mechanisms, different ways for your process to communicate to the other process.

We've already mentioned pipelines.

We'll talk about them a little bit more.

We'll talk about sockets.

We'll talk about shared memory, and this is not the full set.

There are others we're not gonna discuss. mailboxes, name, pipe, simple messages, signals, et cetera.

Let's talk a little bit more about these three examples, however.

Pipelines first.

You are all familiar at the user level at least of dealing with pipelines.

LS, pipe through grep, pipe through sort, pipe through mail.

You end up getting a mail message that shows what is in this directory that matches the pattern you care about and in order.

So that's one thing you can do.

And when you do something like GCC, you're not actually just calling the compiler.

You're probably first calling a macro processor that finds all the macros and expands them.

Then you're going to take the output of that, and you're going to give that to the compiler.

The compiler builds assembly code, which is not machine language.

It can be directly converted to machine language by being run through another program called an assembler.

So what's really probably going on under the covers is when you call this what you think is a compiler, It's first going to the macro processor, piping the output to the compiler, piping the compiler's output to the assembler.

And then you get your machine code out the other end.

Now, if you're using a pipeline, whether it's explicit, as in this first example with LS, or built into the perhaps script that you were using, what's really going on is that you have a stream of bytes.

It's a very, very simple byte stream.

So the sending side, like LS in this first example, takes every byte that it is creating, and it pushes it into this pipe, into this data stream.

Now, what that really means is not, there's not a physical pipe sitting anywhere.

What this means is that the operating system, because you've said, I want a pipe, has set up a buffer that says the sender will be putting bytes into this location that I've reserved in memory to hold those bytes.

And every byte that he puts into that pipe will go into that buffer in the appropriate place.

It's basically a piece of RAM somewhere.

And then when the receiver says, I am receiving data out of this pipe, it will read from that same piece of RAM.

OK, now there is no need for any files to be created here.

So when you do lspipe through grep, no file is created to take the output of ls and say, here's the output of ls, have grep open this file.

It is done with IPC, just with copies of bytes.

There's no-- when you're using pipelines, there is no security, privacy, or trust issues, Because pipelines are used when you have a user who is moving data from one program to another program to another program.

All the programs are running on behalf of the same user.

They all have the same privileges.

They all have the same privacy concerns.

They all have the same degree of trust in each other.

So you don't worry about, "Geez, my pipeline being properly protected.

Am I moving the sensitive data that I shouldn't move to this other process?"

If you don't trust the other process, you shouldn't set up a pipe to it.

Okay.

So, there are still some errors that can occur.

So sooner or later, of course, whatever's at the front of the pipe, the sending end is going to run out of data.

It's going to finish.

So it's going to have to send an end of file.

And the guy at the receiving end did not know that it was out of data until it receives the signal saying end of file.

It's not really a file, but it effectively is called a file for this purpose.

Another possibility, of course, is that one of the two processes crashes. sender crashes, the receiver crashes.

If the sender crashes, then clearly no more bytes are going into the pipe.

If the receiver crashes, clearly no more bytes are getting taken out of the pipe.

In either case, you want to tell the other party, the party that is still running, your partner has crashed, your pipe is finished.

No more data going in, no more data coming out, depending on which one of them crashed.

So pipes are very, very simple.

They're powerful for their purpose, but they're limiting.

There's only a few things you can do with a pipe.

On the other hand, we have sockets.

Sockets are basically saying something like, "I have an address on the sending end."

It will consist of, "This is the process, and within the process, this is a port, an entry point in this process."

Similarly, the receiver or an exit point for the sender.

The receiver will say, "Here is my identifier as which process I am, and here's my entry point into my process.

So what we're going to do if we have a socket is at the sending end we will set the socket up, at the receiving end we will accept the socket.

Once it has been properly set up on both ends we can say fine now we need to put data into the socket.

In order to do this of course we may have to do some moderately complicated stuff like we have to for example say are we sure at the receiver side that the sender has started yet.

He may not have started yet.

We know we expect to hear from him, but he may not have started yet.

So maybe I need to wait until he's ready, until he says I've set up my socket, then I, the receiver, can set up a socket.

Or alternately, I'm on the sending end, I'm gonna create a socket, I don't know if my receiver is ready to receive yet.

I don't know if he's even running yet.

Maybe I need to wait for him to run.

So you need to do things like listen.

This is called listening, to see if your partner is there.

Also, of course, if I'm the receiver, it may be that I'm willing to receive from a certain process, but I don't want to receive from other processes.

So I need to, when a possibility of a socket has come along, I listen and say, "Oh, here's a socket possibility."

I need to say, "Yes, I'm accepting that one," or "No, that's not who I want to talk to.

I'm not accepting that one."

Now, given that we've set up our sockets, we are going to have a bunch of choices.

Sockets are an extremely general mechanism.

They do many, many, many different things.

So, I can say this is a reliable socket.

I am going to make sure that the bits that go in come out the other end, and I'm going to tell you if it becomes impossible to do that.

So for example, your partner has died.

Now that means, because sockets will allow you to communicate between two machines, pipes won't.

Sockets will.

That means that I may have to figure out, gee, you know, is the process over there on that other machine croaked?

It's not obviously clear because if it's in another machine, you don't have them in your own process control table for your operating system.

You're going to have to do something to figure out that he's gone.

It may be that you don't care about that and you just say, "Well, I'm sending off messages to this guy.

Probably he's there.

Probably he's going to listen.

Probably he's going to get the message."

Best effort.

Try to send the message.

If the message is sent, great.

If it isn't sent, well, that's too bad.

I can do things with messages, actual messages.

You know, this is a integral message. for the whole message, send the whole message.

I can do things with streams.

I can say stream of bytes, here's some bytes, here's some bytes, here's some bytes, wait a little bit, here's some more bytes.

And I can put whatever number of bytes I want into the stream, if I have set up the socket to be that way.

I can even do remote procedure calls.

We'll talk about remote procedure calls in future classes, but that basically says, I'm in process A, I'd like to call a procedure that's in process B, and maybe process B isn't even on my machine, it's on a different machine.

Kind of a cute trick.

It can be done.

It is done.

There are mechanisms that allow us to do that in a very reasonable way.

Now, of course, there's issues of flow control.

And sockets are capable of doing very, very complex types of flow control.

So they can do retransmissions.

They can say, gee, I'm supposed to have heard about this.

I haven't heard about this.

I want to time out in a certain amount of period.

I need to know if my node over there has failed.

I'm communicating with a process on the node.

Please tell me if it's failed.

Here's what to do if it fails.

Maybe I try to switch over to a different node, which also offers the service.

Maybe I don't.

Maybe I want to wait for the guy who apparently isn't available right now.

And if he comes back, you know, in three seconds later, maybe I want to try to reset up the communication socket.

He might be okay.

He might know about my socket still, or he might've crashed and recovered, in which case he probably doesn't know about my socket.

Maybe we want to reset up.

Sockets can do all this kind of thing, and many other things as well.

Now there's also of course issues of, gee, trust.

Is this a secure situation?

In particular, that's going to be true if we communicate across the network.

I'm on machine A, you're on machine B.

Well, you know, maybe I trust you and you trust me, so we're okay in that regard with our data, but we're sending data between A and B, which means it's going across the network, and you shouldn't trust the internet or any other network.

Certainly you shouldn't trust a Wi-Fi network.

So what are you gonna do then?

We can talk about that later.

As a rule, sockets have many, many, many, many different options.

And when you set up the socket, you choose which options you want to use, and you create a socket that is specific to the set of options that you have created.

This makes sockets complex.

In many cases, people who use sockets in their program, how do they figure out what options to use on the socket?

They look at another program that's already used that set of options, and they copy whatever code that person used. because it's too complicated to figure out just looking at the man pages or looking at documentation, say, here's how I should set up this socket.

There's a whole lot of copying of code that does sockets in particular ways and saying, OK, I just changed the name of the machines or I just changed the name of the variables or whatever.

And otherwise, I'm using exactly what they did, and it works.

OK, now here's a third form of IPC that's really different than any of the others.

Shared memory.

Shared memory is precisely what it says it is.

Two processes share the same memory.

And by saying they share the same memory, we mean they share the same data in page frames.

It's in the RAM and they're both using the same page frames in RAM.

Okay, so this is really shared.

Now, this is not at all like the protection we expect to see about a process from other processes.

We're now saying my page table has this page frame in it.

Your page table also has that page frame in it.

Frequently, we both have read and write privileges on those pages pointing to the same page frame, which means A can write the page frame, B can read the page frame, B will instantly see what A wrote.

It will not have any delay.

It will not have any required interaction between the operating system.

It will simply be shared memory, shared RAM.

OK, so the operating system can do this.

Why not?

Usually, of course, the operating system says, unless this is code or something which isn't writable, I don't want two processes to have the same page frame shared.

I don't want that to happen.

That would be a disaster, wouldn't it?

Well, not if the processes say that's what they want to do.

Then the operating system, if everything is properly set up, will say, fine.

Process A, you say you want to share this amount of RAM.

Process B, you agree that you want to share that amount of RAM.

Great.

You're sharing the same amount of RAM.

And how do I, the operating system, achieve this?

I go to the page table for A, and I say, here is your page frames for a particular set of pages.

I go to the page frame, the page table for B, and I say, and here are your page frames.

And they're the same ones that A is using.

OK, now, at this point, process A can write something into a word of memory in the shared memory area.

Instantly, not quite instantly, but at the next memory cycle, means before the next instruction completes on any core of the machine, process B will see the content of that data if it chooses to use it.

A set the variable to 12, B reads the variable, it'll be 12 because they're looking at exactly the same cells of RAM.

It's the same hardware.

The operating system does not become involved in this any further, just as with limited direct execution.

When you are writing into memory, the operating system typically is, unless you're going to have something like a page fault, is not going to do any action whatsoever.

It's going to say, "Your memory, do what you wish with it.

You said you wanted to read and write this memory, here's your memory.

Read and write the memory to your heart's content."

And that's true if you're set up as shared memory as well.

Now, this has one tremendous advantage over all the other mechanisms that we use for IPC.

It is really fast.

It transfers data between two processes as quickly as it possibly could.

You could not do better.

You could not make it faster.

It is very, very, very fast for both reads and writes.

Of course, and it has another advantage.

It's simple.

I have a data structure.

I'd like my two processes to share the same data structure.

It's got records in it.

If I write 17 into record one from this one process, all the other process has to do to see what is in that record is look at the record.

It just has to look at that piece of memory.

And it sees 17.

What could be simpler?

Well, it is simple in that sense.

But now you have a very serious issue, which is how do I know when process A, that's sharing memory with me, has actually written something?

I'm expecting him to write something.

When do I know that he's done it?

This is going to be potentially very, very tricky.

And if you aren't careful, you can screw yourself royally with shared memory.

The operating system isn't gonna help.

With other IPC mechanisms, the operating system helps.

You know, I send a message, the operating system says, okay, the message gets sent when you call me and say the message is to be sent, when you call that send call.

When is the message received?

when the guy on the other end calls the message receive call.

I can make sure that you, the sender, finish sending your message before the receiver finishes or starts receiving the message.

I can make sure that happens.

I will help you have things happen in the right order.

But what happens with shared memory if we have a 100 byte record in our shared memory area?

Process A starts writing into that 100 byte record and he writes the first 20 bytes.

Process B starts reading.

It reads the full 100 bytes.

He got 20 bytes of the new stuff and 80 bytes of the old stuff.

Well, that's too bad.

Probably that's not going to be a correct record, because it's partly the new record and partly the old record.

Well, too bad.

The operating system will help you one little bit there.

You, the processes who are sharing memory, will have to do this yourself.

You'll have to synchronize your efforts yourself, which can be complicated.

One other characteristic, of course, of shared memory is that you can't share memory with a process on another machine.

You can't look at their RAM.

They can't look at your RAM.

You cannot share memory.

OK.

Now, we have already alluded to the issue of synchronization.

Let's start talking about synchronization.

We'll be talking about synchronization for several classes.

It is a complicated topic.

What is synchronization?

In computing operations, there are a bunch of different processes or threads that are doing things.

They're writing instructions.

In certain circumstances, you want to make sure that the things the various processes or threads are doing happen in one particular order.

So for that shared memory example I just talked about, if process A starts writing the 100 byte record, you would prefer that process B does not start reading that 100 byte record until A has finished writing the whole record.

You don't want to read half of a record.

Okay, well, in order for that to be guaranteed, absolutely guaranteed, you're gonna have to make sure that B isn't allowed to look at that data or doesn't look at that data until A finishes.

Making sure that happens is synchronization.

Now, in days where you only had one process running on a machine at all, many, many decades ago, that was simple. well, you know, one process is running, got one stream of instructions, it does what it does.

When you have multiple processes, it gets to be more complicated.

When you have multiple threads in the same process, it gets to be even more complicated.

So multiple things are happening at the same time, a whole lot of streams of control, particularly though not exclusively on multi-core machines, because now you have four or eight or 16 sets of instructions that are running at exactly the same time.

There's a little bit of hardware synchronization.

If an instruction is running on core 3, then core 4 will either see the state before that instruction or after that instruction, but not in the middle.

You get that degree of hardware synchronization.

But that's all you get.

And instructions are machine language instructions.

There are things like store, and load, and add.

They are not complicated things.

So things get very, very complicated have multiple streams of instructions that are running simultaneously.

Very, very complicated.

It would be nice if we could avoid that complication.

But it turns out, no, no, we can't avoid that complication.

Why not?

How do we achieve high speed in computing nowadays?

The way we used to achieve high speed a few decades back was we had faster chips.

You just said, I'm going to speed up the clock speed of this chip.

Instead of running this many instructions per second, it's going to run twice that number of instructions Isn't that wonderful?

We reached the limit on what we could do with that in terms of what hardware can be made to do.

How do we run faster now?

Well, do several things at the same time.

This is where multi-core machines come in.

A machine with four cores in principle can run four times as fast as a machine with one core provided you don't screw up the synchronization.

You got to have the parallelism to get the speed we require.

But if you have the parallelism, you must deal with the synchronization issues, at least in some cases.

Now, we get a lot of benefits out of parallelism.

Parallelism is very, very much our friend.

We get better throughput.

You can get more data through, more things done in a unit time.

And in particular, if one thing is blocked, that doesn't mean that nothing else can run.

Other things get to run while a blocked guy is waiting for his block to be resolved.

We can get modularity.

You can say instead of having this one humongous program, I could build four different programs, each of which does one part of the overall thing I want to do.

And I can run them in four different processes.

I can even run them simultaneously, assuming we don't have problems with synchronization.

And then I have simpler pieces of code, which means I'm not as likely to have problems.

We can get robustness.

You can say I have multiple threads that are running as my web server. one process but multiple threads in the process.

If one of those threads runs into an error that means it cannot continue, it's got an error, I don't necessarily have to kill all the other threads in the process.

They can keep running, which means I have a much more robust system.

And this of course is basically what we are doing in modern computing.

This is how we do client server computing.

This is how we do web-based services.

This is what cloud computing is doing.

This is what all of our high-speed computing applications must do in order to achieve their speed.

Now, this actually is a fairly acceptable thing because it's what the world is like to begin with.

You know, the world is not one thing happening at a time.

It's a bunch of things happening all over the place at the same time.

You know, I am taping this lecture.

My wife is playing some games on her computer somewhere, Somewhere else you guys are doing something, not listening to the lecture, perhaps live, because I'm not streaming it live, but you're doing something, you know, you're doing your homework, you're doing your lab, other people are in other countries doing other things entirely, and it's all happening at the same time.

The universe is parallel processes, some of which are cooperative parallel processes.

When we have a conversation between two people, Those are cooperating parallel processes.

I'm saying things, you're listening, you're saying things, I'm listening.

We are cooperating in parallel.

So we expect that our computers must be able to work the same way.

If we did not have parallelism, if we got rid of all the parallelism, which would mean we wouldn't have to worry about that synchronization issue that's so unpleasant, our performance would go back to the 1970s.

That would not be acceptable.

Okay, so what is the problem here?

Fine, we've got to have parallelism.

Why is there a problem with this?

What's the difficulty?

Well, what's going to happen when we have a bunch of things going on in a program?

If we have one program with one set of instructions happening in one order, we know pretty well what's going to happen in that program.

We're going to do instruction one, then instruction two, then instruction three.

Maybe we'll loop.

We'll go back to instruction and do one, two, and three again.

Maybe we will skip over a bunch of instructions and go from instruction 12 to instruction 250.

But that's all built into the code.

We can look at the code and say, here's what could happen.

We could loop here.

If we don't loop here, we'll go to the next instruction here.

We could, under some circumstances, take a jump here.

If we take a jump here, we'll go there.

If we don't take a jump here, we'll go to the next instruction.

It's relatively simple to figure out what's going to happen in these sequential programs.

Now, if we have totally independent parallel programs, I'm running a compilation, you're doing web browsing, we're on the same machine, separate processes on the same machine, under most circumstances, except for performance issues, we're not going to interfere with each other.

My compilation has nothing to do with your web browsing.

Your web browsing has nothing to do with my compilation.

Everything will go just fine with the two executing on separate cores.

Or if it's a single core machine, just taking turns on the same core, it's all going to work out just fine.

They don't interact, so who cares what order things happen in.

But if we have cooperating parallel processes, things get hard.

The results will depend on the order of instruction execution in each of the streams of instructions, and some of those may not be too predictable.

They may not be what we thought would happen.

So we start to get non-deterministic outcomes of running programs.

We cannot always perfectly predict what happens when we run these two programs that interact with each other.

And if they are complex programs, we can't even say, well, here are all the possible things that could happen for all the possible orders of instructions.

We run millions or billions or trillions of instructions in a program.

Each of them is doing that.

How many possible interactions are there?

How many possible cases are there of what happens before what?

Now, we see two classes of synchronization problems typically because of this cooperating parallel streams of execution.

Race conditions, non-deterministic execution.

What do we mean?

What is a race condition?

Well, we have two or three or n threads or processes that are running in parallel.

And sometimes because of scheduling, because of hardware issues, certain things in one of those threads of control happened before things in other threads of control.

But if we ran them a second time, maybe they wouldn't happen in the same order.

Sometimes they go one way, sometimes they go another way in terms of what happens first.

Okay, now this occurs all the time in modern computers.

As I keep saying, you may have 300 processes running on your laptop computer.

Clearly, a lot of things happening, and not sure ahead of time, perfectly predictably, for each of those 300 things, what will happen relative to what's happening in the other 299.

But in most cases, it doesn't matter, because they're not sharing much of anything.

They don't care what the other process is doing.

They're doing their own thing.

As long as they get to do their own thing without interference from the other process, all fine.

So most of the time, we do have these non-deterministic outcomes that happen in one order or perhaps in another order.

But we don't care.

We don't even notice that there's a difference.

However, when we have processes or threads that actually interact in a meaningful way with each other.

We see problems here.

So conflicting updates.

You got a shared data structure that you stored in a file.

Process A is writing to that data structure in the file.

Process B is writing to the same data structure in the file.

You should have done one update, process A's, then you should have done process B's update.

But is that always the order it happens?

Might it happen that process B's update sometimes occurs before process A's?

Or maybe half of Bs occurs, then half of As occurs, then the other half of Bs occurs, then the other half of As occurs.

What's gonna happen then?

There are other things that are kind of the check act race.

We'll talk about this in more detail when we talk about sleep wake up problems.

This is basically saying, I am running a process.

I need my partner over there to finish something, to be done with something before I can continue.

I shall go to sleep until my partner does something.

Under some circumstances, you may never wake up, even though your partner did do that thing.

We'll talk about how that could happen later.

Most times, that won't happen.

You go to sleep, your partner does the thing, you wake up.

Sometimes it doesn't happen that way, in which case you may never wake up.

If you have multi-object updates, so this is something we see in databases all the time, you're going to update five fields in five different records in a database.

You update one of them, then another one, then another one.

What happens if some other transaction in the database is using some of those fields that you have updated and also some of the fields you haven't gotten around to updating yet?

Will it see some of your updates, but not all of them?

That would be bad.

This is something databases have to worry about quite a lot.

We also have to worry about this in the context of operating systems more generally.

How about when we have a bunch of processes on different computers, and they're all trying to run a big combined computation.

And they go through phases of the computation.

They finish one phase, they move on to the next phase.

You would like to be able to make a quick, correct decision where everyone says, we have finished phase five, let's move to phase six.

You don't want some of the parties saying, oh, we haven't finished phase five, and other parties saying we've finished phase five, on to phase six.

Because then, you're not sure what's gonna happen.

People are gonna make decisions that are inconsistent, which would be unfortunate in many cases.

Now, these are not unmanageable problems.

We can manage these problems.

However, we have to recognize that we have this race condition problem and take actions.

And it's going to have a cost, of course.

Things always have a cost.

The other issue is a non-deterministic execution.

There's all kinds of things going on on a computer simultaneously, some of which are streams of control.

Others are pieces of hardware.

The other pieces beyond the CPU, things like the flash drive, things like the network card, they're working at their own speed, not completely under control of what happens in the CPU or the processes that are running on the CPU.

So things are going to happen.

Clocks are going to go off.

Messages are going to arrive.

A piece of data that you requested from the flash drive is suddenly going to become available, and you don't know when it's going to happen.

It could happen almost any moment.

So this can lead to all kinds of problems.

And there are some-- here are just many, many examples, and there are far more, of non-deterministic things that can happen in a modern computer.

So it becomes very hard to predict what will happen when I actually run this program, where one of these non-deterministic things may get in the way.

OK, now we're going to solve these problems by using mechanisms called synchronization mechanisms.

What is synchronization?

Well, we cannot do true parallelism where we say we are being very, very intelligent.

We are extremely intelligent.

We can look at these streams of control and say we can predict ahead of time all the outcomes that might happen, regardless of what order things have, what race conditions we have, what asynchronous things happen outside our control that suddenly take things in a different direction.

We foresee it all.

Well, you can't.

It's just too complicated.

You're not smart enough to understand it.

So what do we do?

Well, we use pseudo-parallelism, which we say, okay, we're going to assume that most of these things that could happen that aren't under our control, that could happen in different orders.

Most of them make no difference at all.

We don't care.

Happen in whatever order you want.

We're happy with the outcome.

However, we're going to have to identify here are a few places where we are not going to be happy if things don't happen in the right order.

They must happen in this particular order.

You must finish that update to that 100-byte record in memory.

You must complete the update to the file.

You must do your full database transaction before another database transaction is allowed to mess with the same data.

Those are things where we say we can identify that this is important.

We cannot allow uncontrolled behavior here. we must control the behavior.

Synchronization is the control of that behavior to ensure that things happen in a controlled order, sometimes in a particular order, sometimes simply saying, something has to happen before another thing can happen.

Maybe the other thing can happen first, but then the something must happen second.

So there are two independent problems here.

One is the critical section serialization problem.

The other is the notification of asynchronous completion problem.

We'll talk about each of these.

Sometimes depending on what treatment of synchronization you're reading about or hearing in a lecture, these will be treated as if they were the same problem, but they aren't really the same problem.

They're a little different.

However, mechanisms that we use to solve one can sometimes be used to solve the other.

And we have to solve both.

Actual ordinary computer science applications that are going to deal with parallelism must solve both these problems.

So we'll take a look at the two problems individually.

First, the critical section problem.

What is a critical section?

A critical section is some kind of resource that is shared by multiple computers.

Most commonly, that resource is code.

But the resource could be a data structure, for example.

And there are other things it could be as well.

So if you have this resource that is shared by multiple different interpreters, like multiple threads, multiple processes, multiple CPUs, by code that is interrupted or that could happen in different orders.

If by doing something with a shared resource, the thread or the process or the CPU changes the resource's state, values, properties like who can access it, relationship to other resources, where is it in this directory, hierarchy, that kind of thing, then And we may run into a critical section problem.

If by changing that state, it matters a whole lot who among the multiple interpreters gets to do it first and what they get to do before other processes are allowed to use that shared resource, then we have a critical section problem.

And this happens, for example, when we have a threat.

It's running on a CPU.

It starts to do something.

It needs to do several things.

It does part of them. like it updates that first 20 bytes of the record that contains 100 bytes.

If it gets interrupted, only 20 bytes rather than 100 bytes have been updated at the point which gets interrupted.

If another thread gets scheduled and it gets to look at that record, it sees an inconsistent record, 20 bytes new, 80 bytes old.

That's a problem.

It can also happen if we have multiple threads, we've got kernel level threads, they're running on separate processor, separate cores of the same machine.

One of them has updated the 20 bytes, the other has started reading the record.

If the other one reads the record before the first one gets around to updating the other 80 bytes, then we're gonna have a problem potentially.

So let's take a look at a few examples of critical section problems.

Here's one, files.

We're gonna update a file.

So we have process one, we have process two.

They're both going to share the same files.

File called inventory. keeping track of a business's inventory, what it's got sitting in the warehouse ready to sell to its customers.

Now process one says, I have done a lot of work and I know what should be in the inventory.

It's not all what we have in the inventory file and that's out of date.

I got a bunch of new stuff.

We've just put a bunch of new things in the warehouse.

I need to update the inventory.

How am I going to do it?

Well, here's one way I could do it.

I could say, I'm removing the old inventory file.

It's not correct.

Now I'm going to create a new file with the same name, Inventory.

I'm going to take all the information I've got about what should be in the inventory and write it into the new file.

Then I'll close the new file.

So now we have a file called Inventory, which has an updated version of what's in the warehouse.

Process 2, on the other hand, is going to say, I need to find out what's in the warehouse.

So what do I do?

I go to the inventory file, I open it for read, and I say what's in the inventory file.

I'm going to, for example, say, well, how many different records do we have in the inventory file?

Let's read in all the records and then perhaps I'll look at the records to see what they are.

Okay, so those are two processes that are making perfectly reasonable use of the inventory file.

They're not doing anything that should be illegal, not doing anything that should be a problem, except there may be some interactions.

What happens if this is the order in which these instructions that are performed by these two processes occur.

Process one removes the inventory, then it creates the inventory.

Creating typically will create a file, but it will create an empty file because it hasn't specified any content of that file.

So after process one has done these two operations, it has put a file called inventory in the appropriate place, but it's empty.

There's nothing in it.

What then, if the next thing that happens on this computer is process two starts running code, and it opens the inventory for read.

Now there is a file called inventory there, and presumably it can read that file, so it successfully opens that file for read.

And then it says, let's count what's in there.

Well, at this instant, at this moment, there's nothing in there.

It's an empty file that was just created.

So the count is clearly going to come back zero.

There's nothing in there.

Then we write 50 megabytes of data into this inventory file with all kinds of records and then we close it.

Okay, so what's happened?

Well, we have gotten clearly what is not the right answer for process two.

Process one is fine.

Got the new inventory file.

It's got everything that should be in the inventory file at the end of the process one.

Process two, on the other hand, is reported there's nothing in the inventory or warehouse's empty.

Now, this isn't true.

And moreover, when we read this empty file, if we did things in order-- so we said, first I'm going to run process one, then I'm going to run process two, or alternately.

We said, first I'm going to run process two, then I'm going to run process one.

We would not have seen the result of nothing in the warehouse.

In the first case, we would have seen the result where it was the count of all the new stuff in the warehouse.

In the second case, where process two ran first and process one round second, we would say, this is the old version of what was in the warehouse.

Maybe we didn't get the most up-to-date version, but we aren't being lied to about the fact that the warehouse appears to be empty when it never was empty.

There was always plenty of stuff in the warehouse.

So this is a result of a critical section.

Here's another result.

Let's say we're a bank, a very, very simple banking application.

So we're gonna be multi-threaded, and here we're gonna work with assembly language.

You might say, well, you know, those are big commands, full commands.

They were running at the user level.

Somebody typed them in.

Now we're talking about assembly language.

So this is actual code that people wrote.

So thread one is going to say, OK, somebody's got a balance in their account.

There's some amount of money in their account.

I am going to add money to their account.

Somebody's made a $50 deposit to their account.

So what do I do?

Well, I load in the balance they had before they made the deposit.

I put that in register one.

I figure out how much do they deposit.

I'll put that in register two.

Then I will add together register one and register two, and that will say this is their new balance.

They used to have $100, they deposited $50, they got $150, and of course I need to put that back into balance so that we have a permanent record saying, "In this person's balance, here's what they got, $150."

Then there's thread two.

Now the person has also written a check saying, I'm going to remove some money from my account and I would like to remove $25 from my account, which is a perfectly reasonable thing to do.

That 100, eventually that 150, in either case, taking out $25 is no problem.

So what we expect to have happen if we didn't do anything with thread one's deposit is we would say, okay, fine, got a $25 check.

Let's load up balance.

That's $100.

Let's take out $25.

That's the amount of the check.

So let's subtract amount two, the $25 from the $100 balance.

That's $75.

And then let's store the result, which will be kept in R1 from the subtraction.

Let's store that back into balance.

So balance would contain 75.

So if we did thread one, but we didn't do thread two, we'd end up with a balance of 150.

If we did thread two, but we didn't do thread one, we'd end up with a balance of 75.

Clearly, if we did the two properly, if we did the two operations, the deposit and the withdrawal in the correct way, we would end up with the balance being $125.

We add 100, we put in 50, we took out 25, should be 125.

Okay, so here's what would happen.

We would say, okay, somewhere there's a variable in RAM memory called balance.

They can both access balance.

They're threads, they can access the same data.

Okay, but they have their own version of the registers.

So when thread one is running, it's going to say, "Here's what I do."

Okay, so thread one says, "I would like to deposit $50."

It's read the 50 into amount one.

Thread two has said, "I'm going to withdraw $25."

It's put that into amount two.

So let's say that thread one runs first.

Thread one then says, "Okay, fine.

I'm going to load up R1 with balance.

That's 100.

R2 with the amount to be deposited, that's $50.

I'm going to add together R1 and R2.

This instruction puts the result in R1.

So there we go, $150 in R1.

Isn't life wonderful?

However, at this point, instead of running that fourth instruction for thread one, what happens if we have a context switch and thread two starts running?

So we're careful about context switches.

Thread 1's values for the registers R1 and R2 are saved, and we know what instruction we will run again when next, when thread 1 gets to execute again.

So we're fine.

So we have a context switch, but now we're going to run thread 2.

What is thread 2 going to do?

It's going to say, okay, let's load up balance, still 100.

Let's load up amount 2, which is the amount we're going to that I've loaded up.

That's 100. 100 minus 25 is 75.

The subtract instruction puts it in R1.

Then we're going to store the result back in the balance.

Balance will be 75.

Great.

Everything looks just fine.

Now, having completed thread two, we'll do a context switch back to thread one.

Now, thread one had its own values for the registers, and it had a program counter saying saying, what's the next instruction to execute?

So when we do the context switch, it's going to load up its values into the register.

R1 will have 150, R2 will have 50.

And now we do the final instruction that thread one was supposed to do, and we store its value of R1 back into balance, and then bang, get $150 in balance.

Now at this point, we have done all the code that was in thread one and thread two.

There is no more code from either thread to run.

So nothing is going to change from this point onward based on these two threads.

But we lost the $25 debit.

It disappeared.

We should have ended up with a balance of 125.

We ended up with a balance of 150.

Now, if you're writing something like, let's say, C code, you might say, well, fine, as long as I make sure that every line of C code has everything that needs to happen in that C code, then it's got to be OK, right?

Wrong.

So let's say that we have two threads that are going to do update of a counter.

They're going to say counter equals counter plus 1.

So they're basically each going to add 1 to the counter.

And they're doing exactly the same thing.

However, we do not run C code on a computer.

We run machine language, which can be represented by assembly language on a one-to-one basis.

So for example, on some particular architecture, This code, which is the same in both threads, it may even be the same lines of code in the program they're running.

This code, counter equal counter plus one, could get compiled to something like move counter comma EAX, add one to EAX, move EAX back to counter.

This is a somewhat different assembly language than what we just saw.

That's a perfectly reasonable thing to do.

It says I take the value of counter, I put it in a register, I add one to the register, I take content of the register, I put it back in counter.

Okay, what looked like one instruction, counter equal counter plus one at the C level, is actually three instructions.

Each of those three instructions is atomic.

You do the whole instruction before you do anything else.

But only a single instruction is atomic.

Three instructions are not atomic.

This is three instructions, they could be done in different orders.

So here's how this could happen.

So we have these two threads running.

This could happen.

We move counter into EAX and we add one to EAX.

So that would mean that EAX for thread one is two because counter was one, we added one, two.

Then we get a context switch and we start running thread two.

Thread two says, okay, let's move counter into EAX.

We haven't updated counter by thread one yet.

So it's still the same value.

It's one.

All right, so we put one into EAX.

We add one to EAX.

Great, now EAX would be two.

Okay, and we move EAX back into counter.

Counter is equal to two.

We context switch back to thread one to finish it up.

What does it do?

It says move EAX to counter.

Now it has saved its own copy of the registers, so its copy of EAX is two.

So it moves two back into EAX.

Now the counter should end up at three because you started at one, you added one and thread one, you added one and thread two, one plus one plus one equals three.

In this execution, it ends up being two.

Unfortunate.

Now you might say, well, how is it likely to happen?

How likely is it that you'll get a context switch, a clock interrupt, let's say, at exactly that moment, and you'll switch to exactly the wrong thing that's gonna cause a problem?

That is absolutely true.

The probability is very, very low.

However, on modern computers, you're executing a billion instructions per second.

If you're executing a billion instructions per second and a bad thing happens one time in a million, congratulations, it's happening a thousand times a second.

Low probability events at a very, very, very high speeds, which is what we've got in modern computers, happen at a very frightening rate.

And often, we get into a situation where if it happens once, just one time, you get the unlikely event happening, then everything from that point onward related to whatever data, related to whatever computation it is that's being performed, everything gets corrupted.

It all goes wrong.

Okay, so the problem that we had here was there was a critical section.

Two or more threads or processes were trying to do things with a shared resource, and only one of them should have been allowed to do it at the same time.

The other one should have been prevented from working with that critical section until the first one was finished.

Okay, now, if we had some way of ensuring that once one of these threads or processes started working with the critical section, it could continue to work with the critical section until it did everything it needed to do.

And meanwhile, no other thread or process work with that critical section, then we wouldn't have these problems.

We would have achieved what is called mutual exclusion.

If you're doing A, you can't do B.

If you're doing B, you can't do A.

Great.

We'd like to have mutual exclusion.

How?

How do we do that?

We want to make sure that if one of the threads or processes is running, the other isn't.

Well, here's one way to do that.

You might say, you know, in those examples you gave me, what was going on here?

Well, there was a clock interrupt, wasn't there?

There were context switches.

Probably, assuming the processes haven't blocked themselves, the context switch occurred because of a clock interrupt.

What if we disabled the interrupt?

Maybe not just the clock interrupts, but all the interrupts.

Let's do that.

So if there's no interrupts, then nobody preempts my code in the middle.

Now, do they?

And it is possible to block interrupts on modern computers.

There is hardware instructions that block interrupts.

Typically, you do this by setting values your processor status word, loading up the new processor status word.

And this would prevent things like context switches due to time or interrupts.

And if we had device driver code, device driver gets called when there's an interrupt.

And if we had code that might be called multiple times for the same device, meaning you'd be running multiple invocations of the same code, you might run into that kind of problem as well.

But if you turn off the interrupts, that won't happen.

Okay, now there's a problem with this.

First, when you turn off interrupts, you are essentially saying there's very, very important stuff happening outside of the realm of the CPU itself, in the clock, in this peripheral devices.

I will ignore all those very important things in order to ensure that I don't get the context switches.

That can delay things very, very badly, particularly if you have the interrupts disabled for a long time, particularly, especially if you forget to re-enable the interrupts.

Further, on the multi-core machines that we have nowadays, 16 cores, you can have these two threads of control that we've been talking about running on separate cores simultaneously, and you do not need to have context switches to have these misordered operations occur.

It may just be a question of, you know, what order they run their instructions in, and we are not controlling what happens on other cores directly.

So, It's not going to solve your problems.

So it's also the case that users can't disable interrupts.

This is very important.

If we allowed an ordinary user to write a piece of code that says, disable all my interrupts, then we would be at the mercy of that user's code.

If he chose to disable interrupts at a bad time for the computer, tough luck.

If he forgot to re-enable the interrupts, tough luck.

So the instruction that loads up that processor status to allow you to disable or re-enable interrupts, that's a privileged instruction.

Ordinary user code cannot run that instructions.

The operating system could run it, if it chooses to and sometimes will, but you'd have to be really careful in the operating system as well.

And that isn't gonna solve all your problems.

It's only gonna solve problems in the operating system's code.

Further, if you allow any code to disable the interrupts, things get kind of tricky.

You may destroy preemptive scheduling. for example, which wouldn't be good.

It may be that there are very important interrupts, such as you've just gotten an interrupt saying that the power has gone out on your computer and in less than a second, your computer is going to crash because there's no power left and you might want to clean things up now, might you?

Well, you don't get that interrupt, you don't clean things up, you die in a unfortunate state.

And further, just because you think that there might be a critical section here, maybe there even is a critical section, that doesn't mean that the other party that is participating in the critical section is actually gonna do anything.

When you've disabled interrupts, you've disabled interrupts for everybody.

And that means in turn that if there were things other parties could be doing, if only the interrupts were enabled, you can't do them because you've disabled the interrupts.

You're killing safe concurrency.

So disabling interrupts is not really a good solution.

What else can you do?

Well, here's some options.

Don't share data.

Generally speaking, most critical sections involve some form of shared data.

If you don't share data, probably don't have very many critical sections.

Another thing you can do is you can say, even on a multi-core machine, every single CPU instruction performed by any of the cores is atomic.

Nothing else that happens on any of the other cores gets in the way of completing this instruction.

The instruction starts, it completes, nothing gets in the way.

You cannot interrupt or interfere with the partial instructions of what's happening on one of these cores.

So if our critical section in terms of the code that represents the update we need to perform was one single instruction, one single machine language instruction, then fine, we can do the machine language instruction.

We know nobody else can interfere with it.

That's great.

However, what can we do with a single machine language instruction?

Generally, you can work on a limited amount of data, depending on the architecture between one and eight bytes is common.

And what can you do?

Well, you can increment, you can decrement, you can perform arithmetic operations, add, subtract.

You can do logical operations.

You can do and and ors and exclusive ors and things like that.

There are somewhat more complex ones.

Test and set, compare and swap.

We'll talk about those in a bit in the next class, and perhaps even a square root.

Some CPUs do have that in their instruction set.

But that's about it.

You can't, for example, perform updates on one memory word and then another update on another memory word, typically in a single instruction.

It's going to be two instructions.

So that's going to be limiting.

But what we can do is we can use those instructions, those uninterruptible atomic instructions, to implement a concept called LOCKS, which is essentially a software way of ensuring concurrency.

How do you do that?

Well, we'll talk about that in the next class.

That's all I intend to say in this particular lecture.

But to sum things up, what did we talk about in this lecture?

Processes are one useful form of interpreter, but they're too expensive for some purposes.

For some purposes, we need multiple interpreters and we don't want to pay the full cost, performance costs of processes.

Threads are the typical alternative we use.

We have multiple threads within a single process.

We might have multiple processes, each with multiple threads.

How do threads communicate with each other?

They communicate via memory.

They share the data area of their owning process.

Every thread is owned by one and only one process.

Every thread can access the entire data area of that process, but it does have its own stack.

If you want to have multiple processes instead of multiple threads, or in addition to multiple threads, then you don't share the same data area unless you're using that shared memory stuff.

So then what you need to do is perform inter-process communication to allow the communication between the processes.

Even shared memory is a form of inter-process communication.

However you're going to do things, whether you're going to have multiple processes, multiple threads, multiple processes each with multiple threads, you are going to eventually start to encounter some problems with parallelism.

You've got to have the parallelism to get the performance you need.

It's vital for your performance.

But you then start to run into the problems of synchronization.

We've only, in this lecture, touched on the issues of synchronization we're going to have to worry about.

We'll talk about more of those and the solutions we use to provide synchronization in systems in the next few lectures.